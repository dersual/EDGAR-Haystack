{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDGAR Ground Truth Extraction Tournament\n",
        "\n",
        "This notebook compares three strategies for extracting data from SEC 10-K filings:\n",
        "\n",
        "1. **Pure Regex**: Fast, baseline method using regular expressions.\n",
        "2. **Hybrid (Smart Locator + LLM)**: Uses **Strict Regex Anchors** first to locate the exact paragraph. If that fails, it falls back to **Keywords**. Then it asks Qwen 2.5 to extract the final answer from that zoomed-in context.\n",
        "3. **Pure LLM**: Feeds the entire section (truncated to fit context) to the LLM.\n",
        "\n",
        "### Goal\n",
        "Determine which method provides the best trade-off between accuracy and cost/speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Installation & Setup\n",
        "# Uncomment the line below if you need to install these libraries\n",
        "#!pip install -q torch transformers datasets pandas tqdm accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Import installed libraries\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Configure display to show full text in pandas\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Setup Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Load Model (Qwen 2.5-7B-Instruct)\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, logging\n",
        "\n",
        "# Quiet HF logs\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DTYPE = torch.bfloat16\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"cuda\" if DEVICE == \"cuda\" else None,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded:\", MODEL_NAME, \"| device:\", DEVICE, \"| PID:\", os.getpid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Unified Configuration (Question Bank)\n",
        "# UPGRADED: Now uses 'regex_anchors' (List) from compare_extractions.py for smarter location.\n",
        "\n",
        "QUESTION_BANK = {\n",
        "    \"section_1\": [\n",
        "        {\n",
        "            \"id\": \"company_name\",\n",
        "            # PROMPT: Targets the specific filing entity\n",
        "            \"prompt\": (\n",
        "                \"What is the exact legal name of the registrant? \"\n",
        "                \"1. Look for the very first sentence of the 'Business' section or the cover page intro \"\n",
        "                \"(e.g., 'Apple Inc. (the Registrant)...'). \"\n",
        "                \"2. Do NOT use 'Doing Business As' (DBA) names or brand names. \"\n",
        "                \"3. Do NOT include the stock ticker symbol. \"\n",
        "                \"4. Include legal suffixes like 'Inc.', 'Corp.', 'Ltd.' if present. \"\n",
        "                \"Answer with ONLY the legal name string.\"\n",
        "            ),\n",
        "            # REGEX: Finds the intro paragraph\n",
        "            \"regex_anchors\": [\n",
        "                r\"([A-Z0-9][\\w\\s.,&'-]+?)\\s*\\(?(?:the\\s+)?(?:Company|Registrant)\\b\",\n",
        "                r\"([A-Z0-9][\\w\\s.,&'-]+?),\\s+a\\s+\\w+\\s+corporation\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"incorporated\",\n",
        "                \"organized\",\n",
        "                \"the company\",\n",
        "                \"registrant\",\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"headquarters_city\",\n",
        "            \"prompt\": (\n",
        "                \"In which city are the registrant‚Äôs *principal executive offices* physically located? \"\n",
        "                \"1. Look for the address under 'Executive Offices' or 'Address of Principal Executive Offices'. \"\n",
        "                \"2. CRITICAL WARNING: Do NOT return the city of the 'Registered Agent' or 'State of Incorporation' \"\n",
        "                \"(e.g., ignore 'Wilmington' or 'Dover' unless the CEO actually works there). \"\n",
        "                \"3. Ignore P.O. Boxes. \"\n",
        "                \"Answer with ONLY the city name.\"\n",
        "            ),\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)executive\\s+offices.*?(?:located|address).*?[\\r\\n]+.*?,?\\s*([A-Z][a-z]+(?: [A-Z][a-z]+)*),?\\s+[A-Z]{2}\\s+\\d{5}\",\n",
        "                r\"(?i)located\\s+at\\s+.*?,?\\s*([A-Z][a-z]+(?: [A-Z][a-z]+)*),?\\s+[A-Z]{2}\\s+\\d{5}\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\"executive offices\", \"located at\", \"address\"],\n",
        "        },\n",
        "        # --- STATE: Hybrid Logic (Successor=New, Reinc=Old) ---\n",
        "        {\n",
        "            \"id\": \"original_incorporation_state\",\n",
        "            # PROMPT: Explicitly handles the Monsanto vs Hexcel conflict\n",
        "            \"prompt\": (\n",
        "                \"In which U.S. state was the registrant *originally* incorporated or organized? \"\n",
        "                \"Follow this strict hierarchy: \"\n",
        "                \"1. PRIORITIZE HISTORY: Look for phrases like 'originally incorporated in', 'formerly organized in', \"\n",
        "                \"or 'predecessor company incorporated in'. \"\n",
        "                \"2. REINCORPORATION RULE: If the company reincorporated (e.g., moved from California to Delaware), \"\n",
        "                \"you MUST return the OLD state (California), not the current one. \"\n",
        "                \"3. MERGER EXCEPTION: Only if the registrant is a *new* successor entity formed by a merger, \"\n",
        "                \"return the state of that successor. \"\n",
        "                \"4. If no history is mentioned, return the current state. \"\n",
        "                \"Answer with ONLY the state name.\"\n",
        "            ),\n",
        "            # REGEX: Casts a wide net to find any mention of inc, org, or predecessors\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)incorporated (?:in|under the laws of) (?:the state of )?(\\w+(?:\\s+\\w+)?)\",\n",
        "                r\"(?i)organized (?:in|under the laws of) (?:the state of )?(\\w+(?:\\s+\\w+)?)\",\n",
        "                r\"(?i)a (\\w+(?:\\s+\\w+)?) corporation\",\n",
        "                r\"(?i)state of incorporation[:\\s]+(\\w+(?:\\s+\\w+)?)\",\n",
        "                r\"(?i)originally\\s+(?:incorporated|organized).*?in\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?)\",\n",
        "                r\"(?i)predecessor.*?incorporated.*?in\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?)\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"incorporated\",\n",
        "                \"organized\",\n",
        "                \"originally\",\n",
        "                \"predecessor\",\n",
        "                \"laws of the state\",\n",
        "            ],\n",
        "        },\n",
        "        # --- YEAR: Hybrid Logic (Successor=New, Reinc=Old) ---\n",
        "        {\n",
        "            \"id\": \"original_incorporation_year\",\n",
        "            # PROMPT: Explicitly handles the Monsanto vs Hexcel conflict\n",
        "            \"prompt\": (\n",
        "                \"In which year was the registrant *originally* incorporated or organized? \"\n",
        "                \"1. IGNORE 'FOUNDED' dates. Only look for 'incorporated', 'organized', or 'formed'. \"\n",
        "                \"2. REINCORPORATION RULE: If the text says 'originally incorporated in 1980' and 'reincorporated in 1995', \"\n",
        "                \"return the EARLIEST year (1980). \"\n",
        "                \"3. MERGER EXCEPTION: If the current entity was formed by a merger of equals, use the year of that merger. \"\n",
        "                \"Answer with ONLY the year (YYYY).\"\n",
        "            ),\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)(?:incorporated|organized|founded|established|formed) (?:in |on |)(?:\\w+ )?(18\\d{2}|19\\d{2}|20\\d{2})\",\n",
        "                r\"(?i)originally\\s+incorporated.*?(18\\d{2}|19\\d{2}|20\\d{2})\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"founded\",\n",
        "                \"incorporated\",\n",
        "                \"organized\",\n",
        "                \"year\",\n",
        "                \"originally\",\n",
        "            ],\n",
        "        },\n",
        "        # --- EMPLOYEES: Exclusion Logic ---\n",
        "        {\n",
        "            \"id\": \"employee_count\",\n",
        "            \"prompt\": (\n",
        "                \"What is the total number of employees the registrant has? \"\n",
        "                \"1. PREFER FULL-TIME: If the text distinguishes between full-time and part-time, return the full-time count. \"\n",
        "                \"2. If only 'total' is given, use that. \"\n",
        "                \"3. EXCLUDE: Do not count independent contractors, agents, or temporary staff unless they are the only number given. \"\n",
        "                \"4. FORMAT: Remove commas and return ONLY the integer (e.g., return 14500, not 14,500). \"\n",
        "                \"If the number is 'approximately 5,000', return 5000.\"\n",
        "            ),\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)(?:had|employ).*?([0-9,]+)\\s+(?:full-time)?\\s+employees\",\n",
        "                r\"(?i)(?:had|employ(?:ed|s)?|totaling)\\s+(?:approximately|over|roughly|about\\s+)?([0-9,]+)\\s+(?:full-time|total)?\\s+employees\",\n",
        "                r\"(?i)([0-9,]+)\\s+(?:full-time\\s+)?(?:people|persons|employees)\\s+(?:were|are)\\s+employed\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\"employees\", \"full-time\"],\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"headquarters_state\",\n",
        "            \"prompt\": (\n",
        "                \"In which U.S. state are the registrant‚Äôs *principal executive offices* physically located? \"\n",
        "                \"1. This is the state where the HQ building is, NOT necessarily the state of incorporation. \"\n",
        "                \"2. CRITICAL: If the text says 'Incorporated in Delaware' but 'Executive offices in California', \"\n",
        "                \"return CALIFORNIA. \"\n",
        "                \"Answer with ONLY the state name.\"\n",
        "            ),\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)(?:headquarters|principal (?:executive )?offices?|corporate offices?) (?:is |are |)(?:located |)in ([^,\\.\\n]+)\",\n",
        "                r\"(?i)executive offices.*?,[\\s\\r\\n]+([A-Z][a-z]+(?: [A-Z][a-z]+)*)[\\s\\r\\n]+\\d{5}\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"executive offices\",\n",
        "                \"headquarters\",\n",
        "                \"principal offices\",\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        "    \"section_10\": [\n",
        "        {\n",
        "            \"id\": \"ceo_lastname\",\n",
        "            \"What is the LAST NAME of the registrant‚Äôs current Chief Executive Officer (CEO)? \"\n",
        "                \"1. Look for 'Chief Executive Officer', 'CEO', or 'Principal Executive Officer'. \"\n",
        "                \"2. If 'Co-CEOs' are listed, pick the first one mentioned. \"\n",
        "                \"3. EXCLUDE titles (Mr., Dr.) and first/middle names. \"\n",
        "                \"4. If the CEO has a compound last name (e.g., 'Von Trap'), include the full last name. \"\n",
        "                \"Answer with ONLY the last name string.\"\n",
        "            ),\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)([A-Z][a-z]+ [A-Z][a-z]+)[,\\s]+(?:is |serves as |)(?:the |our |)(?:Chief Executive Officer|CEO)\",\n",
        "                r\"(?i)(?:Chief Executive Officer|CEO)[:\\s]+([A-Z][a-z]+ [A-Z][a-z]+)\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\"chief executive officer\", \"ceo\", \"serves as\"],\n",
        "        }\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. New \"Smart Context\" Logic\n",
        "# This implements the logic: strict regex match -> zoom out -> fallback keyword -> zoom out\n",
        "\n",
        "\n",
        "def merge_ranges(ranges):\n",
        "    \"\"\"\n",
        "    Merges overlapping or adjacent integer ranges.\n",
        "    Input: [(0, 100), (50, 150), (300, 400)]\n",
        "    Output: [(0, 150), (300, 400)]\n",
        "    \"\"\"\n",
        "    if not ranges:\n",
        "        return []\n",
        "    # Sort by start time\n",
        "    ranges.sort(key=lambda x: x[0])\n",
        "\n",
        "    merged = [ranges[0]]\n",
        "    for current in ranges[1:]:\n",
        "        last = merged[-1]\n",
        "        # If overlap or adjacent, merge\n",
        "        if current[0] <= last[1]:\n",
        "            new_end = max(last[1], current[1])\n",
        "            merged[-1] = (last[0], new_end)\n",
        "        else:\n",
        "            merged.append(current)\n",
        "    return merged\n",
        "\n",
        "\n",
        "def get_smart_context(full_text, config_question, window_size=1500):\n",
        "    \"\"\"\n",
        "    Priority 1: Check STRICT regex anchors. If found, zoom out around the match and append to list.\n",
        "    Priority 2: Check GENERIC keywords. If found, zoom out around the first hit.\n",
        "    Fallback: Return full text.\n",
        "    \"\"\"\n",
        "    if not full_text:\n",
        "        return \"\"\n",
        "    found_ranges = []\n",
        "    # Anchors first\n",
        "    for pattern in config_question.get(\"regex_anchors\", []):\n",
        "        # findinter gives us all matches\n",
        "        matches = re.finditer(pattern, full_text)\n",
        "        matched = False\n",
        "        for match in matches:\n",
        "            if not matched:\n",
        "                matched = True\n",
        "                print(f\"  [SmartLocator] Found Anchor: {pattern[:30]}...\")\n",
        "            start = max(0, match.start() - (window_size // 2))\n",
        "            end = min(len(full_text), match.end() + (window_size // 2))\n",
        "            found_ranges.append((start, end)) \n",
        "    \n",
        "    if found_ranges: \n",
        "        merged = merge_ranges(found_ranges)  \n",
        "        context_parts = []\n",
        "        for r in merged:\n",
        "            context_parts.append(full_text[r[0]:r[1]])\n",
        "            \n",
        "        print(f\"  [SmartLocator] Found {len(found_ranges)} matches, merged into {len(merged)} windows.\")\n",
        "        return \"\\n\\n...[SECTION BREAK]...\\n\\n\".join(context_parts)\n",
        "    \n",
        "    # Keywords if no anchors found\n",
        "    text_lower = full_text.lower()\n",
        "\n",
        "    for keyword in config_question.get(\"fallback_keywords\", []):\n",
        "        index = text_lower.find(keyword.lower())\n",
        "        if index != -1:\n",
        "            print(f\"  [SmartLocator] Fallback Keyword: {keyword}\")\n",
        "            start = max(0, index - (window_size // 2))\n",
        "            end = min(len(full_text), index + (window_size // 2))\n",
        "            return full_text[start:end]\n",
        "\n",
        "    # 3. Fallback to full_text\n",
        "    return full_text\n",
        "\n",
        "\n",
        "def ask_llm(context, prompt, model, tokenizer, max_new_tokens=50):\n",
        "    \"\"\"Sends a prompt to the LLM with the given context.\"\"\"\n",
        "    if not context or not context.strip():\n",
        "        return None\n",
        "\n",
        "    model_limit = getattr(tokenizer, \"model_max_length\", 32768) \n",
        "    print(f\"  [LLM] Model limit: {model_limit}\")\n",
        "    available_for_context = model_limit - 650\n",
        "    char_limit = available_for_context * 4\n",
        "\n",
        "    if len(context) > char_limit:\n",
        "        context = context[:char_limit]\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Read this SEC 10-K filing excerpt and answer the question. \\nContext: \\\"{context}\\\"\\n\\nQuestion: {prompt} \\nIf the information is not present in the context, reply with 'NULL'.\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs, max_new_tokens=max_new_tokens, do_sample=False, temperature=0.0\n",
        "        )\n",
        "\n",
        "    # Clean response\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "    answer = response.strip().split(\"\\n\")[0]\n",
        "\n",
        "    if \"null\" in answer.lower() or \"not found\" in answer.lower():\n",
        "        return None\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Strategy Implementations\n",
        "\n",
        "\n",
        "def extract_pure_regex(doc):\n",
        "    \"\"\"Strategy 1: Pure Regex Matching (Updated to use List of Anchors)\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        text = doc.get(section, \"\")\n",
        "        if not text:\n",
        "            for q in questions:\n",
        "                results[q[\"id\"]] = None\n",
        "            continue\n",
        "\n",
        "        for q in questions:\n",
        "            # Try all strict regex patterns in order\n",
        "            found_val = None\n",
        "            for pattern in q.get(\"regex_anchors\", []):\n",
        "                match = re.search(pattern, str(text), re.DOTALL)\n",
        "                if match:\n",
        "                    # Found a match, capture group 1\n",
        "                    found_val = match.group(1).strip(\" .,;\")[:100]\n",
        "                    break  # Stop after first match\n",
        "\n",
        "            results[q[\"id\"]] = found_val\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def extract_hybrid_llm(doc, model, tokenizer):\n",
        "    \"\"\"Strategy 2: Hybrid (Smart Locator + LLM Extraction)\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        full_text = doc.get(section, \"\")\n",
        "\n",
        "        for q in questions:\n",
        "            # 1. SMART LOCATE: Try regex anchor -> Zoom -> Fallback Keyword\n",
        "            context = get_smart_context(full_text, q)\n",
        "\n",
        "            # 2. Extract with LLM\n",
        "            answer = ask_llm(context, q[\"prompt\"], model, tokenizer)\n",
        "            results[q[\"id\"]] = answer\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def extract_pure_llm(doc, model, tokenizer):\n",
        "    \"\"\"Strategy 3: Pure LLM (Feed entire section context)\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        full_text = doc.get(section, \"\")\n",
        "        if not full_text:\n",
        "            for q in questions:\n",
        "                results[q[\"id\"]] = None\n",
        "            continue\n",
        "        for q in questions:\n",
        "            answer = ask_llm(full_text, q[\"prompt\"], model, tokenizer)\n",
        "            results[q[\"id\"]] = answer\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. The Tournament Loop\n",
        "# Runs detailed comparison on a small batch of documents\n",
        "\n",
        "def create_df_tournament(model, tokenizer, TARGET_DOCS=5): \n",
        "    print(\"Loading Data Stream...\")\n",
        "    dataset = load_dataset(\n",
        "        \"c3po-ai/edgar-corpus\",\n",
        "        \"default\",\n",
        "        split=\"train\",\n",
        "        streaming=True,\n",
        "        revision=\"refs/convert/parquet\",\n",
        "    )\n",
        "\n",
        "    comparison_data = []\n",
        "\n",
        "    print(f\"Running Tournament on {TARGET_DOCS} docs...\")\n",
        "    for i, doc in enumerate(dataset):\n",
        "        if i >= TARGET_DOCS:\n",
        "            break\n",
        "\n",
        "        print(f\"Processing {doc.get('filename', f'Doc {i}')}...\")\n",
        "\n",
        "        # Strategy 1: Regex (Now Smart Regex)\n",
        "        res_regex = extract_pure_regex(doc)\n",
        "\n",
        "        # Strategy 2: Hybrid (Now Smart Context)\n",
        "        res_hybrid = extract_hybrid_llm(doc, model, tokenizer)\n",
        "\n",
        "        # Strategy 3: Pure LLM\n",
        "        res_pure = extract_pure_llm(doc, model, tokenizer)\n",
        "\n",
        "        # Combine results\n",
        "        row = {\n",
        "            \"filename\": doc.get(\"filename\"),\n",
        "            \"cik\": doc.get(\"cik\"),\n",
        "            \"year\": doc.get(\"year\"),\n",
        "        }\n",
        "\n",
        "        all_keys = list(res_regex.keys())\n",
        "\n",
        "        for key in all_keys:\n",
        "            # Shorten key for column name width\n",
        "            short_key = key.replace(\"incorporation_\", \"Inc_\").replace(\n",
        "                \"headquarters_\", \"HQ_\"\n",
        "            )\n",
        "\n",
        "            val_hybrid = res_hybrid.get(key)\n",
        "            val_pure = res_pure.get(key)\n",
        "\n",
        "            row[f\"{short_key}_Regex\"] = res_regex.get(key)\n",
        "            row[f\"{short_key}_Hybrid\"] = val_hybrid\n",
        "            row[f\"{short_key}_LLM\"] = val_pure\n",
        "\n",
        "            # Consensus Check\n",
        "            if val_hybrid is None and val_pure is None:\n",
        "                row[f\"{short_key}_Match\"] = None \n",
        "            elif val_hybrid is None or val_pure is None: \n",
        "                row[f\"{short_key}_Match\"] = \"0\"\n",
        "            else:\n",
        "                is_match = val_hybrid.strip().lower() == val_pure.strip().lower()\n",
        "                row[f\"{short_key}_Match\"] = \"1\" if is_match else \"0\"\n",
        "\n",
        "        comparison_data.append(row)\n",
        "\n",
        "    df_tournament = pd.DataFrame(comparison_data)\n",
        "    print(\"\\n--- TOURNAMENT RESULTS ---\")  \n",
        "    display(df_tournament)\n",
        "    df_tournament.fillna(\"NULL\", inplace=True)\n",
        "    return df_tournament"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Run the tournament and save the results to a CSV file. \n",
        "\n",
        "# sample so only do 5 \n",
        "df_sample = create_df_tournament(model, tokenizer)   \n",
        "# Feel free to add code blocks below to analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. If fine create sample doc csv\n",
        "df_sample.to_csv(\"edgar_tournament_sample.csv\", index=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. If fine create full doc csv \n",
        "df_full = create_df_tournament(model, tokenizer, 250)\n",
        "df_full.to_csv(\"edgar_tournament_full.csv\", index=False) \n",
        "df_results = df_full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparing Different Extraction Approaches With Ground Truth "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# 1. Identify the columns you want to KEEP\n",
        "# We keep a column ONLY if it does NOT contain \"Match\", \"filename\", \"year\", or \"cik\"\n",
        "exclude_terms = [\"Match\", \"filename\", \"year\", \"cik\"]\n",
        "cols_to_keep = [\n",
        "    col for col in df_full.columns if not any(term in col for term in exclude_terms)\n",
        "]\n",
        "\n",
        "# 2. Create a subset (this does not change df_full)\n",
        "subset = df_full[cols_to_keep]\n",
        "\n",
        "# 3. Calculate stats on the subset\n",
        "# Compare entire subset to the string 'NULL'\n",
        "null_counts = (subset == \"NULL\").sum()\n",
        "\n",
        "# Calculate percentage based on total rows\n",
        "null_percentages = (null_counts / len(df_full) * 100).apply(math.floor)\n",
        "\n",
        "# 4. Create the final result table\n",
        "summary_df = pd.DataFrame({\"Null Counts\": null_counts, \"Null %\": null_percentages})\n",
        "\n",
        "# Filter out columns with 0 nulls if you only want to see problems\n",
        "# summary_df = summary_df[summary_df['Null Counts'] > 0]\n",
        "\n",
        "print(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use this if you have a csv that has the results of your different approaches load them here too through this function \n",
        "df_results = pd.read_csv(\"filename here\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load in ground truth data and compare with different approaches\n",
        "ground_truth_file = \"\" # put file name here \n",
        "df_truth = pd.read_csv(ground_truth_file)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def normalize_text(val):\n",
        "    if pd.isna(val) or str(val).lower() in ['null', 'nan', 'none', '']:\n",
        "        return \"NULL\"\n",
        "    return str(val).strip().lower().replace('.', '').replace(',', '')\n",
        "\n",
        "def normalize_year(val):\n",
        "    try:\n",
        "        if pd.isna(val) or str(val).lower() in ['null', 'nan', '']:\n",
        "            return \"NULL\"\n",
        "        return str(int(float(val)))\n",
        "    except:\n",
        "        return \"NULL\" \n",
        "\n",
        "def normalize_count(val):\n",
        "    \"\"\"\n",
        "    Cleans numbers: removes commas, handles floats, enforces Integer format.\n",
        "    Example: \"4,000\" -> \"4000\",  465.0 -> \"465\"\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if pd.isna(val) or str(val).lower() in ['null', 'nan', 'none', '']:\n",
        "            return \"NULL\"\n",
        "        \n",
        "        # Remove commas first (e.g. \"5,317\" -> \"5317\")\n",
        "        clean_string = str(val).replace(',', '').strip()\n",
        "        \n",
        "        # Convert to float first (handles \"465.0\"), then int, then string\n",
        "        return str(int(float(clean_string)))\n",
        "    except:\n",
        "        return \"NULL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_approaches(df_truth, df_approach, comparison_map):\n",
        "    gt = df_truth.copy()\n",
        "    res = df_approach.copy()\n",
        "\n",
        "    if \"filename\" in gt.columns:\n",
        "        gt.set_index(\"filename\", inplace=True)\n",
        "    if \"filename\" in res.columns:\n",
        "        res.set_index(\"filename\", inplace=True)\n",
        "    common_files = gt.index.intersection(res.index)\n",
        "    if len(common_files) == 0:\n",
        "        print(\"No matching files found between Truth and Results!\")\n",
        "        return\n",
        "\n",
        "    gt = gt.loc[common_files]\n",
        "    res = res.loc[common_files]\n",
        "\n",
        "    print(f\"--- üèÜ TOURNAMENT RESULTS ({len(common_files)} docs) ---\")\n",
        "\n",
        "    # 2. Loop through the Map\n",
        "    for truth_col, pred_cols_list in comparison_map.items():\n",
        "\n",
        "        if truth_col not in gt.columns:\n",
        "            print(f\"\\n[SKIP] Ground Truth column '{truth_col}' not found.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nEvaluating Feature: {truth_col}\")\n",
        "        print(f\"{'Method':<25} | {'Accuracy':<10} | {'Correct/Total'}\")\n",
        "        print(\"-\" * 55)\n",
        "\n",
        "        # Select Cleaner Logic\n",
        "        cleaner = normalize_text\n",
        "        if \"year\" in truth_col.lower():\n",
        "            cleaner = normalize_year\n",
        "        if \"count\" in truth_col.lower() or \"employee\" in truth_col.lower():\n",
        "            cleaner = normalize_count\n",
        "\n",
        "        # Normalize Truth once\n",
        "        y_true = gt[truth_col].apply(cleaner)\n",
        "\n",
        "        # 3. Compare each Approach against this Truth\n",
        "        scores = []\n",
        "        for pred_col in pred_cols_list:\n",
        "            if pred_col not in res.columns:\n",
        "                print(f\"{pred_col:<25} | [MISSING COLUMN]\")\n",
        "                continue\n",
        "\n",
        "            y_pred = res[pred_col].apply(cleaner)\n",
        "\n",
        "            # Calculate Score\n",
        "            matches = y_true == y_pred\n",
        "            acc = matches.mean()\n",
        "            correct = matches.sum()\n",
        "\n",
        "            print(f\"{pred_col:<25} | {acc:.2%}    | {correct}/{len(matches)}\")\n",
        "\n",
        "            scores.append((pred_col, acc))\n",
        "\n",
        "        # Optional: Print Winner of this round\n",
        "        if scores:\n",
        "            best_method, best_score = max(scores, key=lambda x: x[1])\n",
        "            print(f\" >> WINNER: {best_method} ({best_score:.2%})\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tournament_map = {\n",
        "    # Truth Column                : [List of Approaches to Test]\n",
        "    'original_inc_state_truth':   ['Inc_state_Regex', 'Inc_state_Hybrid', 'Inc_state_LLM'],\n",
        "    'original_inc_year_truth':    ['Inc_year_Regex', 'Inc_year_Hybrid', 'Inc_year_LLM'],\n",
        "    'employee_count_truth':       ['employee_count_Regex', 'employee_count_Hybrid', 'employee_count_LLM']\n",
        "} \n",
        "\n",
        "compare_approaches(df_truth, df_results,tournament_map)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
