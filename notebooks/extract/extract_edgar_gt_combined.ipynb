{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDGAR Ground Truth Combined Extraction (Full Context)\n",
        "\n",
        "**Goal**: Extract all 8 fields from SEC 10-K filings with evidence provenance.\n",
        "\n",
        "**Model**: Llama 3.3 70B Instruct via vLLM on Lambda GPU\n",
        "\n",
        "**Strategy**: Per-field sequential extraction with JSON structured output.\n",
        "\n",
        "**Output**: CSV with `{field}_value`, `{field}_evidence`, `{field}_source_sentence`, `{field}_evidence_verified` for each field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Installation (Run once on Lambda) \n",
        "#!export HF_TOKEN=\"your_token\"\n",
        "#!pip install -q vllm datasets pandas tqdm thefuzz python-Levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Imports & Setup\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import json\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(\"Setup Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Load Model (Llama 3.3 70B Instruct via vLLM)\n",
        "\n",
        "MODEL_NAME = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
        "\n",
        "print(f\"Loading {MODEL_NAME} via vLLM...\")\n",
        "llm = LLM(\n",
        "    model=MODEL_NAME,\n",
        "    tensor_parallel_size=torch.cuda.device_count(),  # Use all available GPUs\n",
        "    max_model_len=65536,  # 64K context to fit ~50-60K docs with room for output\n",
        "    enable_prefix_caching=True,  # Reduces attention dilution\n",
        "    gpu_memory_utilization=0.90,\n",
        "    dtype=\"bfloat16\",\n",
        ")\n",
        "print(f\"Model loaded on {torch.cuda.device_count()} GPU(s).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Configuration\n",
        "\n",
        "OUTPUT_FILE = \"edgar_gt_combined_extracted.csv\"\n",
        "BATCH_SIZE = 10  # Save checkpoint every N documents\n",
        "MAX_DOCUMENTS = 250  # Total documents to process\n",
        "\n",
        "# Sampling parameters for deterministic output\n",
        "SAMPLING_PARAMS = SamplingParams(\n",
        "    temperature=0.0,\n",
        "    max_tokens=500,\n",
        "    stop=[\"}\"],  # Stop after JSON closes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Question Bank (All 8 Fields)\n",
        "\n",
        "QUESTION_BANK = [\n",
        "    {\n",
        "        \"id\": \"registrant_name\",\n",
        "        \"prompt\": (\n",
        "            \"What is the exact legal name of the registrant as explicitly stated in the document? \"\n",
        "            \"Return ONLY the legal name string or NOT_FOUND.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"headquarters_city\",\n",
        "        \"prompt\": (\n",
        "            \"What city is explicitly stated as the location of the registrant's principal executive offices? \"\n",
        "            \"Return ONLY the city name or NOT_FOUND.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"headquarters_state\",\n",
        "        \"prompt\": (\n",
        "            \"What U.S. state is explicitly stated as the location of the registrant's principal executive offices? \"\n",
        "            \"Return ONLY the state name or NOT_FOUND.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"incorporation_state\",\n",
        "        \"prompt\": (\n",
        "            \"What is the registrant's state of incorporation as explicitly stated in the document? \"\n",
        "            \"Return ONLY the state name or NOT_FOUND.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"incorporation_year\",\n",
        "        \"prompt\": (\n",
        "            \"What is the registrant's year of incorporation as explicitly stated in the document? \"\n",
        "            \"Return ONLY the year (YYYY) or NOT_FOUND.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"employee_count\",\n",
        "        \"prompt\": (\n",
        "            \"What number of employees is explicitly stated in the document? \"\n",
        "            \"Return ONLY the integer (remove commas) or NOT_FOUND.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"ceo_lastname\",\n",
        "        \"prompt\": (\n",
        "            \"What is the last name of the individual explicitly identified as the Chief Executive Officer (CEO) in the document? \"\n",
        "            \"Return ONLY the last name string or NOT_FOUND.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"holder_record_amount\",\n",
        "        \"prompt\": (\n",
        "            \"What is the number of holders of record of the registrant's common stock as explicitly stated in the document? \"\n",
        "            \"Return ONLY the integer (remove commas) or NOT_FOUND.\"\n",
        "        ),\n",
        "    },\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Full Context Builder (Concatenates ALL Sections)\n",
        "\n",
        "SECTION_KEYS = [\n",
        "    \"section_1\", \"section_1A\", \"section_1B\", \"section_2\", \"section_3\",\n",
        "    \"section_4\", \"section_5\", \"section_6\", \"section_7\", \"section_7A\",\n",
        "    \"section_8\", \"section_9\", \"section_9A\", \"section_9B\", \"section_10\",\n",
        "    \"section_11\", \"section_12\", \"section_13\", \"section_14\", \"section_15\"\n",
        "]\n",
        "\n",
        "def build_full_context(doc):\n",
        "    \"\"\"\n",
        "    Concatenates ALL available sections from the 10-K filing.\n",
        "    Returns the full text with section headers for context.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for key in SECTION_KEYS:\n",
        "        section_text = doc.get(key, \"\")\n",
        "        if section_text and section_text.strip():\n",
        "            parts.append(f\"\\n\\n--- [{key.upper()}] ---\\n\\n{section_text}\")\n",
        "    \n",
        "    return \"\".join(parts) if parts else \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Extraction Prompt Template\n",
        "\n",
        "EXTRACTION_TEMPLATE = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "You are a precise SEC 10-K filing data extraction assistant. You MUST:\n",
        "1. Extract information ONLY from the provided text.\n",
        "2. If the information is not found, return \"NOT_FOUND\" as the value, and for evidence provide your reasoning on why it was not found.\n",
        "3. Always provide the exact quote from the text as evidence. \n",
        "4. Respond ONLY with valid JSON, nothing else.\n",
        "\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Read this SEC 10-K filing and answer the question.\n",
        "\n",
        "**Question**: {question}\n",
        "\n",
        "**Instructions**:\n",
        "- Provide your answer as a JSON object with exactly these keys:\n",
        "  - \"value\": The extracted answer (or \"NOT_FOUND\" if not present)\n",
        "  - \"evidence\": If value is found, the EXACT substring from the text. If NOT_FOUND, explain WHY it could not be found.\n",
        "  - \"source_sentence\": The complete sentence containing the evidence (or \"N/A\" if NOT_FOUND)\n",
        "\n",
        "**10-K Filing Text**:\n",
        "{context}\n",
        "\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{{\"\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. LLM Extraction Function\n",
        "\n",
        "def extract_field(full_text, question_config, llm, sampling_params):\n",
        "    \"\"\"\n",
        "    Extracts a single field from the full text using the LLM.\n",
        "    Returns: (value, evidence, source_sentence)\n",
        "    \"\"\"\n",
        "    if not full_text or not full_text.strip():\n",
        "        return \"NOT_FOUND\", \"NO_CONTEXT\", \"NO_CONTEXT\"\n",
        "    \n",
        "    prompt = EXTRACTION_TEMPLATE.format(\n",
        "        question=question_config[\"prompt\"],\n",
        "        context=full_text\n",
        "    )\n",
        "    \n",
        "    # Generate with vLLM\n",
        "    outputs = llm.generate([prompt], sampling_params)\n",
        "    response_text = outputs[0].outputs[0].text.strip()\n",
        "    \n",
        "    # Parse JSON response\n",
        "    try:\n",
        "        # The response starts after we injected '{\"', so prepend it back\n",
        "        json_str = '{\"' + response_text\n",
        "        if not json_str.endswith('}'):\n",
        "            json_str += '}'\n",
        "        \n",
        "        data = json.loads(json_str)\n",
        "        value = data.get(\"value\", \"PARSE_ERROR\")\n",
        "        evidence = data.get(\"evidence\", \"PARSE_ERROR\")\n",
        "        source_sentence = data.get(\"source_sentence\", \"PARSE_ERROR\")\n",
        "        \n",
        "        # Clean up value\n",
        "        if value:\n",
        "            value = str(value).strip().rstrip('.')\n",
        "        \n",
        "        return value, evidence, source_sentence\n",
        "        \n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"  [JSON ERROR] {question_config['id']}: {e}\")\n",
        "        print(f\"  Raw response: {response_text[:200]}...\")\n",
        "        return \"JSON_PARSE_ERROR\", response_text[:500], \"JSON_PARSE_ERROR\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. Evidence Verification (\"Judge\" Logic)\n",
        "\n",
        "def get_fingerprint(text):\n",
        "    \"\"\"Removes all non-alphanumeric characters for fuzzy matching.\"\"\"\n",
        "    return re.sub(r'[\\W_]+', '', text).lower()\n",
        "\n",
        "def verify_evidence(full_text, evidence_quote, value):\n",
        "    \"\"\"\n",
        "    Checks if the evidence quote actually exists in the full text.\n",
        "    Returns: \n",
        "        - True if verified (evidence found in text)\n",
        "        - False if not found (potential hallucination)\n",
        "        - \"NOT_APPLICABLE\" if value was NOT_FOUND (evidence is reasoning, not a quote)\n",
        "    \"\"\"\n",
        "    # If value is NOT_FOUND, evidence is reasoning - don't try to verify it exists\n",
        "    if value and str(value).upper() == \"NOT_FOUND\":\n",
        "        return \"NOT_APPLICABLE\"\n",
        "    \n",
        "    if not evidence_quote or evidence_quote in [\"NO_CONTEXT\", \"PARSE_ERROR\", \"JSON_PARSE_ERROR\", \"N/A\"]:\n",
        "        return None  # Missing data\n",
        "    \n",
        "    if not full_text:\n",
        "        return False\n",
        "    \n",
        "    # 1. Exact match\n",
        "    if evidence_quote in full_text:\n",
        "        return True\n",
        "    \n",
        "    # 2. Normalized match (ignore whitespace/punctuation differences)\n",
        "    clean_text = \" \".join(full_text.split()).lower()\n",
        "    clean_evd = \" \".join(evidence_quote.split()).lower()\n",
        "    \n",
        "    if clean_evd in clean_text:\n",
        "        return True\n",
        "    \n",
        "    # 3. Fingerprint match (ignore all punctuation)\n",
        "    fp_text = get_fingerprint(full_text)\n",
        "    fp_evd = get_fingerprint(evidence_quote)\n",
        "    \n",
        "    if len(fp_evd) > 10 and fp_evd in fp_text:\n",
        "        return True\n",
        "    \n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. Load Dataset (from existing notebook pattern)\n",
        "\n",
        "def load_edgar_dataset():\n",
        "    \"\"\"Load the EDGAR corpus with streaming.\"\"\"\n",
        "    return load_dataset(\n",
        "        \"c3po-ai/edgar-corpus\",\n",
        "        \"default\",\n",
        "        split=\"train\",\n",
        "        streaming=True,\n",
        "        revision=\"refs/convert/parquet\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 11. Process Single Document\n",
        "\n",
        "def process_document(doc, llm, sampling_params):\n",
        "    \"\"\"\n",
        "    Extracts all 8 fields from a single document.\n",
        "    Returns a dict with all columns for the output CSV.\n",
        "    \"\"\"\n",
        "    result = {\n",
        "        \"filename\": doc.get(\"filename\"),\n",
        "        \"cik\": doc.get(\"cik\"),\n",
        "        \"year\": doc.get(\"year\"),\n",
        "    }\n",
        "    \n",
        "    # Build full context once\n",
        "    full_text = build_full_context(doc)\n",
        "    result[\"full_text\"] = full_text  # Store for manual review\n",
        "    \n",
        "    if not full_text:\n",
        "        # No content - mark all fields as not found\n",
        "        for q in QUESTION_BANK:\n",
        "            field_id = q[\"id\"]\n",
        "            result[f\"{field_id}_value\"] = \"NO_CONTENT\"\n",
        "            result[f\"{field_id}_evidence\"] = \"NO_CONTENT\"\n",
        "            result[f\"{field_id}_source_sentence\"] = \"NO_CONTENT\"\n",
        "            result[f\"{field_id}_evidence_verified\"] = None\n",
        "        return result\n",
        "    \n",
        "    # Extract each field\n",
        "    for question in QUESTION_BANK:\n",
        "        field_id = question[\"id\"]\n",
        "        \n",
        "        value, evidence, source_sentence = extract_field(\n",
        "            full_text, question, llm, sampling_params\n",
        "        )\n",
        "        \n",
        "        # Verify evidence exists in text\n",
        "        evidence_verified = verify_evidence(full_text, evidence, value)\n",
        "        \n",
        "        result[f\"{field_id}_value\"] = value\n",
        "        result[f\"{field_id}_evidence\"] = evidence\n",
        "        result[f\"{field_id}_source_sentence\"] = source_sentence\n",
        "        result[f\"{field_id}_evidence_verified\"] = evidence_verified\n",
        "    \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 12. Main Extraction Loop\n",
        "\n",
        "def run_extraction(\n",
        "    output_file=OUTPUT_FILE,\n",
        "    limit=MAX_DOCUMENTS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "):\n",
        "    \"\"\"\n",
        "    Main extraction loop with resume support and batch checkpointing.\n",
        "    \"\"\"\n",
        "    print(f\"--- COMBINED EXTRACTION: {limit} documents ---\")\n",
        "    \n",
        "    # 1. Resume support: Load existing progress\n",
        "    if os.path.exists(output_file):\n",
        "        df_results = pd.read_csv(output_file)\n",
        "        processed_files = set(df_results[\"filename\"].tolist())\n",
        "        print(f\"Resuming: {len(processed_files)} documents already processed.\")\n",
        "    else:\n",
        "        df_results = pd.DataFrame()\n",
        "        processed_files = set()\n",
        "    \n",
        "    # 2. Load dataset\n",
        "    dataset = load_edgar_dataset()\n",
        "    \n",
        "    current_batch = []\n",
        "    total_processed = len(processed_files)\n",
        "    new_processed = 0\n",
        "    \n",
        "    # 3. Main loop\n",
        "    for doc in tqdm(dataset, desc=\"Extracting\", total=limit):\n",
        "        fname = doc.get(\"filename\")\n",
        "        \n",
        "        # Skip if already processed\n",
        "        if fname in processed_files:\n",
        "            continue\n",
        "        \n",
        "        # Limit check\n",
        "        if total_processed + new_processed >= limit:\n",
        "            break\n",
        "        \n",
        "        # Process document\n",
        "        result = process_document(doc, llm, SAMPLING_PARAMS)\n",
        "        current_batch.append(result)\n",
        "        new_processed += 1\n",
        "        \n",
        "        # Batch checkpoint\n",
        "        if len(current_batch) >= batch_size:\n",
        "            df_batch = pd.DataFrame(current_batch)\n",
        "            df_results = pd.concat([df_results, df_batch], ignore_index=True)\n",
        "            df_results.to_csv(output_file, index=False)\n",
        "            current_batch = []\n",
        "            \n",
        "            # Memory cleanup\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"  [Checkpoint] Saved {total_processed + new_processed}/{limit} docs.\")\n",
        "    \n",
        "    # 4. Final save\n",
        "    if current_batch:\n",
        "        df_batch = pd.DataFrame(current_batch)\n",
        "        df_results = pd.concat([df_results, df_batch], ignore_index=True)\n",
        "        df_results.to_csv(output_file, index=False)\n",
        "    \n",
        "    print(f\"--- EXTRACTION COMPLETE: {output_file} ---\")\n",
        "    return df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 13. Run Extraction\n",
        "\n",
        "df_extracted = run_extraction(\n",
        "    output_file=OUTPUT_FILE,\n",
        "    limit=MAX_DOCUMENTS,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Display sample results\n",
        "print(f\"\\nTotal rows: {len(df_extracted)}\")\n",
        "display(df_extracted.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 14. Quality Report\n",
        "\n",
        "def generate_quality_report(df):\n",
        "    \"\"\"Generate a summary of extraction quality.\"\"\"\n",
        "    print(\"\\n=== EXTRACTION QUALITY REPORT ===\")\n",
        "    print(f\"Total documents: {len(df)}\")\n",
        "    print()\n",
        "    \n",
        "    for q in QUESTION_BANK:\n",
        "        field_id = q[\"id\"]\n",
        "        value_col = f\"{field_id}_value\"\n",
        "        verified_col = f\"{field_id}_evidence_verified\"\n",
        "        \n",
        "        if value_col not in df.columns:\n",
        "            continue\n",
        "        \n",
        "        total = len(df)\n",
        "        found = len(df[~df[value_col].isin([\"NOT_FOUND\", \"NO_CONTENT\", \"JSON_PARSE_ERROR\"])])\n",
        "        if verified_col in df.columns:\n",
        "            verified = df[verified_col].sum() if df[verified_col].dtype == bool else 0\n",
        "        else:\n",
        "            verified = \"N/A\"\n",
        "        \n",
        "        print(f\"{field_id}:\")\n",
        "        print(f\"  Found: {found}/{total} ({100*found/total:.1f}%)\")\n",
        "        print(f\"  Evidence Verified: {verified}\")\n",
        "        print()\n",
        "\n",
        "if not df_extracted.empty:\n",
        "    generate_quality_report(df_extracted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 15. Inspect Specific Results (by row number)\n",
        "\n",
        "ROW_NUMBER = 0  # Change this to inspect different rows\n",
        "\n",
        "if not df_extracted.empty and ROW_NUMBER < len(df_extracted):\n",
        "    row = df_extracted.iloc[ROW_NUMBER]\n",
        "    \n",
        "    print(f\"=== ROW {ROW_NUMBER}: {row['filename']} ===\")\n",
        "    print(f\"CIK: {row['cik']} | Year: {row['year']}\")\n",
        "    print()\n",
        "    \n",
        "    for q in QUESTION_BANK:\n",
        "        field_id = q[\"id\"]\n",
        "        print(f\"--- {field_id} ---\")\n",
        "        print(f\"  Value: {row[f'{field_id}_value']}\")\n",
        "        print(f\"  Evidence: {row[f'{field_id}_evidence']}\")\n",
        "        print(f\"  Verified: {row[f'{field_id}_evidence_verified']}\")\n",
        "        print()\n",
        "else:\n",
        "    print(f\"Row {ROW_NUMBER} not found. DataFrame has {len(df_extracted)} rows.\")# 15. Inspect Specific Results (Optional)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "ROW_TO_INSPECT = 0  # Change this to view different rows\n",
        "if not df_extracted.empty and ROW_TO_INSPECT < len(df_extracted):\n",
        "    row = df_extracted.iloc[ROW_TO_INSPECT]\n",
        "    \n",
        "    print(f\"=== FULL TEXT: {row['filename']} ===\")\n",
        "    print(f\"Length: {len(row['full_text'])} characters\")\n",
        "    print(\"=\" * 60)\n",
        "    print(row['full_text'])\n",
        "else:\n",
        "    print(f\"Row {ROW_TO_INSPECT} not found.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
