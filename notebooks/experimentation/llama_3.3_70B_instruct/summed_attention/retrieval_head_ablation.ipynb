{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "632fb1b7",
   "metadata": {},
   "source": [
    "# Retrieval Head Ablation (Step 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49793a50",
   "metadata": {},
   "source": [
    "### Validate identified retrieval heads via targeted ablation on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch transformers numpy huggingface_hub accelerate bitsandbytes python-dotenv pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c13fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from contextlib import nullcontext\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640546f",
   "metadata": {},
   "source": [
    "#### If using Google Colab: Run The Following Cells (Ignore if using some other environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ecdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c376c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"HF_TOKEN loaded and Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9e3f6",
   "metadata": {},
   "source": [
    "#### If using a high end GPU not from Colab, but from Lambda Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469a7ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=False)\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or globals().get(\"HF_TOKEN\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"HF_TOKEN loaded and Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not found. Add HF_TOKEN=... to your .env file or set it in the environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3a548",
   "metadata": {},
   "source": [
    "#### Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78892b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "TARGET_SEQ_LEN = 7000\n",
    "NEEDLE_DEPTH = 0.5\n",
    "SPLIT = 0.8\n",
    "SPLIT_SEED = 42\n",
    "\n",
    "TASKS = [\n",
    "    {\"id\": \"registrant_name\", \"question\": \"What is the registrant's name?\"},\n",
    "    {\"id\": \"headquarters_city\", \"question\": \"What is the registrant's headquarters city?\"},\n",
    "    {\"id\": \"headquarters_state\", \"question\": \"What is the registrant's headquarters state?\"},\n",
    "    {\"id\": \"incorporation_state\", \"question\": \"What is the registrant's incorporation state?\"},\n",
    "    {\"id\": \"incorporation_year\", \"question\": \"What is the incorporation year?\"},\n",
    "    {\"id\": \"employees_count_total\", \"question\": \"How many total employees does the registrant have?\"},\n",
    "    {\"id\": \"holder_record_amount\", \"question\": \"What is the number of holders of record of the registrant's common stock?\"},\n",
    "    {\"id\": \"employees_count_full_time\", \"question\": \"How many full-time employees does the registrant have?\"},\n",
    "    {\"id\": \"ceo_lastname\", \"question\": \"What is the CEO's last name?\"},\n",
    "]\n",
    "TASK_MAP = {t[\"id\"]: t for t in TASKS}\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"data/clean_ground_truth/cleaned_EDGAR_gt_2-22-2026.csv\"\n",
    "HEADS_JSON_PATH = \"data/retrieval_heads/results/retrieval_heads.json\"\n",
    "ABLATION_OUTPUT_DIR = \"data/retrieval_heads/ablation_results\"\n",
    "\n",
    "# Ablation-specific config\n",
    "ABLATION_K_VALUES = [1, 2, 5, 10, 20]\n",
    "MAX_DECODE_TOKENS = 20\n",
    "\n",
    "# Model loading\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "ATTN_IMPL = \"eager\"\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Ablation K values: {ABLATION_K_VALUES}\")\n",
    "print(f\"Max decode tokens: {MAX_DECODE_TOKENS}\")\n",
    "print(f\"Heads JSON: {HEADS_JSON_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e0b5c",
   "metadata": {},
   "source": [
    "##### Tokenizer & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708145c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {MODEL_ID}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d2c3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=ATTN_IMPL,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {MODEL_ID}\")\n",
    "print(f\"dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Num heads (Q): {model.config.num_attention_heads}\")\n",
    "print(f\"Num KV heads: {model.config.num_key_value_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e73f7",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 -- Load Ablation Split & Identified Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76acc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df = df[df[\"haystack_token_length\"] == TARGET_SEQ_LEN].reset_index(drop=True)\n",
    "\n",
    "_, ablation_df = train_test_split(\n",
    "    df,\n",
    "    test_size=1 - SPLIT,\n",
    "    stratify=df[\"task\"],\n",
    "    random_state=SPLIT_SEED,\n",
    ")\n",
    "ablation_df = ablation_df.reset_index(drop=True)\n",
    "\n",
    "print(f\"Ablation set: {len(ablation_df):,} samples\")\n",
    "print(ablation_df.groupby(\"task\").size().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939ec499",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HEADS_JSON_PATH, \"r\") as f:\n",
    "    heads_data = json.load(f)\n",
    "\n",
    "# Per-task top-K heads as (layer, head) tuples\n",
    "task_head_rankings = {}\n",
    "for task_id, head_list in heads_data[\"tasks\"].items():\n",
    "    task_head_rankings[task_id] = [(h[\"layer\"], h[\"head\"]) for h in head_list]\n",
    "\n",
    "# Shared heads\n",
    "shared_heads = [(h[\"layer\"], h[\"head\"]) for h in heads_data[\"shared_heads\"]]\n",
    "\n",
    "print(f\"Loaded {len(task_head_rankings)} task rankings from {HEADS_JSON_PATH}\")\n",
    "for tid, heads in task_head_rankings.items():\n",
    "    print(f\"  {tid:<35} {len(heads)} heads\")\n",
    "print(f\"Shared heads: {len(shared_heads)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd4f8d",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 -- Core Ablation Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAblationHooks:\n",
    "    \"\"\"Context manager that zeros out selected attention heads before o_proj.\"\"\"\n",
    "\n",
    "    def __init__(self, model, heads: list[tuple[int, int]]):\n",
    "        self.model = model\n",
    "        self.heads = heads\n",
    "        self.handles = []\n",
    "        self.head_dim = model.config.hidden_size // model.config.num_attention_heads\n",
    "\n",
    "        # Group heads by layer for efficient hooking\n",
    "        self.by_layer = {}\n",
    "        for layer_idx, head_idx in heads:\n",
    "            self.by_layer.setdefault(layer_idx, set()).add(head_idx)\n",
    "\n",
    "    def __enter__(self):\n",
    "        for layer_idx, layer_heads in self.by_layer.items():\n",
    "            if layer_idx >= len(self.model.model.layers):\n",
    "                continue\n",
    "            o_proj = self.model.model.layers[layer_idx].self_attn.o_proj\n",
    "\n",
    "            def pre_hook(_module, inputs, heads=sorted(layer_heads)):\n",
    "                x = inputs[0]\n",
    "                if x.ndim != 3:\n",
    "                    return inputs\n",
    "                x = x.clone()\n",
    "                for head_idx in heads:\n",
    "                    start = head_idx * self.head_dim\n",
    "                    end = start + self.head_dim\n",
    "                    x[..., start:end] = 0\n",
    "                return (x,)\n",
    "\n",
    "            self.handles.append(o_proj.register_forward_pre_hook(pre_hook))\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n",
    "        self.handles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c60032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, tokenizer, prompt_ids, max_tokens=MAX_DECODE_TOKENS) -> str:\n",
    "    \"\"\"Prefill + token-by-token greedy decode. No attention output needed.\"\"\"\n",
    "    with torch.inference_mode():\n",
    "        out = model(\n",
    "            input_ids=prompt_ids[:, :-1],\n",
    "            use_cache=True,\n",
    "            return_dict=True,\n",
    "            output_attentions=False,\n",
    "        )\n",
    "        past_kv = out.past_key_values\n",
    "        inp = prompt_ids[:, -1:]\n",
    "        position = prompt_ids.size(1) - 1\n",
    "        device = prompt_ids.device\n",
    "\n",
    "        generated = []\n",
    "        for _ in range(max_tokens):\n",
    "            pos_ids = torch.tensor([[position]], dtype=torch.long, device=device)\n",
    "            out = model(\n",
    "                input_ids=inp,\n",
    "                past_key_values=past_kv,\n",
    "                position_ids=pos_ids,\n",
    "                use_cache=True,\n",
    "                output_attentions=False,\n",
    "                return_dict=True,\n",
    "            )\n",
    "            past_kv = out.past_key_values\n",
    "            next_id = out.logits[:, -1].argmax(dim=-1)\n",
    "            generated.append(next_id.item())\n",
    "            if next_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "            position += 1\n",
    "            inp = next_id.unsqueeze(1)\n",
    "\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2824829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_value(text: str) -> str:\n",
    "    \"\"\"Lowercase, strip whitespace, remove commas, convert number words to digits.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", str(text).replace(\"\\n\", \" \")).strip().lower()\n",
    "    s = s.replace(\",\", \"\")\n",
    "\n",
    "    number_words = {\n",
    "        \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\", \"four\": \"4\",\n",
    "        \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\", \"eight\": \"8\", \"nine\": \"9\",\n",
    "        \"ten\": \"10\", \"eleven\": \"11\", \"twelve\": \"12\", \"thirteen\": \"13\",\n",
    "        \"fourteen\": \"14\", \"fifteen\": \"15\", \"sixteen\": \"16\", \"seventeen\": \"17\",\n",
    "        \"eighteen\": \"18\", \"nineteen\": \"19\", \"twenty\": \"20\", \"thirty\": \"30\",\n",
    "        \"forty\": \"40\", \"fifty\": \"50\", \"sixty\": \"60\", \"seventy\": \"70\",\n",
    "        \"eighty\": \"80\", \"ninety\": \"90\",\n",
    "    }\n",
    "    for word, digit in number_words.items():\n",
    "        s = re.sub(r\"\\b\" + word + r\"\\b\", digit, s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c4e49",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 -- Prompt Building & Evaluation Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_TOKENS = [\"<|begin_of_text|>\", \"<|end_of_text|>\", \"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"]\n",
    "TOKEN_PATTERN = re.compile(\"|\".join(re.escape(t) for t in CONTROL_TOKENS))\n",
    "\n",
    "\n",
    "def find_needle_span(\n",
    "    prompt_ids: list[int],\n",
    "    needle_ids: list[int],\n",
    "    threshold: float = 0.9,\n",
    ") -> tuple[int, int]:\n",
    "    \"\"\"Locate needle tokens inside the full tokenized prompt via sliding window overlap.\"\"\"\n",
    "    span_len = len(needle_ids)\n",
    "    if span_len == 0:\n",
    "        return -1, -1\n",
    "\n",
    "    needle_set = set(needle_ids)\n",
    "\n",
    "    for i in range(len(prompt_ids) - span_len + 1):\n",
    "        window = set(prompt_ids[i : i + span_len])\n",
    "        if len(window & needle_set) / len(needle_set) >= threshold:\n",
    "            return i, i + span_len\n",
    "\n",
    "    return -1, -1\n",
    "\n",
    "\n",
    "def build_prompt(row: pd.Series, task: dict, tokenizer) -> dict:\n",
    "    \"\"\"Construct the prompt with needle inserted at NEEDLE_DEPTH and locate the needle span.\"\"\"\n",
    "    haystack = TOKEN_PATTERN.sub(\"\", row[\"haystack_text\"]).strip()\n",
    "    needle = row[\"needle_sentence\"]\n",
    "\n",
    "    mid = len(haystack) // 2\n",
    "    context = haystack[:mid] + \" \" + needle + \" \" + haystack[mid:]\n",
    "\n",
    "    message = f\"<document>{context}</document>\\nQuestion: {task['question']}\\nAnswer:\"\n",
    "    messages = [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    needle_ids = tokenizer.encode(needle, add_special_tokens=False)\n",
    "    needle_start, needle_end = find_needle_span(input_ids[0].tolist(), needle_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"needle_start\": needle_start,\n",
    "        \"needle_end\": needle_end,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20a013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sample(model, tokenizer, row, task, ablation_heads=None) -> dict:\n",
    "    \"\"\"Build prompt, optionally ablate heads, greedy decode, score.\"\"\"\n",
    "    prompt = build_prompt(row, task, tokenizer)\n",
    "\n",
    "    if prompt[\"needle_start\"] == -1:\n",
    "        return {\"decoded\": \"\", \"value_match\": False, \"skipped\": True}\n",
    "\n",
    "    input_ids = prompt[\"input_ids\"].to(model.device)\n",
    "    hook_ctx = HeadAblationHooks(model, ablation_heads) if ablation_heads else nullcontext()\n",
    "\n",
    "    with hook_ctx:\n",
    "        decoded = greedy_decode(model, tokenizer, input_ids)\n",
    "\n",
    "    ground_truth = row[\"needle_value\"]\n",
    "    value_match = normalize_value(ground_truth) in normalize_value(decoded)\n",
    "\n",
    "    del input_ids\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return {\"decoded\": decoded, \"value_match\": value_match, \"skipped\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e7ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_condition(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_df: pd.DataFrame,\n",
    "    condition_name: str,\n",
    "    ablation_heads=None,\n",
    ") -> dict:\n",
    "    \"\"\"Run evaluation on all rows in eval_df. Returns per-task accuracy dict.\"\"\"\n",
    "    results = {}\n",
    "    total = len(eval_df)\n",
    "\n",
    "    for i, (_, row) in enumerate(eval_df.iterrows(), start=1):\n",
    "        task_id = row[\"task\"]\n",
    "        task = TASK_MAP[task_id]\n",
    "\n",
    "        if task_id not in results:\n",
    "            results[task_id] = {\"attempts\": 0, \"matches\": 0, \"samples\": []}\n",
    "\n",
    "        sample_result = evaluate_sample(model, tokenizer, row, task, ablation_heads)\n",
    "\n",
    "        if sample_result[\"skipped\"]:\n",
    "            continue\n",
    "\n",
    "        results[task_id][\"attempts\"] += 1\n",
    "        if sample_result[\"value_match\"]:\n",
    "            results[task_id][\"matches\"] += 1\n",
    "        results[task_id][\"samples\"].append(sample_result)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[{condition_name}] {i}/{total}\")\n",
    "\n",
    "    # Compute accuracy per task\n",
    "    for task_id in results:\n",
    "        r = results[task_id]\n",
    "        r[\"accuracy\"] = r[\"matches\"] / max(1, r[\"attempts\"])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7af599",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 -- Experiment 1: Baseline Evaluation (No Ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff52190",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(ABLATION_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "baseline_results = evaluate_condition(model, tokenizer, ablation_df, \"baseline\")\n",
    "\n",
    "print(\"\\nBaseline results:\")\n",
    "for task_id, r in baseline_results.items():\n",
    "    print(f\"  {task_id:<35} {r['matches']}/{r['attempts']}  ({r['accuracy']:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe5f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ABLATION_OUTPUT_DIR, \"baseline_results.json\"), \"w\") as f:\n",
    "    # Strip sample-level decoded text for cleaner JSON\n",
    "    export = {tid: {\"attempts\": r[\"attempts\"], \"matches\": r[\"matches\"], \"accuracy\": r[\"accuracy\"]} for tid, r in baseline_results.items()}\n",
    "    json.dump(export, f, indent=2)\n",
    "    print(f\"Saved baseline results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b18818",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 -- Experiment 2: Within-Task Ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70502af",
   "metadata": {},
   "source": [
    "For each task, ablate that task's own top-K heads at increasing K values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991e459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_task_results = {}\n",
    "\n",
    "for task_id in TASK_MAP:\n",
    "    task_df = ablation_df[ablation_df[\"task\"] == task_id].reset_index(drop=True)\n",
    "    if len(task_df) == 0:\n",
    "        print(f\"SKIP {task_id} -- no ablation samples\")\n",
    "        continue\n",
    "\n",
    "    available_heads = task_head_rankings.get(task_id, [])\n",
    "    if not available_heads:\n",
    "        print(f\"SKIP {task_id} -- no ranked heads\")\n",
    "        continue\n",
    "\n",
    "    within_task_results[task_id] = {}\n",
    "\n",
    "    for k in ABLATION_K_VALUES:\n",
    "        heads_to_ablate = available_heads[:k]\n",
    "        if not heads_to_ablate:\n",
    "            break\n",
    "\n",
    "        condition_name = f\"within_{task_id}_k{k}\"\n",
    "        results = evaluate_condition(model, tokenizer, task_df, condition_name, ablation_heads=heads_to_ablate)\n",
    "\n",
    "        r = results.get(task_id, {\"attempts\": 0, \"matches\": 0, \"accuracy\": 0.0})\n",
    "        within_task_results[task_id][k] = {\n",
    "            \"k\": k,\n",
    "            \"heads_ablated\": len(heads_to_ablate),\n",
    "            \"attempts\": r[\"attempts\"],\n",
    "            \"matches\": r[\"matches\"],\n",
    "            \"accuracy\": r[\"accuracy\"],\n",
    "        }\n",
    "\n",
    "        print(f\"  {task_id} k={k}: {r['matches']}/{r['attempts']}  ({r['accuracy']:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b174a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ABLATION_OUTPUT_DIR, \"within_task_ablation.json\"), \"w\") as f:\n",
    "    json.dump(within_task_results, f, indent=2)\n",
    "    print(\"Saved within-task ablation results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8a04d",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7 -- Experiment 3: Across-Task Ablation (Shared Heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272a6b34",
   "metadata": {},
   "source": [
    "Ablate the global shared retrieval heads and measure accuracy across all tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1079d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "across_task_results = {}\n",
    "\n",
    "for k in ABLATION_K_VALUES:\n",
    "    heads_to_ablate = shared_heads[:k]\n",
    "    if not heads_to_ablate:\n",
    "        print(f\"Only {len(shared_heads)} shared heads available, stopping at k={k}\")\n",
    "        break\n",
    "\n",
    "    condition_name = f\"across_task_k{k}\"\n",
    "    results = evaluate_condition(model, tokenizer, ablation_df, condition_name, ablation_heads=heads_to_ablate)\n",
    "\n",
    "    total_attempts = sum(r[\"attempts\"] for r in results.values())\n",
    "    total_matches  = sum(r[\"matches\"] for r in results.values())\n",
    "    overall_acc    = total_matches / max(1, total_attempts)\n",
    "\n",
    "    across_task_results[k] = {\n",
    "        \"k\": k,\n",
    "        \"heads_ablated\": len(heads_to_ablate),\n",
    "        \"overall_accuracy\": overall_acc,\n",
    "        \"total_attempts\": total_attempts,\n",
    "        \"total_matches\": total_matches,\n",
    "        \"per_task\": {tid: {\"attempts\": r[\"attempts\"], \"matches\": r[\"matches\"], \"accuracy\": r[\"accuracy\"]} for tid, r in results.items()},\n",
    "    }\n",
    "\n",
    "    print(f\"k={k}: {total_matches}/{total_attempts}  ({overall_acc:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605af181",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ABLATION_OUTPUT_DIR, \"across_task_ablation.json\"), \"w\") as f:\n",
    "    json.dump(across_task_results, f, indent=2)\n",
    "    print(\"Saved across-task ablation results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b4999",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8 -- Random Head Ablation Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da510524",
   "metadata": {},
   "source": [
    "Ablate randomly selected heads to prove the accuracy drop is specific to retrieval heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673935ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = model.config.num_hidden_layers\n",
    "num_heads = model.config.num_attention_heads\n",
    "\n",
    "# All possible (layer, head) pairs, excluding the shared retrieval heads\n",
    "shared_set = set(shared_heads)\n",
    "all_heads = [(l, h) for l in range(num_layers) for h in range(num_heads) if (l, h) not in shared_set]\n",
    "\n",
    "random.seed(SPLIT_SEED)\n",
    "random_pool = random.sample(all_heads, min(max(ABLATION_K_VALUES), len(all_heads)))\n",
    "\n",
    "random_ablation_results = {}\n",
    "\n",
    "for k in ABLATION_K_VALUES:\n",
    "    heads_to_ablate = random_pool[:k]\n",
    "\n",
    "    condition_name = f\"random_k{k}\"\n",
    "    results = evaluate_condition(model, tokenizer, ablation_df, condition_name, ablation_heads=heads_to_ablate)\n",
    "\n",
    "    total_attempts = sum(r[\"attempts\"] for r in results.values())\n",
    "    total_matches  = sum(r[\"matches\"] for r in results.values())\n",
    "    overall_acc    = total_matches / max(1, total_attempts)\n",
    "\n",
    "    random_ablation_results[k] = {\n",
    "        \"k\": k,\n",
    "        \"heads_ablated\": [(l, h) for l, h in heads_to_ablate],\n",
    "        \"overall_accuracy\": overall_acc,\n",
    "        \"total_attempts\": total_attempts,\n",
    "        \"total_matches\": total_matches,\n",
    "    }\n",
    "\n",
    "    print(f\"random k={k}: {total_matches}/{total_attempts}  ({overall_acc:.1%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d0a35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(ABLATION_OUTPUT_DIR, \"random_ablation_control.json\"), \"w\") as f:\n",
    "    json.dump(random_ablation_results, f, indent=2)\n",
    "    print(\"Saved random ablation control results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dc2634",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9 -- Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline accuracy per task\n",
    "baseline_acc = {tid: r[\"accuracy\"] for tid, r in baseline_results.items()}\n",
    "baseline_overall = sum(r[\"matches\"] for r in baseline_results.values()) / max(1, sum(r[\"attempts\"] for r in baseline_results.values()))\n",
    "\n",
    "print(f\"Baseline overall accuracy: {baseline_overall:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84acbbd0",
   "metadata": {},
   "source": [
    "##### Chart 1: Across-task ablation vs random ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7522ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "k_vals = sorted([k for k in across_task_results.keys()])\n",
    "retrieval_acc = [across_task_results[k][\"overall_accuracy\"] for k in k_vals]\n",
    "random_acc = [random_ablation_results[k][\"overall_accuracy\"] for k in k_vals if k in random_ablation_results]\n",
    "\n",
    "ax.axhline(y=baseline_overall, color=\"green\", linestyle=\"--\", label=\"Baseline\")\n",
    "ax.plot(k_vals, retrieval_acc, \"ro-\", label=\"Retrieval heads ablated\")\n",
    "ax.plot(k_vals[:len(random_acc)], random_acc, \"bs-\", label=\"Random heads ablated\")\n",
    "\n",
    "ax.set_xlabel(\"Number of heads ablated (K)\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Shared Retrieval Head Ablation vs Random Control\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(ABLATION_OUTPUT_DIR, \"across_task_vs_random.png\")\n",
    "plt.savefig(fig_path, dpi=150)\n",
    "plt.show()\n",
    "print(f\"Saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3461f",
   "metadata": {},
   "source": [
    "##### Chart 2: Within-task ablation per task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494b9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for task_id, k_results in within_task_results.items():\n",
    "    k_vals = sorted(k_results.keys())\n",
    "    acc_vals = [k_results[k][\"accuracy\"] for k in k_vals]\n",
    "    ax.plot(k_vals, acc_vals, \"o-\", label=task_id)\n",
    "\n",
    "ax.axhline(y=baseline_overall, color=\"green\", linestyle=\"--\", label=\"Baseline (overall)\")\n",
    "\n",
    "ax.set_xlabel(\"Number of heads ablated (K)\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Within-Task Ablation: Accuracy vs K\")\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_path = os.path.join(ABLATION_OUTPUT_DIR, \"within_task_ablation.png\")\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"Saved: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2d62e",
   "metadata": {},
   "source": [
    "##### Summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9eccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "\n",
    "for k in sorted(across_task_results.keys()):\n",
    "    retrieval = across_task_results[k][\"overall_accuracy\"]\n",
    "    rand = random_ablation_results.get(k, {}).get(\"overall_accuracy\", None)\n",
    "    summary_rows.append({\n",
    "        \"k\": k,\n",
    "        \"baseline\": baseline_overall,\n",
    "        \"retrieval_ablated\": retrieval,\n",
    "        \"random_ablated\": rand,\n",
    "        \"delta_retrieval\": retrieval - baseline_overall,\n",
    "        \"delta_random\": (rand - baseline_overall) if rand else None,\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
