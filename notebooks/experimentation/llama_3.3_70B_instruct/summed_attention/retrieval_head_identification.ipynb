{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5e324d",
   "metadata": {},
   "source": [
    "# Identify Retrieval Heads Using Sum Attention Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c1947",
   "metadata": {},
   "source": [
    "### Retrieval Head Identification Using Sum Attention Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a1f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch, transformers, numpy, huggingface_hub accelerate bitsandbytes dotenv, pandas, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedfd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "import pandas as pd  \n",
    "import accelerate \n",
    "from huggingface_hub import login  \n",
    "from dotenv import load_dotenv  \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572bdb1",
   "metadata": {},
   "source": [
    "#### If using Google Colab: Run The Following Cells (Ignore if using some other environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03861c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata \n",
    "\n",
    "HF_TOKEN = userdata.get('HF_TOKEN') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa45ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"HF_TOKEN loaded and Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922277b",
   "metadata": {},
   "source": [
    "#### If using a high end GPU not from Colab, but from Lambda Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a8309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HF_TOKEN from a local .env file (works in Lambda Labs / venv / local runs)\n",
    "load_dotenv(override=False)\n",
    "\n",
    "# Prefer token from .env or environment; fall back to an existing HF_TOKEN variable (e.g., Colab cell)\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") or globals().get(\"HF_TOKEN\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"HF_TOKEN loaded and Hugging Face login successful.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not found. Add HF_TOKEN=... to your .env file or set it in the environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a9eb4",
   "metadata": {},
   "source": [
    "#### Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac1bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Model\n",
    "MODEL_ID = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "# Data and Needle Stuff\n",
    "\n",
    "TARGET_SEQ_LEN = 7000\n",
    "\n",
    "NEEDLE_DEPTH = 0.5\n",
    "SPLIT = 0.8\n",
    "\n",
    "# No magic numbers, for reproduction\n",
    "SPLIT_SEED = 42\n",
    "TOP_K_HEADS = 20\n",
    "\n",
    "TASKS = [\n",
    "    {\"id\": \"registrant_name\", \"question\": \"What is the registrant's name?\"},\n",
    "    {\"id\": \"headquarters_city\", \"question\": \"What is the registrant's headquarters city?\"},\n",
    "    {\"id\": \"headquarters_state\", \"question\": \"What is the registrant's headquarters state?\"}, \n",
    "    # incorporation state and year should (would it be better to ask for the current state and year instead?)\n",
    "    {\"id\": \"incorporation_state\", \"question\": \"What is the registrant's incorporation state?\"},\n",
    "    {\"id\": \"incorporation_year\", \"question\": \"What is the incorporation year?\"},  \n",
    "\n",
    "    {\"id\": \"employees_count_total\", \"question\": \"How many total employees does the registrant have?\"},\n",
    "    {\"id\": \"holder_record_amount\", \"question\": \"What is the number of holders of record of the registrant's common stock?\"},\n",
    "    {\"id\": \"employees_count_full_time\", \"question\": \"How many full-time employees does the registrant have?\"},\n",
    "    {\"id\": \"ceo_lastname\", \"question\": \"What is the CEO's last name?\"},\n",
    "] \n",
    "\n",
    "TASK_MAP = {t[\"id\"]: t for t in TASKS}\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = \"data/clean_ground_truth/cleaned_EDGAR_gt_2-22-2026.csv\"\n",
    "RAW_OUTPUT_DIR = \"data/retrieval_heads/raw\" \n",
    "RESULTS_OUTPUT_DIR = \"data/retrieval_heads/results\"\n",
    "\n",
    "# Model Loading Options\n",
    "TORCH_DTYPE = torch.bfloat16\n",
    "ATTN_IMPL = \"eager\"\n",
    "\n",
    "# Formatted by AI to see configs and all that\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Model: {MODEL_ID}\")\n",
    "print(f\"Seq length : {TARGET_SEQ_LEN} tokens\")\n",
    "print(f\"Needle depth : {NEEDLE_DEPTH * 100:.0f}%\")\n",
    "print(f\"ID split: {SPLIT * 100:.0f}% (seed={SPLIT_SEED})\")\n",
    "print(f\"Top-K heads: {TOP_K_HEADS}\")\n",
    "print(f\"Tasks: {len(TASKS)}\")\n",
    "print(f\"dtype: {TORCH_DTYPE}\")\n",
    "print(f\"attn_impl: {ATTN_IMPL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b32809b",
   "metadata": {},
   "source": [
    "\n",
    "##### Tokenizer & Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeed658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {MODEL_ID}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length:,}\")\n",
    "print(f\"Pad token: '{tokenizer.pad_token}' (id={tokenizer.pad_token_id})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8194a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=ATTN_IMPL,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Formatted by AI to see configs and all that\n",
    "print(f\"Model loaded: {MODEL_ID}\")\n",
    "print(f\"dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Num layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"Num heads (Q): {model.config.num_attention_heads}\")\n",
    "print(f\"Num KV heads: {model.config.num_key_value_heads}\")\n",
    "print(f\"Total heads: {model.config.num_hidden_layers * model.config.num_attention_heads:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d29fd",
   "metadata": {},
   "source": [
    "#### Set up Dataset and splitting and all that jazz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1fa0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "identification_df, ablation_df = train_test_split(\n",
    "    df,\n",
    "    test_size=1 - SPLIT,\n",
    "    stratify=df[\"task\"],\n",
    "    random_state=SPLIT_SEED,\n",
    ")\n",
    "\n",
    "identification_df = identification_df.reset_index(drop=True)\n",
    "ablation_df = ablation_df.reset_index(drop=True)\n",
    "\n",
    "# Asked by AI\n",
    "print(f\"Identification set : {len(identification_df):,} samples ({len(identification_df)/len(df):.0%})\")\n",
    "print(f\"Ablation set : {len(ablation_df):,} samples ({len(ablation_df)/len(df):.0%})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002abb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame({\n",
    "    \"total\": df.groupby(\"task\").size(),\n",
    "    \"identification\": identification_df.groupby(\"task\").size(),\n",
    "    \"ablation\": ablation_df.groupby(\"task\").size(),\n",
    "})\n",
    "counts[\"id_%\"] = (counts[\"identification\"] / counts[\"total\"] * 100).round(1)\n",
    "counts[\"abl_%\"] = (counts[\"ablation\"] / counts[\"total\"] * 100).round(1)\n",
    "\n",
    "print(counts.to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e511e26",
   "metadata": {},
   "source": [
    "#### Building Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038aa544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_needle_span(\n",
    "    prompt_ids: list[int],\n",
    "    needle_ids: list[int],\n",
    "    threshold: float = 0.9,\n",
    ") -> tuple[int, int]:\n",
    "    \"\"\"Locate needle tokens inside the full tokenized prompt via sliding window overlap.\"\"\"\n",
    "    span_len = len(needle_ids)\n",
    "    if span_len == 0:\n",
    "        return -1, -1\n",
    "\n",
    "    needle_set = set(needle_ids)\n",
    "\n",
    "    for i in range(len(prompt_ids) - span_len + 1):\n",
    "        window = set(prompt_ids[i : i + span_len])\n",
    "        if len(window & needle_set) / len(needle_set) >= threshold:\n",
    "            return i, i + span_len\n",
    "\n",
    "    return -1, -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fa37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# It is only <|begin_of_text|> that seems to exist, but I removed all control tokens just to be safe.\n",
    "CONTROL_TOKENS = [\"<|begin_of_text|>\", \"<|end_of_text|>\", \"<|eot_id|>\", \"<|start_header_id|>\", \"<|end_header_id|>\"]\n",
    "TOKEN_PATTERN = re.compile(\"|\".join(re.escape(t) for t in CONTROL_TOKENS))\n",
    "\n",
    "def build_prompt(row: pd.Series, task: dict, tokenizer) -> dict: \n",
    "    \"\"\"Construct the full prompt with the needle sentence inserted, and locate the needle span in the tokenized input.\"\"\" \n",
    "    \n",
    "    haystack = TOKEN_PATTERN.sub(\"\", row[\"haystack_text\"]).strip()\n",
    "    needle = row[\"needle_sentence\"]\n",
    "\n",
    "    \n",
    "    mid = len(haystack) // 2\n",
    "    context = haystack[:mid] + \" \" + needle + \" \" + haystack[mid:]\n",
    "\n",
    "    message = f\"<document>{context}</document>\\nQuestion: {task['question']}\\nAnswer:\"\n",
    "    messages = [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Locate needle span in the tokenized prompt\n",
    "    needle_ids = tokenizer.encode(needle, add_special_tokens=False)\n",
    "    needle_start, needle_end = find_needle_span(input_ids[0].tolist(), needle_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"needle_ids\": needle_ids,\n",
    "        \"needle_start\": needle_start,\n",
    "        \"needle_end\": needle_end,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b6bf40",
   "metadata": {},
   "source": [
    "#### Just some validation of the prompt building logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515eef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "for i in range(5): \n",
    "    random_number = random.randint(0, 200)\n",
    "    sample_row = identification_df.iloc[0]\n",
    "    sample_task = TASK_MAP[sample_row[\"task\"]]\n",
    "    result = build_prompt(sample_row, sample_task, tokenizer)\n",
    "\n",
    "    print(f\"input_ids shape: {result['input_ids'].shape}\")\n",
    "    print(f\"needle_start: {result['needle_start']}\")\n",
    "    print(f\"needle_end: {result['needle_end']}\")\n",
    "    print(f\"needle span len: {result['needle_end'] - result['needle_start']} tokens\")\n",
    "    print(f\"total tokens: {result['input_ids'].shape[1]}\")\n",
    "    print()\n",
    "\n",
    "    decoded = tokenizer.decode(result[\"input_ids\"][0, result[\"needle_start\"]:result[\"needle_end\"]])\n",
    "    print(f\"decoded needle:\\n{decoded}\")\n",
    "    print(f\"\\noriginal needle:\\n{sample_row['needle_sentence']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa059ffa",
   "metadata": {},
   "source": [
    "#### Run Model & Extract the Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8973f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sum_attn(model, prompt_inputs:dict) -> np.ndarray: \n",
    "    \"\"\" \n",
    "    Compute the summed attention scores for each head across all layers, given the prompt inputs.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Just make this a bit more cleaner and easier to read\n",
    "    input_ids = prompt_inputs[\"input_ids\"].to(model.device)\n",
    "    needle_start = prompt_inputs[\"needle_start\"]\n",
    "    needle_end = prompt_inputs[\"needle_end\"]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        prefill = model(\n",
    "            input_ids=input_ids[:, :-1],\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        decode = model(\n",
    "            input_ids=input_ids[:, -1:],\n",
    "            past_key_values=prefill.past_key_values,\n",
    "            use_cache=False,\n",
    "            output_attentions=True,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    num_heads = model.config.num_attention_heads\n",
    "    scores = np.zeros((num_layers, num_heads), dtype=np.float32)\n",
    "\n",
    "    for layer_idx, layer_attn in enumerate(decode.attentions):\n",
    "        attn = layer_attn[0, :, 0, :].float().cpu().numpy() \n",
    "\n",
    "        # scores[layer_idx] = attn[:, needle_start:needle_end].mean(axis=1)  \n",
    "        # we could use mean instead of sum to normalize \n",
    "        # for different needle lengths\n",
    "        scores[layer_idx] = attn[:, needle_start:needle_end].sum(axis=1)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81d1a4",
   "metadata": {},
   "source": [
    "Go grab scores for every task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeac196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(RAW_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "skipped = 0\n",
    "total = len(identification_df)\n",
    "\n",
    "for index, row in identification_df.iterrows():\n",
    "    task = TASK_MAP[row[\"task\"]]\n",
    "    prompt = build_prompt(row, task, tokenizer)\n",
    "\n",
    "    if prompt[\"needle_start\"] == -1:\n",
    "        print(f\"[{index}/{total}] SKIP — needle not found ({row['filename']})\")\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    scores = compute_sum_attn(model, prompt)  # (num_layers, num_heads)\n",
    "\n",
    "    # Save immediately — filename encodes everything you need later\n",
    "    filename = f\"{row['filename'].replace('.txt', '')}_{task['id']}.npy\"\n",
    "    np.save(os.path.join(RAW_OUTPUT_DIR, filename), scores)\n",
    "\n",
    "    print(f\"[{index}/{total}] saved {filename}\") \n",
    "    \n",
    "    del prompt, scores\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nDone. {total - skipped}/{total} saved, {skipped} skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c694cf3",
   "metadata": {},
   "source": [
    "#### Get mean scores across tasks and identify top heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_mean_scores = {} \n",
    "\n",
    "for task_id in TASK_MAP:\n",
    "    files = [f for f in os.listdir(RAW_OUTPUT_DIR) if f.endswith(f\"_{task_id}.npy\")]  \n",
    "    if not files:\n",
    "        print(f\"WARNING: no files found for task '{task_id}' — skipping\")\n",
    "        continue\n",
    "    stacked = np.stack([np.load(os.path.join(RAW_OUTPUT_DIR, f)) for f in files])  \n",
    "    task_mean_scores[task_id] = stacked.mean(axis=0)  \n",
    "    print(f\"{task_id:<35} {len(files):>4} samples -> mean shape {task_mean_scores[task_id].shape}\") \n",
    "\n",
    "print(f\"\\n{len(task_mean_scores)} tasks aggregated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb2d8a2",
   "metadata": {},
   "source": [
    "#### Ranking top-k heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea66632",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(RESULTS_OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119f13c6",
   "metadata": {},
   "source": [
    "##### General top-k heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29665cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_top_heads = {} \n",
    "\n",
    "for task_id, mean_scores in task_mean_scores.items():\n",
    "    flat = mean_scores.flatten()  \n",
    "    top_flat = np.argsort(flat)[::-1][:TOP_K_HEADS]  \n",
    "\n",
    "    num_heads = mean_scores.shape[1]\n",
    "    heads = [\n",
    "        {\n",
    "            \"layer\": int(idx // num_heads),\n",
    "            \"head\": int(idx % num_heads),\n",
    "            \"score\": float(flat[idx]),\n",
    "        }\n",
    "        for idx in top_flat\n",
    "    ] \n",
    "\n",
    "    task_top_heads[task_id] = heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268082b4",
   "metadata": {},
   "source": [
    "#### Cross-task shared heads (intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "\n",
    "# Instead of dealing with the inserection with ALL tasks, I decided\n",
    "# that it would make more sense that given, hey this head intersects  \n",
    "# with 7 or more tasks, I think it could be considered a \"general\" \n",
    "# retrieval head\n",
    "\n",
    "all_top_heads = []\n",
    "for heads in task_top_heads.values():\n",
    "    all_top_heads.extend([(h[\"layer\"], h[\"head\"]) for h in heads]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d6a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_counts = Counter(all_top_heads) \n",
    "MIN_TASKS = 7  \n",
    "shared_head_pairs = [head for head, count in head_counts.items() if count >= MIN_TASKS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9172763",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_heads = []\n",
    "for layer, head in shared_head_pairs:\n",
    "    freq = head_counts[(layer, head)]\n",
    "    avg_score = float(np.mean([\n",
    "        task_mean_scores[tid][layer, head]\n",
    "        for tid in task_mean_scores\n",
    "    ])) \n",
    "\n",
    "    shared_heads.append({\n",
    "        \"layer\": layer, \n",
    "        \"head\": head, \n",
    "        \"avg_score\": avg_score,\n",
    "        \"task_frequency\": freq\n",
    "    })\n",
    "\n",
    "shared_heads.sort(key=lambda x: (x[\"task_frequency\"], x[\"avg_score\"]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65517405",
   "metadata": {},
   "source": [
    "#### Exporting/Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2530e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    \"model\": MODEL_ID,\n",
    "    \"top_k\": TOP_K_HEADS,\n",
    "    \"tasks\": task_top_heads,\n",
    "    \"shared_heads\": shared_heads,\n",
    "}\n",
    "out_path = os.path.join(RESULTS_OUTPUT_DIR, \"retrieval_heads.json\")\n",
    "\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Saved at: {out_path}\")\n",
    "print(\n",
    "    f\"\\nShared retrieval heads across all {len(task_mean_scores)} tasks ({len(shared_heads)} found):\"\n",
    ") \n",
    "\n",
    "for h in shared_heads:\n",
    "    print(\n",
    "        f\"Layer {h['layer']:>2}     Head {h['head']:>2}     avg_score={h['avg_score']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda7f90",
   "metadata": {},
   "source": [
    "#### Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Per-task heatmaps (layer × head mean score)\n",
    "for task_id, mean_scores in task_mean_scores.items():\n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    im = ax.imshow(mean_scores, aspect=\"auto\", cmap=\"viridis\")\n",
    "\n",
    "    ax.set_title(f\"Sum Attention Scores — {task_id}\", fontsize=14)\n",
    "    ax.set_xlabel(\"Head\")\n",
    "    ax.set_ylabel(\"Layer\")\n",
    "    plt.colorbar(im, ax=ax, label=\"Mean Sum Attention\")\n",
    "\n",
    "    for h in task_top_heads[task_id]:\n",
    "        ax.plot(h[\"head\"], h[\"layer\"], \"r+\", markersize=8, markeredgewidth=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(RESULTS_OUTPUT_DIR, f\"heatmap_{task_id}.png\")\n",
    "    plt.savefig(fig_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved at: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b02c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Shared head distribution across layers\n",
    "if shared_heads:\n",
    "    layers = [h[\"layer\"] for h in shared_heads]\n",
    "    heads  = [h[\"head\"]  for h in shared_heads]\n",
    "    scores = [h[\"avg_score\"] for h in shared_heads]\n",
    "\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    num_heads  = model.config.num_attention_heads\n",
    "    grid = np.zeros((num_layers, num_heads))\n",
    "    for h in shared_heads:\n",
    "        grid[h[\"layer\"], h[\"head\"]] = h[\"avg_score\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    im = ax.imshow(grid, aspect=\"auto\", cmap=\"hot\")\n",
    "    ax.set_title(\"Shared Retrieval Heads — All Tasks\", fontsize=14)\n",
    "    ax.set_xlabel(\"Head\")\n",
    "    ax.set_ylabel(\"Layer\")\n",
    "    plt.colorbar(im, ax=ax, label=\"Avg Sum Attention Score\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(RESULTS_OUTPUT_DIR, \"shared_retrieval_heads.png\")\n",
    "    plt.savefig(fig_path, dpi=150)\n",
    "    plt.show()\n",
    "    print(f\"Saved at: {fig_path}\")\n",
    "else:\n",
    "    print(\"No shared heads found across all tasks, consider relaxing TOP_K_HEADS.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
