{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e83b61a",
   "metadata": {},
   "source": [
    "# Edgar Testing No Context Recall Experiment In Different Years \n",
    "\n",
    "**Goal:** To evaluate the performance of large language model ins recalling ground truth (Extractive QA) given no context beyond registrant name and  across different years in the Edgar Dataset.  \n",
    "\n",
    "Should be able to: \n",
    "- Compare performance across different years (Being able to pick specific years to compare) \n",
    "- Compare performance between different models (Qwen, Llama, etc) \n",
    "- Compare performance between different questions (state of incorporation, year of incorporation, year, etc) \n",
    "- Compare performance with no context vs with context \n",
    "- Visualize results (simple bar charts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "#%env HF_TOKEN=your_token\n",
    "#!pip install -q datasets torch pandas tqdm thefuzz python-Levenshtein transformers huggingface_hub accelerate bitsandbytes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, logging, BitsAndBytesConfig\n",
    "from huggingface_hub import login \n",
    "\n",
    "logging.set_verbosity_error() \n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Setup device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "login(token=os.environ[\"HF_TOKEN\"]) # or login(token=\"your_token\")   \n",
    "\n",
    "print(f\"Running on: {DEVICE}\")\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fab42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Configuration\n",
    "# Modify these settings to change the experiment focus\n",
    "\n",
    "CONFIG = {\n",
    "    # Resources \n",
    "    # Test with these models Qwen/Qwen2.5-7B-Instruct, meta-llama/Meta-Llama-3-8B-Instruct, meta-llama/Llama-3.1-8B-Instruct, or some other huggingface model. \n",
    "    \"model_id\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"output_file\": \"experiment_results_qwen_nocontext_1993.csv\",\n",
    "    \n",
    "    # Experiment Mode\n",
    "    # 'NO_CONTEXT': Ask model about specific company/year facts using only its internal weights.\n",
    "    # 'CONTEXT': Provide full 10-K text (Extractive QA).\n",
    "    \"mode\": \"NO_CONTEXT\", \n",
    "    \n",
    "    # Ground Truth Data\n",
    "    # Path to a CSV containing ground truth data.\n",
    "    \"local_data_path\": \"../data/ground_truth/v2_250_(1993)_(1-18-2025).csv\", \n",
    "    \n",
    "    # Filters\n",
    "    \"target_years\": [1993], \n",
    "    \"limit_samples\": 250, # Set None for all\n",
    "    \n",
    "    # Generation\n",
    "    \"generation_params\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"do_sample\": False, # Deterministic\n",
    "        \"temperature\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configured for Mode: {CONFIG['mode']} on Years: {CONFIG['target_years']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load da Data\n",
    "def build_experiment_dataset(config):\n",
    "    \"\"\"\n",
    "    Creates a generator that yields:\n",
    "    (doc_from_hf, gt_row_from_csv)\n",
    "    \n",
    "    This ensures we have both the full text (from HF) and the verified truth labels (from CSV).\n",
    "    \"\"\"\n",
    "    # 1. Load Ground Truth CSV to memory (contains labels + filenames)\n",
    "    if not os.path.exists(config[\"local_data_path\"]):\n",
    "        raise FileNotFoundError(f\"GT file not found: {config['local_data_path']}\")\n",
    "        \n",
    "    df_gt = pd.read_csv(config[\"local_data_path\"])\n",
    "    \n",
    "    # Filter by Year if specified\n",
    "    if config[\"target_years\"]:\n",
    "        df_gt = df_gt[df_gt[\"year\"].isin(config[\"target_years\"])]\n",
    "        \n",
    "    # Create lookup map: filename -> row\n",
    "    gt_lookup = df_gt.set_index(\"filename\").to_dict(\"index\")\n",
    "    target_filenames = set(gt_lookup.keys())\n",
    "    \n",
    "    print(f\"Loaded GT CSV. Targets: {len(target_filenames)} documents.\")\n",
    "\n",
    "    # 2. Stream HF Dataset to match filenames\n",
    "    dataset = load_dataset(\n",
    "        \"c3po-ai/edgar-corpus\",\n",
    "        \"default\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    "    \n",
    "    matched_count = 0\n",
    "    limit = config.get(\"limit_samples\")\n",
    "    \n",
    "    for doc in dataset:\n",
    "        fname = doc.get(\"filename\")\n",
    "        if fname in target_filenames:\n",
    "            # combine doc (text) with gt_row (labels)\n",
    "            gt_data = gt_lookup[fname]\n",
    "            yield doc, gt_data\n",
    "            \n",
    "            matched_count += 1\n",
    "            if limit and matched_count >= limit:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model(config):\n",
    "    \"\"\"Loads the model and tokenizer.\"\"\"\n",
    "    model_id = config[\"model_id\"]\n",
    "    \n",
    "    print(f\"Loading model: {model_id}...\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_type=torch.float16\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded.\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e304dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Bank & Prompt Generation\n",
    "\n",
    "SECTION_KEYS = [\n",
    "    \"section_1\", \"section_1A\", \"section_1B\", \"section_2\", \"section_3\",\n",
    "    \"section_4\", \"section_5\", \"section_6\", \"section_7\", \"section_7A\",\n",
    "    \"section_8\", \"section_9\", \"section_9A\", \"section_9B\", \"section_10\",\n",
    "    \"section_11\", \"section_12\", \"section_13\", \"section_14\", \"section_15\"\n",
    "]\n",
    "\n",
    "GT_COLUMN_MAP = { \n",
    "    \"headquarters_city\": \"headquarters_city_truth\",\n",
    "    \"headquarters_state\": \"headquarters_state_truth\",\n",
    "    \"incorporation_state\": \"original_Inc_state_truth\",\n",
    "    \"incorporation_year\": \"original_Inc_year_truth\",\n",
    "    \"employee_count\": \"employee_count_truth\",    \n",
    "    \"holder_record_amount\": \"holder_record_amount_truth\"\n",
    "}\n",
    "\n",
    "QUESTION_BANK = [\n",
    "    {\"id\": \"headquarters_state\", \"prompt\": \"What U.S. state is the location of the registrant's principal executive offices?\"},\n",
    "    {\"id\": \"incorporation_state\", \"prompt\": \"What is the registrant's state of incorporation?\"},\n",
    "    {\"id\": \"incorporation_year\", \"prompt\": \"What is the registrant's year of incorporation?\"},\n",
    "    {\"id\": \"employee_count\", \"prompt\": \"What is the number of employees that the registrant has?\"},  \n",
    "    {\"id\": \"headquarters_city\", \"prompt\": \"What city is explicitly stated as the location of the registrant's principal executive offices?\"}, \n",
    "    {\"id\": \"holder_record_amount\", \"prompt\": \"What is the number of holders of record of the registrant's common stock?\"}\n",
    "]\n",
    "\n",
    "def build_full_context(doc):\n",
    "    \"\"\"Concatenates all non-empty sections from the document.\"\"\"\n",
    "    parts = [\n",
    "        f\"\\n\\n--- [{key.upper()}] ---\\n\\n{doc.get(key, '')}\"\n",
    "        for key in SECTION_KEYS if doc.get(key, \"\").strip()\n",
    "    ]\n",
    "    return \"\".join(parts)\n",
    "\n",
    "def format_prompt(doc, gt_data, question_item, mode, tokenizer):\n",
    "    \"\"\"Builds the chat template prompt based on mode.\"\"\"\n",
    "    q_text = question_item[\"prompt\"]\n",
    "\n",
    "    if mode == \"NO_CONTEXT\":\n",
    "        company = gt_data.get(\"registrant_name_truth\", \"the company\")\n",
    "        year = gt_data.get(\"year\", \"N/A\")\n",
    "        \n",
    "        user_content = (\n",
    "            f\"Answer based on your internal knowledge.\\n\"\n",
    "            f\"Context: {year} SEC 10-K filing for '{company}'.\\n\"\n",
    "            f\"Question: {q_text}\\n\"\n",
    "            f\"Return ONLY the value. If unknown, return NOT_FOUND.\"\n",
    "        )\n",
    "        system_content = \"You are a financial data assistant. Be concise.\"\n",
    "\n",
    "    else:\n",
    "        # Hopefully works praying ðŸ˜­ðŸ˜­\n",
    "        full_text = build_full_context(doc)\n",
    "        context_window = full_text[:150000]\n",
    "\n",
    "        user_content = (\n",
    "            f\"Read this SEC 10-K filing and answer the question.\\n\\n\"\n",
    "            f\"Question: {q_text}\\n\\n\"\n",
    "            f\"Text:\\n{context_window}\\n\\n\"\n",
    "            f\"Return ONLY the extracted value.\"\n",
    "        )\n",
    "        system_content = \"You are a precise extraction assistant.\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    \n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6667ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Execution Engine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize_answer(text):\n",
    "    \"\"\"Basic normalization for comparison.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return str(text).lower().replace(\",\", \"\").replace(\".\", \"\").strip()\n",
    "\n",
    "\n",
    "def check_accuracy(prediction, truth):\n",
    "    \"\"\"\n",
    "    Simple exact/substring match checker.\n",
    "    Returns: IsCorrect (bool)\n",
    "    \"\"\"\n",
    "    p = normalize_answer(prediction)\n",
    "    t = normalize_answer(truth)\n",
    "\n",
    "    if p == \"not_found\" or t == \"null\" or t == \"nan\":\n",
    "        return False\n",
    "\n",
    "    # Exact match or truth inside prediction (e.g. \"Delaware\" inside \"State of Delaware\")\n",
    "    return t in p or p == t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f71890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(stats, config):\n",
    "    \"\"\"Plots accuracy breakdown by question type.\"\"\"\n",
    "    labels = list(stats.keys())\n",
    "    values = [(d[\"correct\"] / d[\"total\"] * 100) if d[\"total\"] > 0 else 0 for d in stats.values()]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    #cool color\n",
    "    bars = plt.bar(labels, values, color=\"#4c72b0\")\n",
    "    \n",
    "    plt.title(f\"Accuracy by Field\\n{config['model_id']} | {config['mode']} | Year: {config['target_years']}\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    \n",
    "    #lowkey chatted this because it was hard to get the graphs looking right, how i wanted\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., h + 1, f'{h:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config):   \n",
    "    '''\n",
    "    Docstring for run_experiment \n",
    "\n",
    "    Runs the main experiment loop: \n",
    "\n",
    "    '''\n",
    "    model, tokenizer = setup_model(config)\n",
    "    dataset = build_experiment_dataset(CONFIG)\n",
    "\n",
    "    results = []\n",
    "    stats = {q[\"id\"]: {\"correct\": 0, \"total\": 0} for q in QUESTION_BANK}\n",
    "    \n",
    "    print(f\"--- Running Experiment: {config['mode']} ---\")\n",
    "    \n",
    "    for i, (doc, gt_data) in enumerate(tqdm(dataset, total=config.get(\"limit_samples\"))):\n",
    "        row = {\n",
    "            \"filename\": doc[\"filename\"], \n",
    "            \"company\": gt_data.get(\"registrant_name_truth\"),\n",
    "            \"year\": gt_data.get(\"year\")\n",
    "        }\n",
    "        \n",
    "        for q in QUESTION_BANK:\n",
    "            # 1. Generate\n",
    "            prompt = format_prompt(doc, gt_data, q, config[\"mode\"], tokenizer)\n",
    "            inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    **config[\"generation_params\"]\n",
    "                )\n",
    "            \n",
    "            pred = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # 2. Evaluate\n",
    "            truth = gt_data.get(GT_COLUMN_MAP.get(q[\"id\"]), \"N/A\")\n",
    "            is_correct = check_accuracy(pred, truth)\n",
    "            \n",
    "            # 3. Stats & Logging\n",
    "            stats[q[\"id\"]][\"total\"] += 1\n",
    "            if is_correct: stats[q[\"id\"]][\"correct\"] += 1\n",
    "            \n",
    "            row[f\"{q['id']}_pred\"] = pred\n",
    "            row[f\"{q['id']}_truth\"] = truth\n",
    "            row[f\"{q['id']}_correct\"] = is_correct\n",
    "            \n",
    "        results.append(row)\n",
    "        \n",
    "        # Periodic Save\n",
    "        if len(results) % 5 == 0:\n",
    "            pd.DataFrame(results).to_csv(config[\"output_file\"], index=False)\n",
    "            \n",
    "    # Final Output\n",
    "    df_res = pd.DataFrame(results)\n",
    "    df_res.to_csv(config[\"output_file\"], index=False)\n",
    "    print(f\"Results saved to {config['output_file']}\")\n",
    "    \n",
    "    plot_accuracy(stats, config)\n",
    "    return df_res\n",
    "\n",
    "\n",
    "# Run it\n",
    "df_results = run_experiment(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc9a9de",
   "metadata": {},
   "source": [
    "### Feel free to explore different df_results by changing CONFIG and re-running the experiment! \n",
    "\n",
    "```python \n",
    "\n",
    "CONFIG = {\n",
    "    # Resources\n",
    "    \"model_id\": \"qwen/Qwen-2.5-32B-Instruct\",\n",
    "    \"output_file\": \"experiment_results_qwen_nocontext_2022.csv\",\n",
    "    \n",
    "    # Experiment Mode\n",
    "    # 'NO_CONTEXT': Ask model about specific company/year facts using only its internal weights.\n",
    "    # 'CONTEXT': Provide full 10-K text (Extractive QA).\n",
    "    \"mode\": \"NO_CONTEXT\", \n",
    "    \n",
    "    # Ground Truth Data\n",
    "    # Path to a CSV containing ground truth data.\n",
    "    \"local_data_path\": \"../data/ground_truth/v1_250_1-6-2025.csv\", \n",
    "    \n",
    "    # Filters\n",
    "    \"target_years\": [1993], \n",
    "    \"limit_samples\": 250, # Set None for all\n",
    "    \n",
    "    # Generation\n",
    "    \"generation_params\": {\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"do_sample\": False, # Deterministic\n",
    "        \"temperature\": None\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96946d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG[\"model_id\"] = \"meta-llama/Meta-Llama-3-8B-Instruct\"  \n",
    "CONFIG[\"output_file\"] = \"experiment_results_llama3_nocontext_1993.csv\" \n",
    "df_results = run_experiment(CONFIG)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
