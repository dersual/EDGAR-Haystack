{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDGAR Ground Truth Extraction Tournament\n",
        "\n",
        "This notebook compares three strategies for extracting data from SEC 10-K filings:\n",
        "\n",
        "1. **Pure Regex**: Fast, baseline method using regular expressions.\n",
        "2. **Hybrid (Smart Locator + LLM)**: Uses **Strict Regex Anchors** first to locate the exact paragraph. If that fails, it falls back to **Keywords**. Then it asks Qwen 2.5 to extract the final answer from that zoomed-in context.\n",
        "3. **Pure LLM**: Feeds the entire section (truncated to fit context) to the LLM.\n",
        "\n",
        "### Goal\n",
        "Determine which method provides the best trade-off between accuracy and cost/speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Installation & Setup\n",
        "# Uncomment the line below if you need to install these libraries\n",
        "#!pip install -q torch transformers datasets pandas tqdm accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Import installed libraries\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Configure display to show full text in pandas\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Setup Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Load Model (Qwen 2.5-7B-Instruct)\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, logging\n",
        "\n",
        "# Quiet HF logs\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DTYPE = torch.bfloat16\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    use_fast=True,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"cuda\" if DEVICE == \"cuda\" else None,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded:\", MODEL_NAME, \"| device:\", DEVICE, \"| PID:\", os.getpid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Unified Configuration (Question Bank)\n",
        "# UPGRADED: Now uses 'regex_anchors' (List) from compare_extractions.py for smarter location.\n",
        "\n",
        "QUESTION_BANK = {\n",
        "    \"section_1\": [\n",
        "        {\n",
        "            \"id\": \"incorporation_state\",\n",
        "            \"prompt\": \"In which U.S. state was this company incorporated? Answer with ONLY the state name. If not found, answer NULL.\",\n",
        "            # Priority 1: Strict Regex Patterns (from compare_extractions.py)\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)incorporated (?:in|under the laws of) (?:the state of )?(\\w+(?:\\s+\\w+)?)\",\n",
        "                r\"(?i)organized (?:in|under the laws of) (?:the state of )?(\\w+(?:\\s+\\w+)?)\",\n",
        "                r\"(?i)a (\\w+(?:\\s+\\w+)?) corporation\",  # Matches \"a Delaware corporation\"\n",
        "                r\"(?i)state of incorporation[:\\s]+(\\w+(?:\\s+\\w+)?)\",\n",
        "            ],\n",
        "            # Priority 2: Fallback Generic Keywords\n",
        "            \"fallback_keywords\": [\n",
        "                \"incorporated\",\n",
        "                \"organized under\",\n",
        "                \"laws of the state\",\n",
        "                \"formed under\",\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"incorporation_year\",\n",
        "            \"prompt\": \"In what year was this company incorporated? Answer with ONLY the year. If not found, answer NULL.\",\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)(?:incorporated|organized|founded|established|formed) (?:in |on |)(?:\\w+ )?(19\\d{2}|20\\d{2})\",\n",
        "                r\"(?i)since (19\\d{2}|20\\d{2})\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"incorporated\",\n",
        "                \"founded\",\n",
        "                \"organized\",\n",
        "                \"formed\",\n",
        "                \"year\",\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"employee_count\",\n",
        "            \"prompt\": \"How many full-time employees does the company have? Answer with ONLY the number. If not found, answer NULL.\",\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)(?:approximately|approx\\.|had|total of|employ)\\s+([0-9,]+)(?:\\s+full-time)?\\s+employees\",\n",
        "                r\"(?i)employees.*?([0-9,]+)\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"employees\",\n",
        "                \"full-time\",\n",
        "                \"employed\",\n",
        "                \"workforce\",\n",
        "                \"persons\",\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"fiscal_year_end\",\n",
        "            \"prompt\": \"On what date does the company's fiscal year end? Answer with Month and Day (e.g., 'December 31'). If not found, answer NULL.\",\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)fiscal year end(?:ed|s)(?:\\s+on)?\\s+([A-Z][a-z]+ \\d{1,2})\"\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"fiscal year end\",\n",
        "                \"fiscal year ends\",\n",
        "                \"fiscal year ended\",\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"company_product\",\n",
        "            \"prompt\": \"What is the main product, service, or business activity of this company? Answer in 2-5 words. If not found, answer NULL.\",\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)engaged in the (?:business|manufacture|sale|development) of ([^.;]+)\"\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"engaged in\",\n",
        "                \"business of\",\n",
        "                \"manufacture\",\n",
        "                \"sale of\",\n",
        "                \"products\",\n",
        "            ],\n",
        "        },\n",
        "    ],\n",
        "    \"section_2\": [\n",
        "        {\n",
        "            \"id\": \"headquarters_state\",\n",
        "            \"prompt\": \"In which U.S. state are the company's principal executive offices located? Answer with ONLY the state name. If not found, answer NULL.\",\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)(?:headquarters|principal (?:executive )?offices?|corporate offices?) (?:is |are |)(?:located |)in ([^,\\.\\n]+)\",\n",
        "                r\"(?i)executive offices.*?,[\\s\\r\\n]+([A-Z][a-z]+(?: [A-Z][a-z]+)*)[\\s\\r\\n]+\\d{5}\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\n",
        "                \"executive offices\",\n",
        "                \"headquarters\",\n",
        "                \"principal offices\",\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    \"section_10\": [\n",
        "        {\n",
        "            \"id\": \"ceo_lastname\",\n",
        "            \"prompt\": \"What is the Last Name of the current CEO? Answer with ONLY the last name. If not found, answer NULL.\",\n",
        "            \"regex_anchors\": [\n",
        "                r\"(?i)([A-Z][a-z]+ [A-Z][a-z]+)[,\\s]+(?:is |serves as |)(?:the |our |)(?:Chief Executive Officer|CEO)\",\n",
        "                r\"(?i)(?:Chief Executive Officer|CEO)[:\\s]+([A-Z][a-z]+ [A-Z][a-z]+)\",\n",
        "            ],\n",
        "            \"fallback_keywords\": [\"chief executive officer\", \"ceo\", \"serves as\"],\n",
        "        }\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. New \"Smart Context\" Logic\n",
        "# This implements the logic: strict regex match -> zoom out -> fallback keyword -> zoom out\n",
        "\n",
        "def get_window(text, center_idx, size=1500):\n",
        "    \"\"\"Helper to grab a window of text centered around an index.\"\"\"\n",
        "    start = max(0, center_idx - (size // 2))\n",
        "    end = min(len(text), center_idx + (size // 2))\n",
        "    return text[start:end]\n",
        "\n",
        "def get_smart_context(full_text, config_question, window_size=1500):\n",
        "    \"\"\"\n",
        "    Priority 1: Check STRICT regex anchors. If found, zoom out around the match.\n",
        "    Priority 2: Check GENERIC keywords. If found, zoom out around the first hit.\n",
        "    Fallback: Return start of text.\n",
        "    \"\"\"\n",
        "    if not full_text: return \"\"\n",
        "    \n",
        "    # 1. Priority: Valid Regex Anchors\n",
        "    for pattern in config_question.get(\"regex_anchors\", []):\n",
        "        match = re.search(pattern, full_text)\n",
        "        if match:\n",
        "            # We found a strict match (e.g. \"incorporated in Delaware\").\n",
        "            # Don't just trust the group capture, grab the whole context window so LLM can verify.\n",
        "            print(f\"  [SmartLocator] Found Anchor: {pattern[:30]}...\") # Debug log\n",
        "            return get_window(full_text, match.start(), window_size)\n",
        "            \n",
        "    # 2. Priority: Keywords\n",
        "    text_lower = full_text.lower()\n",
        "    for keyword in config_question.get(\"fallback_keywords\", []):\n",
        "        index = text_lower.find(keyword.lower())\n",
        "        if index != -1:\n",
        "            # print(f\"  [SmartLocator] Fallback Keyword: {kw}\")\n",
        "            return get_window(full_text, index, window_size)\n",
        "            \n",
        "    # 3. Fallback to start\n",
        "    return full_text[:window_size]\n",
        "\n",
        "\n",
        "def ask_llm(context, prompt, model, tokenizer, max_new_tokens=50):\n",
        "    \"\"\"Sends a prompt to the LLM with the given context.\"\"\"\n",
        "    if not context or not context.strip():\n",
        "        return None\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Read this SEC 10-K filing excerpt and answer the question. \\nContext: \\\"{context}\\\"\\n\\nQuestion: {prompt} \\nIf the information is not present in the context, reply with 'NULL'.\"}\n",
        "    ]\n",
        "    \n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "    \n",
        "    # Clean response\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    answer = response.strip().split('\\n')[0]\n",
        "    \n",
        "    if \"null\" in answer.lower() or \"not found\" in answer.lower():\n",
        "        return None\n",
        "        \n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Strategy Implementations\n",
        "\n",
        "\n",
        "def extract_pure_regex(doc):\n",
        "    \"\"\"Strategy 1: Pure Regex Matching (Updated to use List of Anchors)\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        text = doc.get(section, \"\")\n",
        "        if not text:\n",
        "            for q in questions:\n",
        "                results[q[\"id\"]] = None\n",
        "            continue\n",
        "\n",
        "        for q in questions:\n",
        "            # Try all strict regex patterns in order\n",
        "            found_val = None\n",
        "            for pattern in q.get(\"regex_anchors\", []):\n",
        "                match = re.search(pattern, str(text), re.DOTALL)\n",
        "                if match:\n",
        "                    # Found a match, capture group 1\n",
        "                    found_val = match.group(1).strip(\" .,;\")[:100]\n",
        "                    break  # Stop after first match\n",
        "\n",
        "            results[q[\"id\"]] = found_val\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def extract_hybrid_llm(doc, model, tokenizer):\n",
        "    \"\"\"Strategy 2: Hybrid (Smart Locator + LLM Extraction)\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        full_text = doc.get(section, \"\")\n",
        "\n",
        "        for q in questions:\n",
        "            # 1. SMART LOCATE: Try regex anchor -> Zoom -> Fallback Keyword\n",
        "            context = get_smart_context(full_text, q)\n",
        "\n",
        "            # 2. Extract with LLM\n",
        "            answer = ask_llm(context, q[\"prompt\"], model, tokenizer)\n",
        "            results[q[\"id\"]] = answer\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def extract_pure_llm(doc, model, tokenizer, max_context_chars=12000):\n",
        "    \"\"\"Strategy 3: Pure LLM (Feed entire section context)\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        full_text = doc.get(section, \"\")\n",
        "        if not full_text:\n",
        "            for q in questions:\n",
        "                results[q[\"id\"]] = None\n",
        "            continue\n",
        "\n",
        "        # Truncate to fit in context\n",
        "        context = full_text[:max_context_chars]\n",
        "\n",
        "        for q in questions:\n",
        "            answer = ask_llm(context, q[\"prompt\"], model, tokenizer)\n",
        "            results[q[\"id\"]] = answer\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. The Tournament Loop\n",
        "# Runs detailed comparison on a small batch of documents\n",
        "\n",
        "def create_df_tournament(model, tokenizer, TARGET_DOCS=5): \n",
        "    print(\"Loading Data Stream...\")\n",
        "    dataset = load_dataset(\n",
        "        \"c3po-ai/edgar-corpus\",\n",
        "        \"default\",\n",
        "        split=\"train\",\n",
        "        streaming=True,\n",
        "        revision=\"refs/convert/parquet\",\n",
        "    )\n",
        "\n",
        "    comparison_data = []\n",
        "\n",
        "    print(f\"Running Tournament on {TARGET_DOCS} docs...\")\n",
        "    for i, doc in enumerate(dataset):\n",
        "        if i >= TARGET_DOCS:\n",
        "            break\n",
        "\n",
        "        print(f\"Processing {doc.get('filename', f'Doc {i}')}...\")\n",
        "\n",
        "        # Strategy 1: Regex (Now Smart Regex)\n",
        "        res_regex = extract_pure_regex(doc)\n",
        "\n",
        "        # Strategy 2: Hybrid (Now Smart Context)\n",
        "        res_hybrid = extract_hybrid_llm(doc, model, tokenizer)\n",
        "\n",
        "        # Strategy 3: Pure LLM\n",
        "        res_pure = extract_pure_llm(doc, model, tokenizer)\n",
        "\n",
        "        # Combine results\n",
        "        row = {\n",
        "            \"filename\": doc.get(\"filename\"),\n",
        "            \"cik\": doc.get(\"cik\"),\n",
        "            \"year\": doc.get(\"year\"),\n",
        "        }\n",
        "\n",
        "        all_keys = list(res_regex.keys())\n",
        "\n",
        "        for key in all_keys:\n",
        "            # Shorten key for column name width\n",
        "            short_key = key.replace(\"incorporation_\", \"Inc_\").replace(\n",
        "                \"headquarters_\", \"HQ_\"\n",
        "            )\n",
        "\n",
        "            val_hybrid = res_hybrid.get(key)\n",
        "            val_pure = res_pure.get(key)\n",
        "\n",
        "            row[f\"{short_key}_Regex\"] = res_regex.get(key)\n",
        "            row[f\"{short_key}_Hybrid\"] = val_hybrid\n",
        "            row[f\"{short_key}_LLM\"] = val_pure\n",
        "\n",
        "            # Consensus Check\n",
        "            if val_hybrid is None and val_pure is None:\n",
        "                row[f\"{short_key}_Match\"] = None \n",
        "            elif val_hybrid is None or val_pure is None: \n",
        "                row[f\"{short_key}_Match\"] = \"0\"\n",
        "            else:\n",
        "                is_match = val_hybrid.strip().lower() == val_pure.strip().lower()\n",
        "                row[f\"{short_key}_Match\"] = \"1\" if is_match else \"0\"\n",
        "\n",
        "        comparison_data.append(row)\n",
        "\n",
        "    df_tournament = pd.DataFrame(comparison_data)\n",
        "    print(\"\\n--- TOURNAMENT RESULTS ---\")  \n",
        "    display(df_tournament)\n",
        "    df_tournament.fillna(\"NULL\", inplace=True)\n",
        "    return df_tournament"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Run the tournament and save the results to a CSV file. \n",
        "\n",
        "# sample so only do 5 \n",
        "df_sample = create_df_tournament(model, tokenizer)   \n",
        "# Feel free to add code blocks below to analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. If fine create sample doc csv\n",
        "df_sample.to_csv(\"edgar_tournament_sample.csv\", index=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. If fine create full doc csv \n",
        "df_full = create_df_tournament(model, tokenizer, 250)\n",
        "df_full.to_csv(\"edgar_tournament_full.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# 1. Identify the columns you want to KEEP\n",
        "# We keep a column ONLY if it does NOT contain \"Match\", \"filename\", \"year\", or \"cik\"\n",
        "exclude_terms = [\"Match\", \"filename\", \"year\", \"cik\"]\n",
        "cols_to_keep = [\n",
        "    col for col in df_full.columns if not any(term in col for term in exclude_terms)\n",
        "]\n",
        "\n",
        "# 2. Create a subset (this does not change df_full)\n",
        "subset = df_full[cols_to_keep]\n",
        "\n",
        "# 3. Calculate stats on the subset\n",
        "# Compare entire subset to the string 'NULL'\n",
        "null_counts = (subset == \"NULL\").sum()\n",
        "\n",
        "# Calculate percentage based on total rows\n",
        "null_percentages = (null_counts / len(df_full) * 100).apply(math.floor)\n",
        "\n",
        "# 4. Create the final result table\n",
        "summary_df = pd.DataFrame({\"Null Counts\": null_counts, \"Null %\": null_percentages})\n",
        "\n",
        "# Filter out columns with 0 nulls if you only want to see problems\n",
        "# summary_df = summary_df[summary_df['Null Counts'] > 0]\n",
        "\n",
        "print(summary_df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
