{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDGAR Ground Truth Extraction Tournament\n",
        "\n",
        "This notebook compares three strategies for extracting data from SEC 10-K filings:\n",
        "\n",
        "1. **Pure Regex**: Fast, baseline method using regular expressions.\n",
        "2. **Hybrid (Locator + LLM)**: Uses keywords/lax regex to find the relevant context, then asks an LLM (Qwen 2.5) to extract the exact answer.\n",
        "3. **Pure LLM**: Feeds the entire section (truncated to fit context) to the LLM.\n",
        "\n",
        "### Goal\n",
        "Determine which method provides the best trade-off between accuracy and cost/speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Installation & Setup\n",
        "# Uncomment the line below if you need to install these libraries\n",
        "# !pip install -q torch transformers datasets pandas tqdm accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 2. Import installed libraries\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Configure display to show full text in pandas\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Setup Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Load Model (Qwen 2.5-7B-Instruct)\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "try:\n",
        "    print(f\"Loading {MODEL_NAME}...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    print(\"Model Loaded Successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    print(\"Make sure you have a GPU enabled and the libraries installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Unified Configuration (Question Bank)\n",
        "# Contains logic for ALL strategies: Regex Patterns, Locator Keywords, and LLM Prompts.\n",
        "\n",
        "QUESTION_BANK = {\n",
        "    \"section_1\": [\n",
        "        {\n",
        "            \"id\": \"incorporation_state\",\n",
        "            \"prompt\": \"In which U.S. state was this company incorporated? Answer with ONLY the state name.\",\n",
        "            # Regex: Matches \"Incorporated in the State of Delaware\"\n",
        "            \"extract_regex\": r\"(?i)(?:[Ii]ncorporated|[Oo]rganized)(?: (?:under the laws of|in))? (?:the [Ss]tate of\\s*)?([A-Z][a-z]+(?: [A-Z][a-z]+)*)\",\n",
        "            # Hybrid Locator: Keywords to find the paragraph\n",
        "            \"keywords\": [\"incorporated\", \"organized under\", \"laws of the state\", \"formed under\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"incorporation_year\",\n",
        "            \"prompt\": \"In what year was this company incorporated? Answer with ONLY the year.\",\n",
        "            # Regex: Matches \"incorporated ... in 1985\"\n",
        "            \"extract_regex\": r\"(?i)(?:incorporated|founded|organized).*?in (19\\d{2}|20\\d{2})\",\n",
        "            \"keywords\": [\"incorporated\", \"founded\", \"organized\", \"formed\", \"year\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"employee_count\",\n",
        "            \"prompt\": \"How many full-time employees does the company have? Answer with ONLY the number.\",\n",
        "            # Regex: Matches \"approximately 5,000 employees\"\n",
        "            \"extract_regex\": r\"(?i)(?:approximately|approx\\.|had|total of|employ)\\s+([0-9,]+)(?:\\s+full-time)?\\s+employees\",\n",
        "            \"keywords\": [\"employees\", \"full-time\", \"employed\", \"workforce\", \"persons\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"fiscal_year_end\",\n",
        "            \"prompt\": \"On what date does the company's fiscal year end? Answer with Month and Day (e.g., 'December 31').\",\n",
        "            # Regex: Matches \"fiscal year ends December 31\"\n",
        "            \"extract_regex\": r\"(?i)fiscal year end(?:ed|s)(?:\\s+on)?\\s+([A-Z][a-z]+ \\d{1,2})\",\n",
        "            \"keywords\": [\"fiscal year end\", \"fiscal year ends\", \"fiscal year ended\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"company_product\",\n",
        "            \"prompt\": \"What is the main product, service, or business activity of this company? Answer in 2-5 words.\",\n",
        "            # Regex: Matches \"engaged in the business of...\"\n",
        "            \"extract_regex\": r\"(?i)engaged in the (?:business|manufacture|sale|development) of ([^.;]+)\",\n",
        "            \"keywords\": [\"engaged in\", \"business of\", \"manufacture\", \"sale of\", \"products\"]\n",
        "        }\n",
        "    ],\n",
        "    \"section_2\": [\n",
        "         {\n",
        "            \"id\": \"headquarters_state\",\n",
        "            \"prompt\": \"In which U.S. state are the company's principal executive offices located? Answer with ONLY the state name.\",\n",
        "            # Regex: Matches \"executive offices ... [State] [Zip]\"\n",
        "            \"extract_regex\": r\"(?i)executive offices.*?,[\\s\\r\\n]+([A-Z][a-z]+(?: [A-Z][a-z]+)*)[\\s\\r\\n]+\\d{5}\",\n",
        "            \"keywords\": [\"executive offices\", \"headquarters\", \"principal offices\"]\n",
        "        }\n",
        "    ],\n",
        "    \"section_10\": [\n",
        "       {\n",
        "            \"id\": \"ceo_lastname\",\n",
        "            \"prompt\": \"What is the Last Name of the current CEO? Answer with ONLY the last name.\",\n",
        "            # Regex: Matches \"Mr. Smith ... CEO\"\n",
        "            \"extract_regex\": r\"(?i)(?:Mr\\.|Ms\\.|Mrs\\.|Dr\\.)\\s+([A-Z][a-z]+).*?(?:Chief Executive Officer|CEO)\",\n",
        "            \"keywords\": [\"chief executive officer\", \"ceo\", \"serves as\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"ceo_fullname_backup\",\n",
        "            \"prompt\": \"What is the Full Name of the current CEO? Answer with ONLY the full name.\",\n",
        "             # Regex: Matches Name followed by CEO title\n",
        "            \"extract_regex\": r\"(?m)([A-Z][a-z]+ [A-Z]\\.? [A-Z][a-z]+|[A-Z][a-z]+ [A-Z][a-z]+)\\s*,?\\s*(?:Chief Executive Officer|CEO)\",\n",
        "             \"keywords\": [\"chief executive officer\", \"ceo\"]\n",
        "        }\n",
        "    ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Helper Functions (LLM & Context)\n",
        "\n",
        "\n",
        "def get_focused_context(full_text, keywords, window_size=1500):\n",
        "    \"\"\"HYBRID: Finds the best keyword match and grabs a context window around it.\"\"\"\n",
        "    if not full_text:\n",
        "        return \"\"\n",
        "    text_lower = full_text.lower()\n",
        "\n",
        "    best_word_index = -1\n",
        "    # Simple heuristic: find first occurrence of any keyword\n",
        "    # (Could be improved to find 'densest' keyword region)\n",
        "    for word in keywords:\n",
        "        word_index = text_lower.find(word.lower())\n",
        "        if word_index != -1:\n",
        "            best_word_index = word_index\n",
        "            break\n",
        "\n",
        "    if best_word_index != -1:\n",
        "        start = max(0, best_word_index - (window_size // 2))\n",
        "        end = min(len(full_text), best_word_index + (window_size // 2))\n",
        "        return full_text[start:end]\n",
        "\n",
        "    # Fallback: Return start of text if no keywords found\n",
        "    return full_text[:window_size]\n",
        "\n",
        "\n",
        "def ask_llm(context, prompt, model, tokenizer, max_new_tokens=50):\n",
        "    \"\"\"Sends a prompt to the LLM with the given context.\"\"\"\n",
        "    if not context or not context.strip():\n",
        "        return None\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Read this SEC 10-K filing excerpt and answer the question. \\nContext: \\\"{context}\\\"\\n\\nQuestion: {prompt} \\nIf the information is not present in the context, reply with 'NULL'.\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs, max_new_tokens=max_new_tokens, do_sample=False\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "    return response.strip().split(\"\\n\")[0]  # Take first line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Strategy Implementations\n",
        "\n",
        "\n",
        "def extract_pure_regex(doc):\n",
        "    \"\"\"Strategy 1: Pure Regex Matching\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        text = doc.get(section, \"\")\n",
        "        if not text:\n",
        "            for q in questions:\n",
        "                results[q[\"id\"]] = None\n",
        "            continue\n",
        "\n",
        "        for q in questions:\n",
        "            match = re.search(q[\"extract_regex\"], str(text), re.DOTALL)\n",
        "            if match:\n",
        "                # Clean up match\n",
        "                clean_ans = match.group(1).strip(\" .,;\")[:150]\n",
        "                results[q[\"id\"]] = clean_ans\n",
        "            else:\n",
        "                results[q[\"id\"]] = None\n",
        "    return results\n",
        "\n",
        "\n",
        "def extract_hybrid_llm(doc, model, tokenizer):\n",
        "    \"\"\"Strategy 2: Hybrid (Lax Keyword Locator + LLM Extraction)\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        full_text = doc.get(section, \"\")\n",
        "\n",
        "        for q in questions:\n",
        "            # 1. Locate relevant generic context\n",
        "            # Use keywords to find the 1000 char window\n",
        "            context = get_focused_context(full_text, q[\"keywords\"])\n",
        "\n",
        "            # 2. Extract with LLM\n",
        "            answer = ask_llm(context, q[\"prompt\"], model, tokenizer)\n",
        "            results[q[\"id\"]] = answer\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def extract_pure_llm(doc, model, tokenizer, max_context_chars=12000):\n",
        "    \"\"\"Strategy 3: Pure LLM (Feed entire section context)\"\"\"\n",
        "    results = {}\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        full_text = doc.get(section, \"\")\n",
        "        if not full_text:\n",
        "            for q in questions:\n",
        "                results[q[\"id\"]] = None\n",
        "            continue\n",
        "\n",
        "        # Truncate to fit in context if necessary (simple truncation)\n",
        "        context = full_text[:max_context_chars]\n",
        "\n",
        "        # Note: For just LLM setup, might run one big prompt asking for ALL fields at once to save tokens.\n",
        "        for q in questions:\n",
        "            answer = ask_llm(context, q[\"prompt\"], model, tokenizer)\n",
        "            results[q[\"id\"]] = answer\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. The Tournament Loop Function\n",
        "# Runs detailed comparison on a small batch of documents\n",
        "\n",
        "def create_df_tournament(model, tokenizer, TARGET_DOCS=5):\n",
        "    print(\"Loading Data Stream...\")\n",
        "    dataset = load_dataset(\n",
        "        \"c3po-ai/edgar-corpus\",\n",
        "        \"default\",\n",
        "        split=\"train\",\n",
        "        streaming=True,\n",
        "        revision=\"refs/convert/parquet\",\n",
        "    )\n",
        "\n",
        "    comparison_data = []\n",
        "\n",
        "    print(f\"Running Tournament on {TARGET_DOCS} docs...\")\n",
        "\n",
        "    for i, doc in enumerate(dataset):\n",
        "        if i >= TARGET_DOCS:\n",
        "            break\n",
        "\n",
        "        print(f\"Processing {doc.get('filename', f'Doc {i}')}...\")\n",
        "\n",
        "        res_regex = extract_pure_regex(doc)\n",
        "        res_hybrid = extract_hybrid_llm(doc, model, tokenizer)\n",
        "        res_pure = extract_pure_llm(doc, model, tokenizer)\n",
        "\n",
        "        # Combine results\n",
        "        row = {\"Filename\": doc.get(\"filename\")}\n",
        "\n",
        "        # Add all fields dynamically (Regex, Hybrid, PureLLM)\n",
        "        # iterate over all the extracted keys to ensure we get everything\n",
        "        all_keys = list(res_regex.keys())\n",
        "\n",
        "        for key in all_keys:\n",
        "            # Shorten key for column name width\n",
        "            short_key = (\n",
        "                key.replace(\"incorporation_\", \"Inc_\")\n",
        "                .replace(\"headquarters_\", \"HQ_\")\n",
        "                .replace(\"company_\", \"Hz_\")\n",
        "            )\n",
        "\n",
        "            row[f\"{short_key}_Re\"] = res_regex.get(key)\n",
        "            row[f\"{short_key}_Hy\"] = res_hybrid.get(key)\n",
        "            row[f\"{short_key}_Lu\"] = res_pure.get(key)\n",
        "\n",
        "        comparison_data.append(row)\n",
        "\n",
        "    df_tournament = pd.DataFrame(comparison_data) \n",
        "    df_tournament.fillna(\"NULL\", inplace=True)\n",
        "    print(\"\\n--- TOURNAMENT RESULTS ---\")\n",
        "    display(df_tournament)\n",
        "    return df_tournament"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 8. Run the tournament and save the results to a CSV file. \n",
        "\n",
        "# sample so only do 5 \n",
        "num_of_docs = 5 \n",
        "df_sample = create_df_tournament(model, tokenizer)  \n",
        "print(\"\\n--- Test Run Preview ---\") \n",
        "df_sample.head()  \n",
        "# Feel free to add code blocks below to analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 9. If fine create sample doc csv\n",
        "df_sample.to_csv(\"edgar_tournament_sample.csv\", index=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 10. If fine create full doc csv \n",
        "df_full = create_df_tournament(model, tokenizer, 150)\n",
        "df_full.to_csv(\"edgar_tournament_full.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
