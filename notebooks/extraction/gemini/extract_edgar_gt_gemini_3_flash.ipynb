{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGAR Ground Truth Extraction (Gemini 3 Flash)\n",
    "\n",
    "**Goal**: Extract fields from SEC 10-K filings using Google Gemini (Flash).\n",
    "\n",
    "**Key Features**:\n",
    "1. **Toggle Mode**: Single (per-field) or Batch (all-at-once) queries.\n",
    "2. **Integrated Cleanup**: Appends full section text and specific booleans for found/verified evidence.\n",
    "3. **Evidence Verification**: Checks if the specific evidence sentence actually exists in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup\n",
    "# !pip install -q -google-genai pandas tqdm python-Levenshtein datasets\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from datasets import load_dataset\n",
    "import Levenshtein \n",
    "import time   \n",
    "\n",
    "# from google.colab import userdata # (if you are using colab)\n",
    "# Configure API\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"YOUR_API_KEY\" # Uncomment if not set in env\n",
    "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"]) # or use userdata.get('GEMINI_API_KEY')\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration\n",
    "\n",
    "# Model Name\n",
    "MODEL_ID = \"gemini-3-flash-preview\"\n",
    "\n",
    "# Extraction Mode\n",
    "# True = One prompt for all fields (Cheaper/Faster)\n",
    "# False = One prompt per field (Better precision/isolation)\n",
    "USE_BATCH_MODE = False \n",
    "\n",
    "# Paths\n",
    "OUTPUT_FILE = \"edgar_gt_gemini_extraction.csv\"\n",
    "MAX_DOCUMENTS = 250\n",
    "CHECKPOINT_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Question Bank\n",
    "\n",
    "QUESTION_BANK = [\n",
    "    {\n",
    "        \"id\": \"registrant_name\",\n",
    "        \"prompt\": (\n",
    "            \"What is the exact legal name of the registrant as explicitly stated in the document? \"\n",
    "            \"Return ONLY the legal name string or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"headquarters_city\",\n",
    "        \"prompt\": (\n",
    "            \"What city is explicitly stated as the location of the registrant's principal executive offices? \"\n",
    "            \"Return ONLY the city name or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"headquarters_state\",\n",
    "        \"prompt\": (\n",
    "            \"What U.S. state is explicitly stated as the location of the registrant's principal executive offices? \"\n",
    "            \"Return ONLY the state name or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"incorporation_state\",\n",
    "        \"prompt\": (\n",
    "            \"Identify the state or other jurisdiction under the laws of which the registrant is **currently** organized or incorporated. \"\n",
    "            \"Exclude former jurisdictions. \"\n",
    "            \"Return ONLY the state name or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"incorporation_year\",\n",
    "        \"prompt\": (\n",
    "            \"What is the year of the registrant's incorporation in its **current** jurisdiction (state of incorporation)? \"\n",
    "            \"Return ONLY the year (YYYY) or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"employees_count_total\",\n",
    "        \"prompt\": (\n",
    "            \"What is the **total** number of persons employed by the registrant? \"\n",
    "            \"Include full-time and part-time employees. \"\n",
    "            \"Return ONLY the integer (remove commas) or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"employees_count_full_time\",\n",
    "        \"prompt\": (\n",
    "            \"What is the number of **full-time** employees explicitly stated? \"\n",
    "            \"Return ONLY the integer (remove commas) or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"irs_tax_id\",\n",
    "        \"prompt\": (\n",
    "            \"What is the IRS Employer Identification Number (EIN) or I.R.S. Employer Identification No. of the registrant? \"\n",
    "            \"Return ONLY the number (format XX-XXXXXXX) or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ceo_lastname\",\n",
    "        \"prompt\": (\n",
    "            \"What is the last name of the individual explicitly identified as the Chief Executive Officer (CEO) of the registrant as explicitly stated in the document? \"\n",
    "            \"Return ONLY the last name string or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"holder_record_amount\",\n",
    "        \"prompt\": (\n",
    "            \"What is the number of holders of record of the registrant's common stock as explicitly stated in the document? \"\n",
    "            \"Return ONLY the integer (remove commas) or NOT_FOUND.\"\n",
    "        ),\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Helper Functions: Context & Verification\n",
    "\n",
    "SECTION_KEYS = [\n",
    "    \"section_1\", \"section_1A\", \"section_1B\", \"section_2\", \"section_3\",\n",
    "    \"section_4\", \"section_5\", \"section_6\", \"section_7\", \"section_7A\",\n",
    "    \"section_8\", \"section_9\", \"section_9A\", \"section_9B\", \"section_10\",\n",
    "    \"section_11\", \"section_12\", \"section_13\", \"section_14\", \"section_15\"\n",
    "]\n",
    "\n",
    "def build_full_context(doc):\n",
    "    \"\"\"Concatenates all sections.\"\"\"\n",
    "    parts = []\n",
    "    for key in SECTION_KEYS:\n",
    "        section_text = doc.get(key, \"\")\n",
    "        if section_text and section_text.strip():\n",
    "            parts.append(f\"\\n\\n--- [{key.upper()}] ---\\n\\n{section_text}\")\n",
    "    return \"\".join(parts) if parts else \"\"\n",
    "\n",
    "def verify_evidence(full_text, evidence_quote, value):\n",
    "    \"\"\"\n",
    "    Verifies if the evidence quote exists in the text.\n",
    "    Returns: True (Found), False (Not Found/Hallucinated), or None (N/A)\n",
    "    \"\"\"\n",
    "    if str(value) == \"NOT_FOUND\" or not value:\n",
    "        return None\n",
    "        \n",
    "    if not evidence_quote or evidence_quote == \"NOT_FOUND\":\n",
    "        return False\n",
    "\n",
    "    # 1. Exact Match\n",
    "    if evidence_quote in full_text:\n",
    "        return True\n",
    "        \n",
    "    # 2. Normalized Match\n",
    "    clean_text = \" \".join(full_text.split()).lower()\n",
    "    clean_evd = \" \".join(evidence_quote.split()).lower()\n",
    "    if clean_evd in clean_text:\n",
    "        return True\n",
    "        \n",
    "    # 3. Fuzzy/Levenshtein (Threshold 90%)\n",
    "    # Only if evidence is substantial (>10 chars)\n",
    "    if len(clean_evd) > 10:\n",
    "        # Check fingerprint\n",
    "        fp_text = re.sub(r'[^a-z0-9]', '', clean_text)\n",
    "        fp_evd = re.sub(r'[^a-z0-9]', '', clean_evd)\n",
    "        if fp_evd in fp_text:\n",
    "            return True\n",
    "            \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Gemini Extraction Logic\n",
    "\n",
    "class ExtractionResult(pd.Series):\n",
    "    # Just a helper for type hinting\n",
    "    pass\n",
    "\n",
    "# Schema for single field extraction\n",
    "response_schema_single = {\n",
    "    \"type\": \"OBJECT\",\n",
    "    \"properties\": {\n",
    "        \"value\": {\"type\": \"STRING\"},\n",
    "        \"evidence\": {\"type\": \"STRING\"},\n",
    "        \"source_sentence\": {\"type\": \"STRING\"}\n",
    "    },\n",
    "    \"required\": [\"value\", \"evidence\", \"source_sentence\"]\n",
    "}\n",
    "\n",
    "def call_gemini_single(full_text, prompt_question):\n",
    "    \"\"\"Calls Gemini for a single field using structured output.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Read the following SEC 10-K filing text and answer the question.\n",
    "    \n",
    "    Question: {prompt_question}\n",
    "    \n",
    "    Instructions:\n",
    "    - Extract the answer strictly from the text.\n",
    "    - If not found, set value to \"NOT_FOUND\".\n",
    "    - Provide the EXACT evidence quote from the text.\n",
    "    - Provide the full source sentence containing the evidence.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=[prompt, full_text],\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=response_schema_single,\n",
    "                temperature=0.1\n",
    "            )\n",
    "        )\n",
    "        return json.loads(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Gemini Call Failed: {e}\")\n",
    "        return {\"value\": \"ERROR\", \"evidence\": \"ERROR\", \"source_sentence\": str(e)}\n",
    "\n",
    "def call_gemini_batch(full_text, question_bank):\n",
    "    \"\"\"Calls Gemini for ALL fields in one go.\"\"\"\n",
    "    \n",
    "    # Dynamically build schema for all fields\n",
    "    properties = {}\n",
    "    required = []\n",
    "    \n",
    "    for q in question_bank:\n",
    "        fid = q[\"id\"]\n",
    "        properties[f\"{fid}_value\"] = {\"type\": \"STRING\"}\n",
    "        properties[f\"{fid}_evidence\"] = {\"type\": \"STRING\"}\n",
    "        properties[f\"{fid}_source_sentence\"] = {\"type\": \"STRING\"}\n",
    "        required.extend([f\"{fid}_value\", f\"{fid}_evidence\", f\"{fid}_source_sentence\"])\n",
    "        \n",
    "    batch_schema = {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": properties,\n",
    "        \"required\": required\n",
    "    }\n",
    "    \n",
    "    questions_str = \"\\n\".join([f\"- {q['id']}: {q['prompt']}\" for q in question_bank])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Read the following SEC 10-K filing and extract the following fields.\n",
    "    For each field, provide the value, exact evidence quote, and source sentence.\n",
    "    If not found, use \"NOT_FOUND\".\n",
    "    \n",
    "    Fields to Extract:\n",
    "    {questions_str}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=[prompt, full_text],\n",
    "            config=types.GenerateContentConfig(\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=batch_schema,\n",
    "                temperature=0.1\n",
    "            )\n",
    "        )\n",
    "        return json.loads(response.text)\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Gemini Batch Call Failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Main Processing Loop\n",
    "\n",
    "def process_document(doc):\n",
    "    result = {\n",
    "        \"filename\": doc[\"filename\"],\n",
    "        \"cik\": doc.get(\"cik\"),\n",
    "        \"year\": doc.get(\"year\")\n",
    "    }\n",
    "    \n",
    "    # 1. Build Context\n",
    "    full_text = build_full_context(doc)\n",
    "    \n",
    "    # 2. Append Section Texts (Cleanup Logic)\n",
    "    for key in SECTION_KEYS:\n",
    "        result[key] = doc.get(key, None)\n",
    "    \n",
    "    if not full_text:\n",
    "        # Fill Empty\n",
    "        for q in QUESTION_BANK:\n",
    "            result[f\"{q['id']}_value\"] = \"NO_TEXT\"\n",
    "            result[f\"{q['id']}_found\"] = False\n",
    "        return result\n",
    "\n",
    "    # 3. Extraction\n",
    "    if USE_BATCH_MODE:\n",
    "        # -- BATCH MODE --\n",
    "        data = call_gemini_batch(full_text, QUESTION_BANK)\n",
    "        if data:\n",
    "            for q in QUESTION_BANK:\n",
    "                fid = q[\"id\"]\n",
    "                val = data.get(f\"{fid}_value\", \"NOT_FOUND\")\n",
    "                ev = data.get(f\"{fid}_evidence\", \"NOT_FOUND\")\n",
    "                sent = data.get(f\"{fid}_source_sentence\", \"NOT_FOUND\")\n",
    "                \n",
    "                result[f\"{fid}_value\"] = val\n",
    "                result[f\"{fid}_evidence\"] = ev\n",
    "                result[f\"{fid}_source_sentence\"] = sent\n",
    "                \n",
    "                # Verify\n",
    "                verified = verify_evidence(full_text, ev, val)\n",
    "                result[f\"{fid}_evidence_verified\"] = verified\n",
    "                result[f\"{fid}_found\"] = (val != \"NOT_FOUND\")\n",
    "        else:\n",
    "             # Batch failed\n",
    "            for q in QUESTION_BANK:\n",
    "                result[f\"{q['id']}_value\"] = \"ERROR\"\n",
    "\n",
    "    else:\n",
    "        # -- SINGLE MODE --\n",
    "        for q in QUESTION_BANK:\n",
    "            fid = q[\"id\"]\n",
    "            # print(f\"  Extracting {fid}...\")\n",
    "            data = call_gemini_single(full_text, q[\"prompt\"])\n",
    "            time.sleep(1) \n",
    "            \n",
    "            val = data.get(\"value\", \"NOT_FOUND\")\n",
    "            ev = data.get(\"evidence\", \"NOT_FOUND\")\n",
    "            sent = data.get(\"source_sentence\", \"NOT_FOUND\")\n",
    "            \n",
    "            result[f\"{fid}_value\"] = val\n",
    "            result[f\"{fid}_evidence\"] = ev\n",
    "            result[f\"{fid}_source_sentence\"] = sent\n",
    "            \n",
    "            # Verify\n",
    "            verified = verify_evidence(full_text, ev, val)\n",
    "            result[f\"{fid}_evidence_verified\"] = verified\n",
    "            result[f\"{fid}_found\"] = (str(val) != \"NOT_FOUND\" and str(val) != \"None\")\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Run Extraction\n",
    "\n",
    "# Load Dataset\n",
    "dataset = load_dataset(\n",
    "    \"c3po-ai/edgar-corpus\",\n",
    "    \"default\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    revision=\"refs/convert/parquet\",\n",
    ")\n",
    "\n",
    "# Resume Check\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df_results = pd.read_csv(OUTPUT_FILE)\n",
    "    processed_files = set(df_results[\"filename\"].astype(str))\n",
    "    print(f\"Resuming: {len(processed_files)} documents processed.\")\n",
    "else:\n",
    "    df_results = pd.DataFrame()\n",
    "    processed_files = set()\n",
    "\n",
    "buffer = []\n",
    "count = 0\n",
    "\n",
    "print(f\"Starting Extraction (Batch={USE_BATCH_MODE}, Model={MODEL_ID})...\")\n",
    "\n",
    "for doc in tqdm(dataset):\n",
    "    if count >= MAX_DOCUMENTS:\n",
    "        break\n",
    "        \n",
    "    fname = str(doc[\"filename\"])\n",
    "    if fname in processed_files:\n",
    "        continue\n",
    "        \n",
    "    # Process\n",
    "    try:\n",
    "        row = process_document(doc) \n",
    "        if USE_BATCH_MODE: \n",
    "            time.sleep(2) \n",
    "        buffer.append(row)\n",
    "        processed_files.add(fname)\n",
    "        count += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {fname}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Checkpoint\n",
    "    if len(buffer) >= CHECKPOINT_INTERVAL:\n",
    "        new_df = pd.DataFrame(buffer)\n",
    "        df_results = pd.concat([df_results, new_df], ignore_index=True)\n",
    "        df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "        buffer = []\n",
    "        print(f\"Saved checkpoint: {len(df_results)} total\")\n",
    "\n",
    "# Final Save\n",
    "if buffer:\n",
    "    new_df = pd.DataFrame(buffer)\n",
    "    df_results = pd.concat([df_results, new_df], ignore_index=True)\n",
    "    df_results.to_csv(OUTPUT_FILE, index=False)\n",
    "\n",
    "print(f\"Done. Saved to {OUTPUT_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
