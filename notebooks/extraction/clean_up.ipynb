{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ground Truth Cleanup Script\n",
        "\n",
        "This notebook:\n",
        "1. Loads a ground truth CSV file\n",
        "2. Fills \"NULL\" values or empty strings with NaN or None\n",
        "3. Appends sections from the EDGAR corpus (HuggingFace)\n",
        "4. Shows bar graphs of non-null values per category\n",
        "5. Saves the cleaned CSV with timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "from datetime import datetime\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the ground truth file\n",
        "GROUND_TRUTH_PATH = \"../../data/ground_truth/v3_250_(1993)_(1-27-2026).csv\"\n",
        "\n",
        "# Output directory\n",
        "OUTPUT_DIR = \"../../data/ground_truth\"\n",
        "\n",
        "# Year to filter EDGAR corpus (based on ground truth file)\n",
        "YEAR = 1993"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Ground Truth CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the ground truth file\n",
        "gt_df = pd.read_csv(GROUND_TRUTH_PATH)\n",
        "print(f\"Loaded ground truth with {len(gt_df)} rows\")\n",
        "print(f\"Columns: {list(gt_df.columns)}\")\n",
        "gt_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Fill NA values with \"NULL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "gt_df = gt_df.replace([\"NULL\", \"\"], np.nan)\n",
        "\n",
        "print(\"Replaced 'NULL' and empty strings with NaN\")\n",
        "print(f\"NaN counts per column:\\n{gt_df.isna().sum()}\")\n",
        "gt_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load EDGAR Corpus from HuggingFace and Append Sections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the EDGAR corpus (streaming for memory efficiency)\n",
        "print(f\"Loading EDGAR corpus from HuggingFace...\")\n",
        "edgar_dataset = load_dataset(\n",
        "    \"c3po-ai/edgar-corpus\",\n",
        "    \"default\",\n",
        "    split=\"train\",\n",
        "    streaming=True,\n",
        "    revision=\"refs/convert/parquet\",\n",
        ")\n",
        "print(\"Dataset loaded in streaming mode\")\n",
        "\n",
        "# Define section columns to extract\n",
        "section_cols = [\n",
        "    \"section_1\", \"section_1A\", \"section_1B\", \"section_2\", \"section_3\",\n",
        "    \"section_4\", \"section_5\", \"section_6\", \"section_7\", \"section_7A\",\n",
        "    \"section_8\", \"section_9\", \"section_9A\", \"section_9B\", \"section_10\",\n",
        "    \"section_11\", \"section_12\", \"section_13\", \"section_14\", \"section_15\"\n",
        "]\n",
        "\n",
        "# Create a set of filenames we need from ground truth\n",
        "needed_filenames = set(gt_df['filename'].tolist())\n",
        "print(f\"Looking for {len(needed_filenames)} specific files from ground truth...\")\n",
        "\n",
        "# Only extract documents that match our ground truth filenames\n",
        "edgar_data = []\n",
        "found_count = 0\n",
        "for doc in edgar_dataset:\n",
        "    # Only process if this filename is in our ground truth\n",
        "    if doc.get(\"filename\") in needed_filenames:\n",
        "        row = {\"filename\": doc[\"filename\"]}\n",
        "        for col in section_cols:\n",
        "            row[col] = doc.get(col, \"\")\n",
        "        edgar_data.append(row)\n",
        "        found_count += 1\n",
        "        \n",
        "        # Stop once we've found all needed files\n",
        "        if found_count >= len(needed_filenames):\n",
        "            print(f\"Found all {found_count} required files, stopping early.\")\n",
        "            break\n",
        "        \n",
        "        # Progress update every 50 files\n",
        "        if found_count % 50 == 0:\n",
        "            print(f\"  Found {found_count}/{len(needed_filenames)} files...\")\n",
        "\n",
        "edgar_df = pd.DataFrame(edgar_data)\n",
        "print(f\"Extracted {len(edgar_df)} filings matching ground truth\")\n",
        "print(f\"Columns: {list(edgar_df.columns)}\")\n",
        "edgar_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a mapping from CIK to sections\n",
        "# The EDGAR corpus uses 'cik' column\n",
        "section_cols = [col for col in edgar_df.columns if col.startswith('section_')]\n",
        "print(f\"Section columns found: {section_cols}\")\n",
        "\n",
        "# Merge on filename directly - both datasets have this column\n",
        "gt_df_merged = gt_df.merge(\n",
        "    edgar_df[['filename'] + section_cols],\n",
        "    on='filename',\n",
        "    how='left',\n",
        "    suffixes=('', '_edgar')\n",
        ")\n",
        "\n",
        "print(f\"Merged DataFrame shape: {gt_df_merged.shape}\")\n",
        "print(f\"New columns: {list(gt_df_merged.columns)}\")\n",
        "print(f\"Merge success rate: {(~gt_df_merged[section_cols[0]].isna()).sum()}/{len(gt_df_merged)} rows matched\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Show Bar Graphs of Non-NULL Values per Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate non-NULL counts for each column\n",
        "def count_non_null(series):\n",
        "    \"\"\"Count values that are not 'NULL' or empty.\"\"\"\n",
        "    return ((series != \"NULL\") & (series != \"\") & (series.notna())).sum()\n",
        "\n",
        "# Get counts for all columns (excluding filename and section columns for truth values)\n",
        "truth_cols = [col for col in gt_df_merged.columns if col.endswith('_truth')]\n",
        "non_null_counts = {col: count_non_null(gt_df_merged[col]) for col in truth_cols}\n",
        "\n",
        "print(\"Non-NULL counts per truth column:\")\n",
        "for col, count in non_null_counts.items():\n",
        "    print(f\"  {col}: {count}/{len(gt_df_merged)} ({count/len(gt_df_merged)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bar chart for truth columns\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "columns = list(non_null_counts.keys())\n",
        "counts = list(non_null_counts.values())\n",
        "\n",
        "# Create shorter labels by removing '_truth' suffix\n",
        "labels = [col.replace('_truth', '') for col in columns]\n",
        "\n",
        "bars = ax.bar(labels, counts, color='steelblue', edgecolor='black')\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for bar, count in zip(bars, counts):\n",
        "    height = bar.get_height()\n",
        "    ax.annotate(f'{count}',\n",
        "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                xytext=(0, 3),\n",
        "                textcoords=\"offset points\",\n",
        "                ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "ax.set_xlabel('Category', fontsize=12)\n",
        "ax.set_ylabel('Non-NULL Count', fontsize=12)\n",
        "ax.set_title('Non-NULL Values per Ground Truth Category', fontsize=14)\n",
        "ax.set_ylim(0, len(gt_df_merged) * 1.15)  # Add space for labels\n",
        "\n",
        "# Add horizontal line for total count\n",
        "ax.axhline(y=len(gt_df_merged), color='red', linestyle='--', alpha=0.7, label=f'Total rows: {len(gt_df_merged)}')\n",
        "ax.legend()\n",
        "\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Also show section columns non-NULL counts\n",
        "section_cols_in_merged = [col for col in gt_df_merged.columns if col.startswith('section_')]\n",
        "section_non_null_counts = {col: count_non_null(gt_df_merged[col]) for col in section_cols_in_merged}\n",
        "\n",
        "print(\"\\nNon-NULL counts per section column:\")\n",
        "for col, count in section_non_null_counts.items():\n",
        "    print(f\"  {col}: {count}/{len(gt_df_merged)} ({count/len(gt_df_merged)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bar chart for section columns\n",
        "if section_non_null_counts:\n",
        "    fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "    columns = list(section_non_null_counts.keys())\n",
        "    counts = list(section_non_null_counts.values())\n",
        "\n",
        "    bars = ax.bar(columns, counts, color='forestgreen', edgecolor='black')\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    for bar, count in zip(bars, counts):\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{count}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    ax.set_xlabel('Section', fontsize=12)\n",
        "    ax.set_ylabel('Non-NULL Count', fontsize=12)\n",
        "    ax.set_title('Non-NULL Values per SEC 10-K Section', fontsize=14)\n",
        "    ax.set_ylim(0, len(gt_df_merged) * 1.15)\n",
        "\n",
        "    # Add horizontal line for total count\n",
        "    ax.axhline(y=len(gt_df_merged), color='red', linestyle='--', alpha=0.7, label=f'Total rows: {len(gt_df_merged)}')\n",
        "    ax.legend()\n",
        "\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Save Cleaned CSV with Timestamp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate timestamp for filename\n",
        "timestamp = datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
        "output_filename = f\"cleaned_EDGAR_gt_({timestamp}).csv\"\n",
        "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "# Save the cleaned DataFrame\n",
        "gt_df_merged.to_csv(output_path, index=False)\n",
        "print(f\"Saved cleaned ground truth to: {output_path}\")\n",
        "print(f\"Total rows: {len(gt_df_merged)}\")\n",
        "print(f\"Total columns: {len(gt_df_merged.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"CLEANUP SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input file: {GROUND_TRUTH_PATH}\")\n",
        "print(f\"Output file: {output_path}\")\n",
        "print(f\"Total rows: {len(gt_df_merged)}\")\n",
        "print(f\"Original columns: {len(gt_df.columns)}\")\n",
        "print(f\"Final columns: {len(gt_df_merged.columns)}\")\n",
        "print(f\"Section columns added: {len(section_cols_in_merged)}\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
