{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Needle Investigation & Fix: Robust Header/Fuzzy Logic\n",
        "\n",
        "**Goal**: \n",
        "1. Ensure 'source_sentence' values are EXACT substrings of the 'section_*' columns.\n",
        "2. Populate a separate column '_section' indicating which section the sentence belongs to.\n",
        "3. Handle \"Header Injection\" (e.g. \"Item 1. Business...\" when section only has \"...\") by stripping headers and finding the best sentence match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import difflib\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 255 rows.\n"
          ]
        }
      ],
      "source": [
        "# Config\n",
        "INPUT_FILE = \"../../data/clean_ground_truth/cleaned_EDGAR_gt_02-16-2026.csv\"\n",
        "OUTPUT_FILE = \"../../data/clean_ground_truth/cleaned_EDGAR_gt_FIXED.csv\"\n",
        "\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    print(f\"ERROR: File not found: {INPUT_FILE}\")\n",
        "else:\n",
        "    df = pd.read_csv(INPUT_FILE)\n",
        "    print(f\"Loaded {len(df)} rows.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20 section columns.\n",
            "Found 9 source sentence columns: ['registrant_name_source_sentence', 'headquarters_city_source_sentence', 'headquarters_state_source_sentence', 'incorporation_state_source_sentence', 'incorporation_year_source_sentence', 'employees_count_total_source_sentence', 'employees_count_full_time_source_sentence', 'ceo_lastname_source_sentence', 'holder_record_amount_source_sentence']\n"
          ]
        }
      ],
      "source": [
        "# Identify columns\n",
        "section_cols = [c for c in df.columns if c.startswith(\"section_\") and not c.endswith(\"_section\")]\n",
        "source_cols = [c for c in df.columns if c.endswith(\"_source_sentence\")]\n",
        "\n",
        "print(f\"Found {len(section_cols)} section columns.\")\n",
        "print(f\"Found {len(source_cols)} source sentence columns: {source_cols}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Preprocessing: Clean 'nan' Strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaning 'nan' strings...\n",
            "Done. Checking non-null counts:\n",
            "registrant_name_source_sentence              249\n",
            "headquarters_city_source_sentence            153\n",
            "headquarters_state_source_sentence           148\n",
            "incorporation_state_source_sentence          186\n",
            "incorporation_year_source_sentence           172\n",
            "employees_count_total_source_sentence        171\n",
            "employees_count_full_time_source_sentence     61\n",
            "ceo_lastname_source_sentence                 194\n",
            "holder_record_amount_source_sentence         161\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(\"Cleaning 'nan' strings...\")\n",
        "for col in source_cols:\n",
        "    # Standardize missing values\n",
        "    df[col] = df[col].replace([\"nan\", \"NaN\", \"None\", \"NULL\", \"\"], np.nan)\n",
        "\n",
        "print(\"Done. Checking non-null counts:\")\n",
        "print(df[source_cols].notna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Core Logic: Robust Matcher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing rows... This may take 2-3 minutes.\n",
            "  Fixing registrant_name_source_sentence -> mapping to registrant_name_section\n",
            "  Fixing headquarters_city_source_sentence -> mapping to headquarters_city_section\n",
            "  Fixing headquarters_state_source_sentence -> mapping to headquarters_state_section\n",
            "  Fixing incorporation_state_source_sentence -> mapping to incorporation_state_section\n",
            "  Fixing incorporation_year_source_sentence -> mapping to incorporation_year_section\n",
            "  Fixing employees_count_total_source_sentence -> mapping to employees_count_total_section\n",
            "  Fixing employees_count_full_time_source_sentence -> mapping to employees_count_full_time_section\n",
            "  Fixing ceo_lastname_source_sentence -> mapping to ceo_lastname_section\n",
            "  Fixing holder_record_amount_source_sentence -> mapping to holder_record_amount_section\n",
            "Done processing.\n"
          ]
        }
      ],
      "source": [
        "def split_into_sentences(text):\n",
        "    \"\"\"Simple sentence splitter.\"\"\"\n",
        "    # Split by period, question mark, or exclamation point followed by space or newline\n",
        "    # Also handle newlines as implicit breaks if they look like list items\n",
        "    if not text: return []\n",
        "    # This regex is a bit simplistic but works for finding candidate chunks\n",
        "    # split on .?! followed by space or end of string\n",
        "    chunks = re.split(r'(?<=[.?!])\\s+', text)\n",
        "    # Also split by double newlines to be safe\n",
        "    final_chunks = []\n",
        "    for c in chunks:\n",
        "        final_chunks.extend(c.split('\\n\\n'))\n",
        "    return [c.strip() for c in final_chunks if c.strip()]\n",
        "\n",
        "def clean_header(text):\n",
        "    \"\"\"Removes common EDGAR headers mostly from start.\"\"\"\n",
        "    # Remove \"Item 1. Business.\", \"PART I\", etc.\n",
        "    text = re.sub(r'^(Item|Part)\\s+\\w+\\.?\\s*', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'^(Business|Properties|Legal Proceedings)\\.?\\s*', '', text, flags=re.IGNORECASE)\n",
        "    return text.strip()\n",
        "\n",
        "def find_needle_in_sections(row, source_sent):\n",
        "    if pd.isna(source_sent):\n",
        "        return np.nan, np.nan\n",
        "        \n",
        "    src = str(source_sent).strip()\n",
        "    if not src or src.lower() == \"nan\":\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    # --- 1. EXACT MATCH (Fastest) ---\n",
        "    for sec in section_cols:\n",
        "        sec_text = str(row.get(sec, \"\"))\n",
        "        if src in sec_text:\n",
        "            return src, sec\n",
        "\n",
        "    # --- 2. ROBUST SEARCH ---\n",
        "    # Prepare candidates from source (Original & Header-Stripped)\n",
        "    candidates = [src]\n",
        "    cleaned = clean_header(src)\n",
        "    if cleaned != src and len(cleaned) > 10:\n",
        "        candidates.append(cleaned)\n",
        "\n",
        "    best_ratio = 0\n",
        "    best_span = src\n",
        "    best_sec = \"NOT_FOUND\"\n",
        "    \n",
        "    for sec in section_cols:\n",
        "        sec_text = str(row.get(sec, \"\"))\n",
        "        if not sec_text or len(sec_text) < 5 or sec_text == \"nan\": continue\n",
        "        \n",
        "        # Optimization: Don't fuzzy match HUGE sections line by line if no n-gram overlap\n",
        "        # But for now, just split into \"phrases\" or sentences\n",
        "        # actually, standard difflib on the WHOLE section is surprisingly good at finding the block\n",
        "        \n",
        "        # Strategy A: Whole Section Fuzzy Search (Finds the best block)\n",
        "        matcher = difflib.SequenceMatcher(None, sec_text, src)\n",
        "        match = matcher.find_longest_match(0, len(sec_text), 0, len(src))\n",
        "        \n",
        "        if match.size > 0:\n",
        "            # Compute score against the CANDIDATE (cleaned), not unrelated junk\n",
        "            # But we only matched against 'src' so far. \n",
        "            # Let's extract the block and score it against 'cleaned'\n",
        "            \n",
        "            found_block = sec_text[match.a : match.a + match.size]\n",
        "            \n",
        "            # If the found block is essentially the cleaned source\n",
        "            # calculated ratio\n",
        "            \n",
        "            # Let's try matching against clean source too if different\n",
        "            current_ratio = match.size / len(src)\n",
        "            \n",
        "            # If match is long (>50 chars), it is almost certainly the right place\n",
        "            if match.size > 50:\n",
        "                current_ratio = max(current_ratio, 0.9)\n",
        "            \n",
        "            if current_ratio > best_ratio:\n",
        "                best_ratio = current_ratio\n",
        "                best_span = found_block\n",
        "                best_sec = sec\n",
        "\n",
        "        # Strategy B: Cleaned Source (Header Stripped) against Section\n",
        "        if len(candidates) > 1:\n",
        "            cleaned_src = candidates[1]\n",
        "            matcher_c = difflib.SequenceMatcher(None, sec_text, cleaned_src)\n",
        "            match_c = matcher_c.find_longest_match(0, len(sec_text), 0, len(cleaned_src))\n",
        "            \n",
        "            if match_c.size > 0:\n",
        "                ratio_c = match_c.size / len(cleaned_src)\n",
        "                if match_c.size > 40: ratio_c = max(ratio_c, 0.9)\n",
        "                \n",
        "                if ratio_c > best_ratio:\n",
        "                    best_ratio = ratio_c\n",
        "                    best_span = sec_text[match_c.a : match_c.a + match_c.size]\n",
        "                    best_sec = sec\n",
        "\n",
        "    # --- 3. DECISION ---\n",
        "    # Threshold 0.6 is safe if we have >40 chars overlap (handled above)\n",
        "    # For short strings, be stricter.\n",
        "    \n",
        "    threshold = 0.6\n",
        "    if len(best_span) < 20: threshold = 0.8\n",
        "    \n",
        "    if best_ratio > threshold:\n",
        "        return best_span, best_sec\n",
        "\n",
        "    return src, \"NOT_FOUND\"\n",
        "\n",
        "# Apply Logic\n",
        "print(\"Processing rows... This may take 2-3 minutes.\")\n",
        "df_fixed = df.copy()\n",
        "\n",
        "for col in source_cols:\n",
        "    feature_base = col.replace(\"_source_sentence\", \"\")\n",
        "    new_sec_col = f\"{feature_base}_section\"\n",
        "    print(f\"  Fixing {col} -> mapping to {new_sec_col}\")\n",
        "    \n",
        "    results = df_fixed.apply(lambda row: find_needle_in_sections(row, row[col]), axis=1, result_type='expand')\n",
        "    \n",
        "    df_fixed[col] = results[0]\n",
        "    df_fixed[new_sec_col] = results[1]\n",
        "\n",
        "print(\"Done processing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Validation Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying matches...\n",
            "  WARNING: registrant_name_source_sentence has 33 mismatches after fix.\n",
            "    [Diag] File: 738339_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: 'American Healthcare Management, Inc. (together, unless the context otherwise requires, with its subsidiaries, \"AHI\" or the \"Company\") is a health care services company engaged in the operation of 16 general acute care hospitals in nine states, with a total of 2,028 licensed beds.'\n",
            "    [Diag] In Section? False\n",
            "  WARNING: headquarters_city_source_sentence has 18 mismatches after fix.\n",
            "    [Diag] File: 738339_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: 'Also, the Company leases approximately 17,000 square feet of executive office space in Valley Forge Square, King of Prussia, Pennsylvania.'\n",
            "    [Diag] In Section? False\n",
            "  WARNING: headquarters_state_source_sentence has 16 mismatches after fix.\n",
            "    [Diag] File: 738339_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: 'Also, the Company leases approximately 17,000 square feet of executive office space in Valley Forge Square, King of Prussia, Pennsylvania.'\n",
            "    [Diag] In Section? False\n",
            "  WARNING: incorporation_state_source_sentence has 26 mismatches after fix.\n",
            "    [Diag] File: 46207_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: 'BUSINESS\\nHEI\\nHEI was incorporated in 1981 under the laws of the State of Hawaii and is a holding company with subsidiaries engaged in the electric utility, financial services, freight transportation, real estate development and other businesses, in each case primarily or exclusively in the State of Hawaii.'\n",
            "    [Diag] In Section? False\n",
            "  WARNING: incorporation_year_source_sentence has 23 mismatches after fix.\n",
            "    [Diag] File: 46207_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: 'BUSINESS\\nHEI\\nHEI was incorporated in 1981 under the laws of the State of Hawaii and is a holding company with subsidiaries engaged in the electric utility, financial services, freight transportation, real estate development and other businesses, in each case primarily or exclusively in the State of Hawaii.'\n",
            "    [Diag] In Section? False\n",
            "  WARNING: employees_count_total_source_sentence has 23 mismatches after fix.\n",
            "    [Diag] File: 800287_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: 'EMPLOYEES\\nThe Company (excluding Rust) employed approximately 4,400 persons at December 31, 1993.'\n",
            "    [Diag] In Section? False\n",
            "  WARNING: employees_count_full_time_source_sentence has 10 mismatches after fix.\n",
            "    [Diag] File: 791445_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: 'See \"Gaming Regulations -- Employees.\" The Company has no employees.'\n",
            "    [Diag] In Section? False\n",
            "  WARNING: ceo_lastname_source_sentence has 48 mismatches after fix.\n",
            "    [Diag] File: 75042_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: \"SIGNATURES\\r\\nPursuant to the requirements of Section 13 or 15(d) of the Securities Exchange Act of 1934, the Registrant has duly cause this report to be signed on its behalf by the undersigned, thereunto duly authorized.\\r\\nOSHKOSH B'GOSH, INC.\\r\\nBY: DOUGLAS W. HYDE President and Chief Executive Officer\"\n",
            "    [Diag] In Section? False\n",
            "  WARNING: holder_record_amount_source_sentence has 16 mismatches after fix.\n",
            "    [Diag] File: 711404_1993.txt\n",
            "    [Diag] Section: NOT_FOUND\n",
            "    [Diag] Type: <class 'str'>\n",
            "    [Diag] Sent: 'At December 31, 1993 and 1992 there were 4,550 and 4,902 common stockholders of record, respectively.'\n",
            "    [Diag] In Section? False\n",
            "\n",
            "Completed with 213 issues remaining (check warnings above).\n"
          ]
        }
      ],
      "source": [
        "print(\"Verifying matches...\")\n",
        "mismatches = 0\n",
        "\n",
        "for col in source_cols:\n",
        "    feature_base = col.replace(\"_source_sentence\", \"\")\n",
        "    sec_col_name = f\"{feature_base}_section\"\n",
        "    \n",
        "    # Check every row where source is not null\n",
        "    valid_rows = df_fixed[df_fixed[col].notna()].copy()\n",
        "    \n",
        "    def verify_row(r):\n",
        "        s_sent = r[col]\n",
        "        s_sec_name = r[sec_col_name]\n",
        "        \n",
        "        if pd.isna(s_sec_name) or s_sec_name == \"NOT_FOUND\":\n",
        "            return False # Should have been found ideally\n",
        "            \n",
        "        # The real test: Is string IN section text?\n",
        "        sec_text = str(r.get(s_sec_name, \"\"))\n",
        "        return str(s_sent) in sec_text\n",
        "\n",
        "    results = valid_rows.apply(verify_row, axis=1)\n",
        "    fails = (~results).sum()\n",
        "    \n",
        "    if fails > 0:\n",
        "        print(f\"  WARNING: {col} has {fails} mismatches after fix.\")\n",
        "        mismatches += fails\n",
        "        \n",
        "        # Show first failure diagnostic\n",
        "        bad_row = valid_rows[~results].iloc[0]\n",
        "        print(f\"    [Diag] File: {bad_row['filename']}\")\n",
        "        print(f\"    [Diag] Section: {bad_row[sec_col_name]}\")\n",
        "        print(f\"    [Diag] Type: {type(bad_row[col])}\")\n",
        "        print(f\"    [Diag] Sent: {repr(bad_row[col])}\")\n",
        "        \n",
        "        sec_val = str(bad_row.get(bad_row[sec_col_name], ''))\n",
        "        print(f\"    [Diag] In Section? {str(bad_row[col]) in sec_val}\")\n",
        "    else:\n",
        "        print(f\"  {col}: ALL CLEAN.\")\n",
        "\n",
        "if mismatches == 0:\n",
        "    print(\"\\nSUCCESS: All source sentences are now perfect substrings of their assigned sections.\")\n",
        "else:\n",
        "    print(f\"\\nCompleted with {mismatches} issues remaining (check warnings above).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save\n",
        "if mismatches == 0:\n",
        "    df_fixed.to_csv(OUTPUT_FILE, index=False)\n",
        "    print(f\"Saved validated file to {OUTPUT_FILE}\")\n",
        "else:\n",
        "    print(\"Output file NOT saved because verification failed (safety first).\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
