{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDGAR Ground Truth Extraction Tournament\n",
    "\n",
    "This notebook compares three strategies for extracting data from SEC 10-K filings:\n",
    "\n",
    "1. **Pure Regex**: Fast, baseline method using regular expressions.\n",
    "2. **Hybrid (Locator + LLM)**: Uses keywords/lax regex to find the relevant context, then asks an LLM (Qwen 2.5) to extract the exact answer.\n",
    "3. **Pure LLM**: Feeds the entire section (truncated to fit context) to the LLM.\n",
    "\n",
    "### Goal\n",
    "Determine which method provides the best trade-off between accuracy and cost/speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (2.7.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (2.1.4+dfsg)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1.0.0 (from datasets)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate) (5.9.8)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting anyio (from httpx<1.0.0->datasets)\n",
      "  Downloading anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets) (2023.11.17)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0->datasets)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx<1.0.0->datasets) (3.6)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.10.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (2.0.7)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m128.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.12.0-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.4/113.4 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, safetensors, regex, pyarrow, propcache, multidict, hf-xet, h11, frozenlist, dill, charset_normalizer, bitsandbytes, anyio, aiohappyeyeballs, yarl, requests, multiprocess, httpcore, aiosignal, huggingface-hub, httpx, aiohttp, tokenizers, accelerate, transformers, datasets\n",
      "Successfully installed accelerate-1.12.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 bitsandbytes-0.49.0 charset_normalizer-3.4.4 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.36.0 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Installation & Setup\n",
    "# Uncomment the line below if you need to install these libraries\n",
    "!pip install torch transformers datasets pandas tqdm accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Import installed libraries\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Configure display to show full text in pandas\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Setup Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 00:52:46.007117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766105566.024744    4573 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766105566.030719    4573 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766105566.047963    4573 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766105566.047976    4573 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766105566.047979    4573 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766105566.047980    4573 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Load Model (Qwen 2.5-7B-Instruct)\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "try:\n",
    "    print(f\"Loading {MODEL_NAME}...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"Model Loaded Successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Make sure you have a GPU enabled and the libraries installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Unified Configuration (Question Bank)\n",
    "# Contains logic for ALL strategies: Regex Patterns, Locator Keywords, and LLM Prompts.\n",
    "\n",
    "QUESTION_BANK = {\n",
    "    \"section_1\": [\n",
    "        {\n",
    "            \"id\": \"incorporation_state\",\n",
    "            \"prompt\": \"In which U.S. state was this company incorporated? Answer with ONLY the state name.\",\n",
    "            # Regex: Matches \"Incorporated in the State of Delaware\"\n",
    "            \"extract_regex\": r\"(?i)(?:[Ii]ncorporated|[Oo]rganized)(?: (?:under the laws of|in))? (?:the [Ss]tate of\\s*)?([A-Z][a-z]+(?: [A-Z][a-z]+)*)\",\n",
    "            # Hybrid Locator: Keywords to find the paragraph\n",
    "            \"keywords\": [\"incorporated\", \"organized under\", \"laws of the state\", \"formed under\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"incorporation_year\",\n",
    "            \"prompt\": \"In what year was this company incorporated? Answer with ONLY the year.\",\n",
    "            # Regex: Matches \"incorporated ... in 1985\"\n",
    "            \"extract_regex\": r\"(?i)(?:incorporated|founded|organized).*?in (19\\d{2}|20\\d{2})\",\n",
    "            \"keywords\": [\"incorporated\", \"founded\", \"organized\", \"formed\", \"year\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"employee_count\",\n",
    "            \"prompt\": \"How many full-time employees does the company have? Answer with ONLY the number.\",\n",
    "            # Regex: Matches \"approximately 5,000 employees\"\n",
    "            \"extract_regex\": r\"(?i)(?:approximately|approx\\.|had|total of|employ)\\s+([0-9,]+)(?:\\s+full-time)?\\s+employees\",\n",
    "            \"keywords\": [\"employees\", \"full-time\", \"employed\", \"workforce\", \"persons\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"fiscal_year_end\",\n",
    "            \"prompt\": \"On what date does the company's fiscal year end? Answer with Month and Day (e.g., 'December 31').\",\n",
    "            # Regex: Matches \"fiscal year ends December 31\"\n",
    "            \"extract_regex\": r\"(?i)fiscal year end(?:ed|s)(?:\\s+on)?\\s+([A-Z][a-z]+ \\d{1,2})\",\n",
    "            \"keywords\": [\"fiscal year end\", \"fiscal year ends\", \"fiscal year ended\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"company_product\",\n",
    "            \"prompt\": \"What is the main product, service, or business activity of this company? Answer in 2-5 words.\",\n",
    "            # Regex: Matches \"engaged in the business of...\"\n",
    "            \"extract_regex\": r\"(?i)engaged in the (?:business|manufacture|sale|development) of ([^.;]+)\",\n",
    "            \"keywords\": [\"engaged in\", \"business of\", \"manufacture\", \"sale of\", \"products\"]\n",
    "        }\n",
    "    ],\n",
    "    \"section_2\": [\n",
    "         {\n",
    "            \"id\": \"headquarters_state\",\n",
    "            \"prompt\": \"In which U.S. state are the company's principal executive offices located? Answer with ONLY the state name.\",\n",
    "            # Regex: Matches \"executive offices ... [State] [Zip]\"\n",
    "            \"extract_regex\": r\"(?i)executive offices.*?,[\\s\\r\\n]+([A-Z][a-z]+(?: [A-Z][a-z]+)*)[\\s\\r\\n]+\\d{5}\",\n",
    "            \"keywords\": [\"executive offices\", \"headquarters\", \"principal offices\"]\n",
    "        }\n",
    "    ],\n",
    "    \"section_10\": [\n",
    "       {\n",
    "            \"id\": \"ceo_lastname\",\n",
    "            \"prompt\": \"What is the Last Name of the current CEO? Answer with ONLY the last name.\",\n",
    "            # Regex: Matches \"Mr. Smith ... CEO\"\n",
    "            \"extract_regex\": r\"(?i)(?:Mr\\.|Ms\\.|Mrs\\.|Dr\\.)\\s+([A-Z][a-z]+).*?(?:Chief Executive Officer|CEO)\",\n",
    "            \"keywords\": [\"chief executive officer\", \"ceo\", \"serves as\"]\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"ceo_fullname_backup\",\n",
    "            \"prompt\": \"What is the Full Name of the current CEO? Answer with ONLY the full name.\",\n",
    "             # Regex: Matches Name followed by CEO title\n",
    "            \"extract_regex\": r\"(?m)([A-Z][a-z]+ [A-Z]\\.? [A-Z][a-z]+|[A-Z][a-z]+ [A-Z][a-z]+)\\s*,?\\s*(?:Chief Executive Officer|CEO)\",\n",
    "             \"keywords\": [\"chief executive officer\", \"ceo\"]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Helper Functions (LLM & Context)\n",
    "\n",
    "\n",
    "def get_focused_context(full_text, keywords, window_size=1500):\n",
    "    \"\"\"HYBRID: Finds the best keyword match and grabs a context window around it.\"\"\"\n",
    "    if not full_text:\n",
    "        return \"\"\n",
    "    text_lower = full_text.lower()\n",
    "\n",
    "    best_word_index = -1\n",
    "    # Simple heuristic: find first occurrence of any keyword\n",
    "    # (Could be improved to find 'densest' keyword region)\n",
    "    for word in keywords:\n",
    "        word_index = text_lower.find(word.lower())\n",
    "        if word_index != -1:\n",
    "            best_word_index = word_index\n",
    "            break\n",
    "\n",
    "    if best_word_index != -1:\n",
    "        start = max(0, best_word_index - (window_size // 2))\n",
    "        end = min(len(full_text), best_word_index + (window_size // 2))\n",
    "        return full_text[start:end]\n",
    "\n",
    "    # Fallback: Return start of text if no keywords found\n",
    "    return full_text[:window_size]\n",
    "\n",
    "\n",
    "def ask_llm(context, prompt, model, tokenizer, max_new_tokens=50):\n",
    "    \"\"\"Sends a prompt to the LLM with the given context.\"\"\"\n",
    "    if not context or not context.strip():\n",
    "        return None\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Read this SEC 10-K filing excerpt and answer the question. \\nContext: \\\"{context}\\\"\\n\\nQuestion: {prompt} \\nIf the information is not present in the context, reply with 'NULL'.\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, max_new_tokens=max_new_tokens, do_sample=False\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
    "    )\n",
    "    return response.strip().split(\"\\n\")[0]  # Take first line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Strategy Implementations\n",
    "\n",
    "\n",
    "def extract_pure_regex(doc):\n",
    "    \"\"\"Strategy 1: Pure Regex Matching\"\"\"\n",
    "    results = {}\n",
    "    for section, questions in QUESTION_BANK.items():\n",
    "        text = doc.get(section, \"\")\n",
    "        if not text:\n",
    "            for q in questions:\n",
    "                results[q[\"id\"]] = None\n",
    "            continue\n",
    "\n",
    "        for q in questions:\n",
    "            match = re.search(q[\"extract_regex\"], str(text), re.DOTALL)\n",
    "            if match:\n",
    "                # Clean up match\n",
    "                clean_ans = match.group(1).strip(\" .,;\")[:150]\n",
    "                results[q[\"id\"]] = clean_ans\n",
    "            else:\n",
    "                results[q[\"id\"]] = None\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_hybrid_llm(doc, model, tokenizer):\n",
    "    \"\"\"Strategy 2: Hybrid (Lax Keyword Locator + LLM Extraction)\"\"\"\n",
    "    results = {}\n",
    "    for section, questions in QUESTION_BANK.items():\n",
    "        full_text = doc.get(section, \"\")\n",
    "\n",
    "        for q in questions:\n",
    "            # 1. Locate relevant generic context\n",
    "            # Use keywords to find the 1000 char window\n",
    "            context = get_focused_context(full_text, q[\"keywords\"])\n",
    "\n",
    "            # 2. Extract with LLM\n",
    "            answer = ask_llm(context, q[\"prompt\"], model, tokenizer)\n",
    "            results[q[\"id\"]] = answer\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_pure_llm(doc, model, tokenizer, max_context_chars=12000):\n",
    "    \"\"\"Strategy 3: Pure LLM (Feed entire section context)\"\"\"\n",
    "    results = {}\n",
    "    for section, questions in QUESTION_BANK.items():\n",
    "        full_text = doc.get(section, \"\")\n",
    "        if not full_text:\n",
    "            for q in questions:\n",
    "                results[q[\"id\"]] = None\n",
    "            continue\n",
    "\n",
    "        # Truncate to fit in context if necessary (simple truncation)\n",
    "        context = full_text[:max_context_chars]\n",
    "\n",
    "        # Note: For just LLM setup, might run one big prompt asking for ALL fields at once to save tokens.\n",
    "        for q in questions:\n",
    "            answer = ask_llm(context, q[\"prompt\"], model, tokenizer)\n",
    "            results[q[\"id\"]] = answer\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. The Tournament Loop Function\n",
    "# Runs detailed comparison on a small batch of documents\n",
    "\n",
    "\n",
    "def create_df_tournament(model, tokenizer, TARGET_DOCS=5):\n",
    "    print(\"Loading Data Stream...\")\n",
    "    dataset = load_dataset(\n",
    "        \"c3po-ai/edgar-corpus\",\n",
    "        \"default\",\n",
    "        split=\"train\",\n",
    "        streaming=True,\n",
    "        revision=\"refs/convert/parquet\",\n",
    "    )\n",
    "\n",
    "    comparison_data = []\n",
    "\n",
    "    print(f\"Running Tournament on {TARGET_DOCS} docs...\")\n",
    "\n",
    "    for i, doc in enumerate(dataset):\n",
    "        if i >= TARGET_DOCS:\n",
    "            break\n",
    "\n",
    "        print(f\"Processing {doc.get('filename', f'Doc {i}')}...\")\n",
    "\n",
    "        res_regex = extract_pure_regex(doc)\n",
    "        res_hybrid = extract_hybrid_llm(doc, model, tokenizer)\n",
    "        res_pure = extract_pure_llm(doc, model, tokenizer)\n",
    "\n",
    "        # Combine results\n",
    "        row = {\n",
    "            \"filename\": doc.get(\"filename\"),\n",
    "            \"cik\": doc.get(\"cik\"),\n",
    "            \"year\": doc.get(\"year\"),\n",
    "        }\n",
    "\n",
    "        # Add all fields dynamically (Regex, Hybrid, PureLLM)\n",
    "        # iterate over all the extracted keys to ensure we get everything\n",
    "        all_keys = list(res_regex.keys())\n",
    "\n",
    "        for key in all_keys:\n",
    "            # Shorten key for column name width\n",
    "            short_key = (\n",
    "                key.replace(\"incorporation_\", \"Inc_\")\n",
    "                .replace(\"headquarters_\", \"HQ_\")\n",
    "                .replace(\"company_\", \"Hz_\")\n",
    "            ) \n",
    "            val_hybrid = res_hybrid.get(key) \n",
    "            val_pure = res_pure.get(key) \n",
    "            row[f\"{short_key}_Re\"] = res_regex.get(key) \n",
    "            row[f\"{short_key}_Hy\"] = val_hybrid \n",
    "            row[f\"{short_key}_Lu\"] = val_pure\n",
    "\n",
    "            # --- NEW CODE: Consensus Check ---\n",
    "            # Returns True if both are not None and match, False otherwise\n",
    "            if val_hybrid and val_pure:\n",
    "                # loose match (ignoring case/whitespace)\n",
    "                is_match = val_hybrid.strip().lower() == val_pure.strip().lower()\n",
    "                row[f\"{short_key}_Match\"] = \"1\" if is_match else \"0\"\n",
    "            else:\n",
    "                row[f\"{short_key}_Match\"] = \"NULL\"  # distinct from mismatch\n",
    "\n",
    "        comparison_data.append(row)\n",
    "\n",
    "    df_tournament = pd.DataFrame(comparison_data)\n",
    "    df_tournament.fillna(\"NULL\", inplace=True)\n",
    "    print(\"\\n--- TOURNAMENT RESULTS ---\")\n",
    "    display(df_tournament)\n",
    "    return df_tournament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Stream...\n",
      "Running Tournament on 5 docs...\n",
      "Processing 92116_1993.txt...\n",
      "Processing 103730_1993.txt...\n",
      "Processing 100240_1993.txt...\n",
      "Processing 58696_1993.txt...\n",
      "Processing 46207_1993.txt...\n",
      "\n",
      "--- TOURNAMENT RESULTS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cik</th>\n",
       "      <th>year</th>\n",
       "      <th>Inc_state_Re</th>\n",
       "      <th>Inc_state_Hy</th>\n",
       "      <th>Inc_state_Lu</th>\n",
       "      <th>Inc_state_Match</th>\n",
       "      <th>Inc_year_Re</th>\n",
       "      <th>Inc_year_Hy</th>\n",
       "      <th>Inc_year_Lu</th>\n",
       "      <th>...</th>\n",
       "      <th>HQ_state_Lu</th>\n",
       "      <th>HQ_state_Match</th>\n",
       "      <th>ceo_lastname_Re</th>\n",
       "      <th>ceo_lastname_Hy</th>\n",
       "      <th>ceo_lastname_Lu</th>\n",
       "      <th>ceo_lastname_Match</th>\n",
       "      <th>ceo_fullname_backup_Re</th>\n",
       "      <th>ceo_fullname_backup_Hy</th>\n",
       "      <th>ceo_fullname_backup_Lu</th>\n",
       "      <th>ceo_fullname_backup_Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92116_1993.txt</td>\n",
       "      <td>92116</td>\n",
       "      <td>1993</td>\n",
       "      <td>in</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "      <td>1929</td>\n",
       "      <td>1929</td>\n",
       "      <td>1929</td>\n",
       "      <td>...</td>\n",
       "      <td>California</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103730_1993.txt</td>\n",
       "      <td>103730</td>\n",
       "      <td>1993</td>\n",
       "      <td>Delaware in</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>1</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100240_1993.txt</td>\n",
       "      <td>100240</td>\n",
       "      <td>1993</td>\n",
       "      <td>Georgia in</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>...</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58696_1993.txt</td>\n",
       "      <td>58696</td>\n",
       "      <td>1993</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>...</td>\n",
       "      <td>Florida</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46207_1993.txt</td>\n",
       "      <td>46207</td>\n",
       "      <td>1993</td>\n",
       "      <td>in</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>1981</td>\n",
       "      <td>1981</td>\n",
       "      <td>...</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Twigg-Smith</td>\n",
       "      <td>Lewis</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Thurston Twigg-Smith</td>\n",
       "      <td>Thurston Twigg-Smith</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename     cik  year Inc_state_Re Inc_state_Hy Inc_state_Lu  \\\n",
       "0   92116_1993.txt   92116  1993           in   California   California   \n",
       "1  103730_1993.txt  103730  1993  Delaware in     Delaware     Delaware   \n",
       "2  100240_1993.txt  100240  1993   Georgia in      Georgia      Georgia   \n",
       "3   58696_1993.txt   58696  1993         NULL         NULL         NULL   \n",
       "4   46207_1993.txt   46207  1993           in       Hawaii       Hawaii   \n",
       "\n",
       "  Inc_state_Match Inc_year_Re Inc_year_Hy Inc_year_Lu  ... HQ_state_Lu  \\\n",
       "0               1        1929        1929        1929  ...  California   \n",
       "1               1        1962        1962        1962  ...        NULL   \n",
       "2               1        1965        1965        1965  ...     Georgia   \n",
       "3               1        NULL        NULL        NULL  ...     Florida   \n",
       "4               1        1981        1981        1981  ...      Hawaii   \n",
       "\n",
       "  HQ_state_Match ceo_lastname_Re ceo_lastname_Hy ceo_lastname_Lu  \\\n",
       "0              0            NULL            NULL            NULL   \n",
       "1              1            NULL            NULL            NULL   \n",
       "2              1            NULL            NULL            NULL   \n",
       "3              1            NULL            NULL            NULL   \n",
       "4              1            NULL     Twigg-Smith           Lewis   \n",
       "\n",
       "  ceo_lastname_Match ceo_fullname_backup_Re ceo_fullname_backup_Hy  \\\n",
       "0                  1                   NULL                   NULL   \n",
       "1               NULL                   NULL                   NULL   \n",
       "2                  1                   NULL                   NULL   \n",
       "3               NULL                   NULL                   NULL   \n",
       "4                  0                   NULL   Thurston Twigg-Smith   \n",
       "\n",
       "  ceo_fullname_backup_Lu ceo_fullname_backup_Match  \n",
       "0                   NULL                         1  \n",
       "1                   NULL                      NULL  \n",
       "2                   NULL                         1  \n",
       "3                   NULL                      NULL  \n",
       "4   Thurston Twigg-Smith                         1  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Run Preview ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cik</th>\n",
       "      <th>year</th>\n",
       "      <th>Inc_state_Re</th>\n",
       "      <th>Inc_state_Hy</th>\n",
       "      <th>Inc_state_Lu</th>\n",
       "      <th>Inc_state_Match</th>\n",
       "      <th>Inc_year_Re</th>\n",
       "      <th>Inc_year_Hy</th>\n",
       "      <th>Inc_year_Lu</th>\n",
       "      <th>...</th>\n",
       "      <th>HQ_state_Lu</th>\n",
       "      <th>HQ_state_Match</th>\n",
       "      <th>ceo_lastname_Re</th>\n",
       "      <th>ceo_lastname_Hy</th>\n",
       "      <th>ceo_lastname_Lu</th>\n",
       "      <th>ceo_lastname_Match</th>\n",
       "      <th>ceo_fullname_backup_Re</th>\n",
       "      <th>ceo_fullname_backup_Hy</th>\n",
       "      <th>ceo_fullname_backup_Lu</th>\n",
       "      <th>ceo_fullname_backup_Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92116_1993.txt</td>\n",
       "      <td>92116</td>\n",
       "      <td>1993</td>\n",
       "      <td>in</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "      <td>1929</td>\n",
       "      <td>1929</td>\n",
       "      <td>1929</td>\n",
       "      <td>...</td>\n",
       "      <td>California</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103730_1993.txt</td>\n",
       "      <td>103730</td>\n",
       "      <td>1993</td>\n",
       "      <td>Delaware in</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>1</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100240_1993.txt</td>\n",
       "      <td>100240</td>\n",
       "      <td>1993</td>\n",
       "      <td>Georgia in</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>...</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58696_1993.txt</td>\n",
       "      <td>58696</td>\n",
       "      <td>1993</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>...</td>\n",
       "      <td>Florida</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46207_1993.txt</td>\n",
       "      <td>46207</td>\n",
       "      <td>1993</td>\n",
       "      <td>in</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>1981</td>\n",
       "      <td>1981</td>\n",
       "      <td>...</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Twigg-Smith</td>\n",
       "      <td>Lewis</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Thurston Twigg-Smith</td>\n",
       "      <td>Thurston Twigg-Smith</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename     cik  year Inc_state_Re Inc_state_Hy Inc_state_Lu  \\\n",
       "0   92116_1993.txt   92116  1993           in   California   California   \n",
       "1  103730_1993.txt  103730  1993  Delaware in     Delaware     Delaware   \n",
       "2  100240_1993.txt  100240  1993   Georgia in      Georgia      Georgia   \n",
       "3   58696_1993.txt   58696  1993         NULL         NULL         NULL   \n",
       "4   46207_1993.txt   46207  1993           in       Hawaii       Hawaii   \n",
       "\n",
       "  Inc_state_Match Inc_year_Re Inc_year_Hy Inc_year_Lu  ... HQ_state_Lu  \\\n",
       "0               1        1929        1929        1929  ...  California   \n",
       "1               1        1962        1962        1962  ...        NULL   \n",
       "2               1        1965        1965        1965  ...     Georgia   \n",
       "3               1        NULL        NULL        NULL  ...     Florida   \n",
       "4               1        1981        1981        1981  ...      Hawaii   \n",
       "\n",
       "  HQ_state_Match ceo_lastname_Re ceo_lastname_Hy ceo_lastname_Lu  \\\n",
       "0              0            NULL            NULL            NULL   \n",
       "1              1            NULL            NULL            NULL   \n",
       "2              1            NULL            NULL            NULL   \n",
       "3              1            NULL            NULL            NULL   \n",
       "4              1            NULL     Twigg-Smith           Lewis   \n",
       "\n",
       "  ceo_lastname_Match ceo_fullname_backup_Re ceo_fullname_backup_Hy  \\\n",
       "0                  1                   NULL                   NULL   \n",
       "1               NULL                   NULL                   NULL   \n",
       "2                  1                   NULL                   NULL   \n",
       "3               NULL                   NULL                   NULL   \n",
       "4                  0                   NULL   Thurston Twigg-Smith   \n",
       "\n",
       "  ceo_fullname_backup_Lu ceo_fullname_backup_Match  \n",
       "0                   NULL                         1  \n",
       "1                   NULL                      NULL  \n",
       "2                   NULL                         1  \n",
       "3                   NULL                      NULL  \n",
       "4   Thurston Twigg-Smith                         1  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Run the tournament and save the results to a CSV file. \n",
    "\n",
    "# sample so only do 5 \n",
    "num_of_docs = 5 \n",
    "df_sample = create_df_tournament(model, tokenizer)  \n",
    "print(\"\\n--- Test Run Preview ---\") \n",
    "df_sample.head()  \n",
    "# Feel free to add code blocks below to analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. If fine create sample doc csv\n",
    "df_sample.to_csv(\"edgar_tournament_sample.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Stream...\n",
      "Running Tournament on 250 docs...\n",
      "Processing 92116_1993.txt...\n",
      "Processing 103730_1993.txt...\n",
      "Processing 100240_1993.txt...\n",
      "Processing 58696_1993.txt...\n",
      "Processing 46207_1993.txt...\n",
      "Processing 60041_1993.txt...\n",
      "Processing 55387_1993.txt...\n",
      "Processing 40878_1993.txt...\n",
      "Processing 53540_1993.txt...\n",
      "Processing 800287_1993.txt...\n",
      "Processing 50178_1993.txt...\n",
      "Processing 725625_1993.txt...\n",
      "Processing 66479_1993.txt...\n",
      "Processing 700674_1993.txt...\n",
      "Processing 854094_1993.txt...\n",
      "Processing 846972_1993.txt...\n",
      "Processing 59478_1993.txt...\n",
      "Processing 5550_1993.txt...\n",
      "Processing 743443_1993.txt...\n",
      "Processing 22767_1993.txt...\n",
      "Processing 738339_1993.txt...\n",
      "Processing 29854_1993.txt...\n",
      "Processing 97210_1993.txt...\n",
      "Processing 7383_1993.txt...\n",
      "Processing 80661_1993.txt...\n",
      "Processing 711404_1993.txt...\n",
      "Processing 75042_1993.txt...\n",
      "Processing 18497_1993.txt...\n",
      "Processing 33565_1993.txt...\n",
      "Processing 745287_1993.txt...\n",
      "Processing 93469_1993.txt...\n",
      "Processing 354869_1993.txt...\n",
      "Processing 36966_1993.txt...\n",
      "Processing 799036_1993.txt...\n",
      "Processing 25890_1993.txt...\n",
      "Processing 846902_1993.txt...\n",
      "Processing 764764_1993.txt...\n",
      "Processing 717605_1993.txt...\n",
      "Processing 92487_1993.txt...\n",
      "Processing 79166_1993.txt...\n",
      "Processing 355883_1993.txt...\n",
      "Processing 865227_1993.txt...\n",
      "Processing 101320_1993.txt...\n",
      "Processing 64605_1993.txt...\n",
      "Processing 884033_1993.txt...\n",
      "Processing 17797_1993.txt...\n",
      "Processing 216228_1993.txt...\n",
      "Processing 791445_1993.txt...\n",
      "Processing 719264_1993.txt...\n",
      "Processing 92050_1993.txt...\n",
      "Processing 788043_1993.txt...\n",
      "Processing 29989_1993.txt...\n",
      "Processing 702163_1993.txt...\n",
      "Processing 75252_1993.txt...\n",
      "Processing 65660_1993.txt...\n",
      "Processing 108721_1993.txt...\n",
      "Processing 37748_1993.txt...\n",
      "Processing 701345_1993.txt...\n",
      "Processing 876858_1993.txt...\n",
      "Processing 310431_1993.txt...\n",
      "Processing 764037_1993.txt...\n",
      "Processing 201461_1993.txt...\n",
      "Processing 26780_1993.txt...\n",
      "Processing 741612_1993.txt...\n",
      "Processing 104669_1993.txt...\n",
      "Processing 797463_1993.txt...\n",
      "Processing 51720_1993.txt...\n",
      "Processing 100826_1993.txt...\n",
      "Processing 40533_1993.txt...\n",
      "Processing 101830_1993.txt...\n",
      "Processing 33619_1993.txt...\n",
      "Processing 215419_1993.txt...\n",
      "Processing 40454_1993.txt...\n",
      "Processing 77227_1993.txt...\n",
      "Processing 65358_1993.txt...\n",
      "Processing 54502_1993.txt...\n",
      "Processing 37785_1993.txt...\n",
      "Processing 893486_1993.txt...\n",
      "Processing 91576_1993.txt...\n",
      "Processing 790603_1993.txt...\n",
      "Processing 94601_1993.txt...\n",
      "Processing 92088_1993.txt...\n",
      "Processing 832427_1993.txt...\n",
      "Processing 83047_1993.txt...\n",
      "Processing 893928_1993.txt...\n",
      "Processing 789292_1993.txt...\n",
      "Processing 95301_1993.txt...\n",
      "Processing 7536_1993.txt...\n",
      "Processing 854884_1993.txt...\n",
      "Processing 10456_1993.txt...\n",
      "Processing 53347_1993.txt...\n",
      "Processing 882240_1993.txt...\n",
      "Processing 74208_1993.txt...\n",
      "Processing 352363_1993.txt...\n",
      "Processing 815917_1993.txt...\n",
      "Processing 14957_1993.txt...\n",
      "Processing 9892_1993.txt...\n",
      "Processing 860713_1993.txt...\n",
      "Processing 66904_1993.txt...\n",
      "Processing 740763_1993.txt...\n",
      "Processing 315189_1993.txt...\n",
      "Processing 93675_1993.txt...\n",
      "Processing 847322_1993.txt...\n",
      "Processing 836400_1993.txt...\n",
      "Processing 771950_1993.txt...\n",
      "Processing 20164_1993.txt...\n",
      "Processing 15393_1993.txt...\n",
      "Processing 36270_1993.txt...\n",
      "Processing 40874_1993.txt...\n",
      "Processing 812701_1993.txt...\n",
      "Processing 81061_1993.txt...\n",
      "Processing 92195_1993.txt...\n",
      "Processing 828530_1993.txt...\n",
      "Processing 67646_1993.txt...\n",
      "Processing 49423_1993.txt...\n",
      "Processing 814153_1993.txt...\n",
      "Processing 77242_1993.txt...\n",
      "Processing 20947_1993.txt...\n",
      "Processing 821026_1993.txt...\n",
      "Processing 716006_1993.txt...\n",
      "Processing 95395_1993.txt...\n",
      "Processing 793421_1993.txt...\n",
      "Processing 2024_1993.txt...\n",
      "Processing 33798_1993.txt...\n",
      "Processing 90185_1993.txt...\n",
      "Processing 36377_1993.txt...\n",
      "Processing 721083_1993.txt...\n",
      "Processing 81100_1993.txt...\n",
      "Processing 100783_1993.txt...\n",
      "Processing 766829_1993.txt...\n",
      "Processing 88204_1993.txt...\n",
      "Processing 16906_1993.txt...\n",
      "Processing 766701_1993.txt...\n",
      "Processing 30547_1993.txt...\n",
      "Processing 73124_1993.txt...\n",
      "Processing 46080_1993.txt...\n",
      "Processing 740868_1993.txt...\n",
      "Processing 708823_1993.txt...\n",
      "Processing 277577_1993.txt...\n",
      "Processing 49071_1993.txt...\n",
      "Processing 77943_1993.txt...\n",
      "Processing 67686_1993.txt...\n",
      "Processing 30573_1993.txt...\n",
      "Processing 52466_1993.txt...\n",
      "Processing 799319_1993.txt...\n",
      "Processing 60026_1993.txt...\n",
      "Processing 72909_1993.txt...\n",
      "Processing 806086_1993.txt...\n",
      "Processing 801124_1993.txt...\n",
      "Processing 105839_1993.txt...\n",
      "Processing 25445_1993.txt...\n",
      "Processing 13239_1993.txt...\n",
      "Processing 311094_1993.txt...\n",
      "Processing 47129_1993.txt...\n",
      "Processing 205402_1993.txt...\n",
      "Processing 36326_1993.txt...\n",
      "Processing 859257_1993.txt...\n",
      "Processing 79879_1993.txt...\n",
      "Processing 42293_1993.txt...\n",
      "Processing 3153_1993.txt...\n",
      "Processing 9548_1993.txt...\n",
      "Processing 311871_1993.txt...\n",
      "Processing 354396_1993.txt...\n",
      "Processing 96943_1993.txt...\n",
      "Processing 728586_1993.txt...\n",
      "Processing 93542_1993.txt...\n",
      "Processing 96638_1993.txt...\n",
      "Processing 99193_1993.txt...\n",
      "Processing 52477_1993.txt...\n",
      "Processing 277821_1993.txt...\n",
      "Processing 72903_1993.txt...\n",
      "Processing 86772_1993.txt...\n",
      "Processing 30554_1993.txt...\n",
      "Processing 276999_1993.txt...\n",
      "Processing 73902_1993.txt...\n",
      "Processing 774203_1993.txt...\n",
      "Processing 7323_1993.txt...\n",
      "Processing 55785_1993.txt...\n",
      "Processing 24454_1993.txt...\n",
      "Processing 313616_1993.txt...\n",
      "Processing 79732_1993.txt...\n",
      "Processing 732714_1993.txt...\n",
      "Processing 352947_1993.txt...\n",
      "Processing 740582_1993.txt...\n",
      "Processing 74928_1993.txt...\n",
      "Processing 72971_1993.txt...\n",
      "Processing 103392_1993.txt...\n",
      "Processing 50548_1993.txt...\n",
      "Processing 101382_1993.txt...\n",
      "Processing 840216_1993.txt...\n",
      "Processing 107832_1993.txt...\n",
      "Processing 714655_1993.txt...\n",
      "Processing 36995_1993.txt...\n",
      "Processing 740694_1993.txt...\n",
      "Processing 35527_1993.txt...\n",
      "Processing 814677_1993.txt...\n",
      "Processing 100923_1993.txt...\n",
      "Processing 66740_1993.txt...\n",
      "Processing 25885_1993.txt...\n",
      "Processing 92236_1993.txt...\n",
      "Processing 200245_1993.txt...\n",
      "Processing 310433_1993.txt...\n",
      "Processing 860004_1993.txt...\n",
      "Processing 754737_1993.txt...\n",
      "Processing 74783_1993.txt...\n",
      "Processing 1800_1993.txt...\n",
      "Processing 105418_1993.txt...\n",
      "Processing 711513_1993.txt...\n",
      "Processing 48305_1993.txt...\n",
      "Processing 711604_1993.txt...\n",
      "Processing 9659_1993.txt...\n",
      "Processing 9534_1993.txt...\n",
      "Processing 59198_1993.txt...\n",
      "Processing 351145_1993.txt...\n",
      "Processing 51296_1993.txt...\n",
      "Processing 15840_1993.txt...\n",
      "Processing 71180_1993.txt...\n",
      "Processing 34501_1993.txt...\n",
      "Processing 7431_1993.txt...\n",
      "Processing 310158_1993.txt...\n",
      "Processing 7649_1993.txt...\n",
      "Processing 25600_1993.txt...\n",
      "Processing 34285_1993.txt...\n",
      "Processing 4310_1993.txt...\n",
      "Processing 351979_1993.txt...\n",
      "Processing 31986_1993.txt...\n",
      "Processing 722079_1993.txt...\n",
      "Processing 49588_1993.txt...\n",
      "Processing 732712_1993.txt...\n",
      "Processing 722573_1993.txt...\n",
      "Processing 64803_1993.txt...\n",
      "Processing 817473_1993.txt...\n",
      "Processing 52485_1993.txt...\n",
      "Processing 310569_1993.txt...\n",
      "Processing 859119_1993.txt...\n",
      "Processing 78066_1993.txt...\n",
      "Processing 277509_1993.txt...\n",
      "Processing 64908_1993.txt...\n",
      "Processing 893958_1993.txt...\n",
      "Processing 868482_1993.txt...\n",
      "Processing 765813_1993.txt...\n",
      "Processing 351825_1993.txt...\n",
      "Processing 64782_1993.txt...\n",
      "Processing 75527_1993.txt...\n",
      "Processing 821480_1993.txt...\n",
      "Processing 793548_1993.txt...\n",
      "Processing 63073_1993.txt...\n",
      "Processing 36672_1993.txt...\n",
      "Processing 879209_1993.txt...\n",
      "Processing 705752_1993.txt...\n",
      "\n",
      "--- TOURNAMENT RESULTS ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>cik</th>\n",
       "      <th>year</th>\n",
       "      <th>Inc_state_Re</th>\n",
       "      <th>Inc_state_Hy</th>\n",
       "      <th>Inc_state_Lu</th>\n",
       "      <th>Inc_state_Match</th>\n",
       "      <th>Inc_year_Re</th>\n",
       "      <th>Inc_year_Hy</th>\n",
       "      <th>Inc_year_Lu</th>\n",
       "      <th>...</th>\n",
       "      <th>HQ_state_Lu</th>\n",
       "      <th>HQ_state_Match</th>\n",
       "      <th>ceo_lastname_Re</th>\n",
       "      <th>ceo_lastname_Hy</th>\n",
       "      <th>ceo_lastname_Lu</th>\n",
       "      <th>ceo_lastname_Match</th>\n",
       "      <th>ceo_fullname_backup_Re</th>\n",
       "      <th>ceo_fullname_backup_Hy</th>\n",
       "      <th>ceo_fullname_backup_Lu</th>\n",
       "      <th>ceo_fullname_backup_Match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92116_1993.txt</td>\n",
       "      <td>92116</td>\n",
       "      <td>1993</td>\n",
       "      <td>in</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "      <td>1929</td>\n",
       "      <td>1929</td>\n",
       "      <td>1929</td>\n",
       "      <td>...</td>\n",
       "      <td>California</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103730_1993.txt</td>\n",
       "      <td>103730</td>\n",
       "      <td>1993</td>\n",
       "      <td>Delaware in</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>1</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100240_1993.txt</td>\n",
       "      <td>100240</td>\n",
       "      <td>1993</td>\n",
       "      <td>Georgia in</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>...</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58696_1993.txt</td>\n",
       "      <td>58696</td>\n",
       "      <td>1993</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>...</td>\n",
       "      <td>Florida</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>46207_1993.txt</td>\n",
       "      <td>46207</td>\n",
       "      <td>1993</td>\n",
       "      <td>in</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>1981</td>\n",
       "      <td>1981</td>\n",
       "      <td>1981</td>\n",
       "      <td>...</td>\n",
       "      <td>Hawaii</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Twigg-Smith</td>\n",
       "      <td>Lewis</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Thurston Twigg-Smith</td>\n",
       "      <td>Thurston Twigg-Smith</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>793548_1993.txt</td>\n",
       "      <td>793548</td>\n",
       "      <td>1993</td>\n",
       "      <td>herein by reference</td>\n",
       "      <td>NULL</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>...</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>63073_1993.txt</td>\n",
       "      <td>63073</td>\n",
       "      <td>1993</td>\n",
       "      <td>Mass</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1926</td>\n",
       "      <td>...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>Dickson</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>36672_1993.txt</td>\n",
       "      <td>36672</td>\n",
       "      <td>1993</td>\n",
       "      <td>in</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>0</td>\n",
       "      <td>1970</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1970</td>\n",
       "      <td>...</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>879209_1993.txt</td>\n",
       "      <td>879209</td>\n",
       "      <td>1993</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>705752_1993.txt</td>\n",
       "      <td>705752</td>\n",
       "      <td>1993</td>\n",
       "      <td>August</td>\n",
       "      <td>California</td>\n",
       "      <td>California</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>1982</td>\n",
       "      <td>1982</td>\n",
       "      <td>...</td>\n",
       "      <td>NULL</td>\n",
       "      <td>1</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Ashner</td>\n",
       "      <td>0</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Michael L. Ashner</td>\n",
       "      <td>NULL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename     cik  year         Inc_state_Re Inc_state_Hy  \\\n",
       "0     92116_1993.txt   92116  1993                   in   California   \n",
       "1    103730_1993.txt  103730  1993          Delaware in     Delaware   \n",
       "2    100240_1993.txt  100240  1993           Georgia in      Georgia   \n",
       "3     58696_1993.txt   58696  1993                 NULL         NULL   \n",
       "4     46207_1993.txt   46207  1993                   in       Hawaii   \n",
       "..               ...     ...   ...                  ...          ...   \n",
       "245  793548_1993.txt  793548  1993  herein by reference         NULL   \n",
       "246   63073_1993.txt   63073  1993                 Mass         NULL   \n",
       "247   36672_1993.txt   36672  1993                   in         NULL   \n",
       "248  879209_1993.txt  879209  1993                 NULL         NULL   \n",
       "249  705752_1993.txt  705752  1993               August   California   \n",
       "\n",
       "      Inc_state_Lu Inc_state_Match Inc_year_Re Inc_year_Hy Inc_year_Lu  ...  \\\n",
       "0       California               1        1929        1929        1929  ...   \n",
       "1         Delaware               1        1962        1962        1962  ...   \n",
       "2          Georgia               1        1965        1965        1965  ...   \n",
       "3             NULL               1        NULL        NULL        NULL  ...   \n",
       "4           Hawaii               1        1981        1981        1981  ...   \n",
       "..             ...             ...         ...         ...         ...  ...   \n",
       "245     New Jersey               0        1994        NULL        NULL  ...   \n",
       "246           NULL               1        1993        NULL        1926  ...   \n",
       "247  Massachusetts               0        1970        NULL        1970  ...   \n",
       "248           NULL               1        NULL        NULL        NULL  ...   \n",
       "249     California               1        1993        1982        1982  ...   \n",
       "\n",
       "       HQ_state_Lu HQ_state_Match ceo_lastname_Re ceo_lastname_Hy  \\\n",
       "0       California              0            NULL            NULL   \n",
       "1             NULL              1            NULL            NULL   \n",
       "2          Georgia              1            NULL            NULL   \n",
       "3          Florida              1            NULL            NULL   \n",
       "4           Hawaii              1            NULL     Twigg-Smith   \n",
       "..             ...            ...             ...             ...   \n",
       "245     New Jersey              1            NULL            NULL   \n",
       "246           NULL              1         Dickson            NULL   \n",
       "247  Massachusetts              1            NULL            NULL   \n",
       "248           NULL              1            NULL            NULL   \n",
       "249           NULL              1            NULL            NULL   \n",
       "\n",
       "    ceo_lastname_Lu ceo_lastname_Match ceo_fullname_backup_Re  \\\n",
       "0              NULL                  1                   NULL   \n",
       "1              NULL               NULL                   NULL   \n",
       "2              NULL                  1                   NULL   \n",
       "3              NULL               NULL                   NULL   \n",
       "4             Lewis                  0                   NULL   \n",
       "..              ...                ...                    ...   \n",
       "245            NULL                  1                   NULL   \n",
       "246            NULL                  1                   NULL   \n",
       "247            NULL                  1                   NULL   \n",
       "248            NULL               NULL                   NULL   \n",
       "249          Ashner                  0                   NULL   \n",
       "\n",
       "    ceo_fullname_backup_Hy ceo_fullname_backup_Lu ceo_fullname_backup_Match  \n",
       "0                     NULL                   NULL                         1  \n",
       "1                     NULL                   NULL                      NULL  \n",
       "2                     NULL                   NULL                         1  \n",
       "3                     NULL                   NULL                      NULL  \n",
       "4     Thurston Twigg-Smith   Thurston Twigg-Smith                         1  \n",
       "..                     ...                    ...                       ...  \n",
       "245                   NULL                   NULL                         1  \n",
       "246                   NULL                   NULL                         1  \n",
       "247                   NULL                   NULL                         1  \n",
       "248                   NULL                   NULL                      NULL  \n",
       "249      Michael L. Ashner                   NULL                         0  \n",
       "\n",
       "[250 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10. If fine create full doc csv \n",
    "df_full = create_df_tournament(model, tokenizer, 250)\n",
    "df_full.to_csv(\"edgar_tournament_full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
