{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDGAR Ground Truth Single Feature Extraction/Validation\n",
        "\n",
        "**Goal**: Validate specific features against Ground Truth with high precision.\n",
        "\n",
        "**Methodology**:\n",
        "1. **Protocol A (Extraction)**: Use LLM to extract `Value ||| Evidence`. The evidence (quote) is required.\n",
        "2. **Protocol B (Validation)**: Compare extracted value with Ground Truth using Fuzzy Matching.\n",
        "3. **Protocol C (Judge)**: (Optional) Use LLM to judge specific ambiguous cases.\n",
        "4. **Protocol D (Discovery)**: Bootstrap Ground Truth for NEW features by extracting and manually reviewing evidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Installation & Setup\n",
        "# !pip install -q torch transformers datasets pandas tqdm accelerate bitsandbytes thefuzz python-Levenshtein\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Fuzzy Matching for Validation\n",
        "from thefuzz import fuzz\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(\"Setup Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Load Model (Qwen 2.5-7B-Instruct)\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, logging\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DTYPE = torch.bfloat16\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "print(\"Loaded:\", MODEL_NAME, \"| device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Configuration & Question Bank\n",
        "\n",
        "# TARGET FEATURE to Run (Change this to switch focus)\n",
        "TARGET_FEATURE_ID = \"original_incorporation_state\"\n",
        "GT_COLUMN_MAPPING = {\n",
        "    \"company_name\": \"company_name_truth\",\n",
        "    \"original_incorporation_state\": \"original_inc_state_truth\",\n",
        "    \"original_incorporation_year\": \"original_inc_year_truth\",\n",
        "    \"employee_count\": \"employee_count_truth\",\n",
        "}\n",
        "\n",
        "QUESTION_BANK = {\n",
        "    \"section_1\": [\n",
        "        {\n",
        "            \"id\": \"company_name\",\n",
        "            \"prompt\": (\n",
        "                \"What is the exact legal name of the registrant? \"\n",
        "                \"1. Look for the very first sentence of the 'Business' section or the cover page intro \"\n",
        "                \"(e.g., 'Apple Inc. (the Registrant)...'). \"\n",
        "                \"2. Do NOT use 'Doing Business As' (DBA) names or brand names. \"\n",
        "                \"3. Do NOT include the stock ticker symbol. \"\n",
        "                \"4. Include legal suffixes like 'Inc.', 'Corp.', 'Ltd.' if present. \"\n",
        "                \"Answer with ONLY the legal name string.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"headquarters_city\",\n",
        "            \"prompt\": (\n",
        "                \"In which city are the registrant's *principal executive offices* physically located? \"\n",
        "                \"1. Look for the address under 'Executive Offices' or 'Address of Principal Executive Offices'. \"\n",
        "                \"2. CRITICAL WARNING: Do NOT return the city of the 'Registered Agent' or 'State of Incorporation' \"\n",
        "                \"(e.g., ignore 'Wilmington' or 'Dover' unless the CEO actually works there). \"\n",
        "                \"3. Ignore P.O. Boxes. \"\n",
        "                \"Answer with ONLY the city name.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"original_incorporation_state\",\n",
        "            \"prompt\": (\n",
        "                \"In which U.S. state was the registrant *originally* incorporated or organized? \"\n",
        "                \"Follow this strict hierarchy: \"\n",
        "                \"1. PRIORITIZE HISTORY: Look for phrases like 'originally incorporated in', 'formerly organized in', \"\n",
        "                \"or 'predecessor company incorporated in'. \"\n",
        "                \"2. REINCORPORATION RULE: If the company reincorporated (e.g., moved from California to Delaware), \"\n",
        "                \"you MUST return the OLD state (California), not the current one. \"\n",
        "                \"3. MERGER EXCEPTION: Only if the registrant is a *new* successor entity formed by a merger, \"\n",
        "                \"return the state of that successor. \"\n",
        "                \"4. If no history is mentioned, return the current state. \"\n",
        "                \"Answer with ONLY the state name.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"original_incorporation_year\",\n",
        "            \"prompt\": (\n",
        "                \"In which year was the registrant *originally* incorporated or organized? \"\n",
        "                \"1. IGNORE 'FOUNDED' dates. Only look for 'incorporated', 'organized', or 'formed'. \"\n",
        "                \"2. REINCORPORATION RULE: If the text says 'originally incorporated in 1980' and 'reincorporated in 1995', \"\n",
        "                \"return the EARLIEST year (1980). \"\n",
        "                \"3. MERGER EXCEPTION: If the current entity was formed by a merger of equals, use the year of that merger. \"\n",
        "                \"Answer with ONLY the year (YYYY).\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"employee_count\",\n",
        "            \"prompt\": (\n",
        "                \"What is the total number of employees the registrant has? \"\n",
        "                \"1. PREFER FULL-TIME: If the text distinguishes between full-time and part-time, return the full-time count. \"\n",
        "                \"2. If only 'total' is given, use that. \"\n",
        "                \"3. EXCLUDE: Do not count independent contractors, agents, or temporary staff unless they are the only number given. \"\n",
        "                \"4. FORMAT: Remove commas and return ONLY the integer (e.g., return 14500, not 14,500). \"\n",
        "                \"If the number is 'approximately 5,000', return 5000.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"headquarters_state\",\n",
        "            \"prompt\": (\n",
        "                \"In which U.S. state are the registrant's *principal executive offices* physically located? \"\n",
        "                \"1. This is the state where the HQ building is, NOT necessarily the state of incorporation. \"\n",
        "                \"2. CRITICAL: If the text says 'Incorporated in Delaware' but 'Executive offices in California', \"\n",
        "                \"return CALIFORNIA. \"\n",
        "                \"Answer with ONLY the state name.\"\n",
        "            ),\n",
        "        },\n",
        "    ],\n",
        "    \"section_10\": [\n",
        "        {\n",
        "            \"id\": \"ceo_lastname\",\n",
        "            \"prompt\": (\n",
        "                \"What is the LAST NAME of the registrant's current Chief Executive Officer (CEO)? \"\n",
        "                \"1. Look for 'Chief Executive Officer', 'CEO', or 'Principal Executive Officer'. \"\n",
        "                \"2. If 'Co-CEOs' are listed, pick the first one mentioned. \"\n",
        "                \"3. EXCLUDE titles (Mr., Dr.) and first/middle names. \"\n",
        "                \"4. If the CEO has a compound last name (e.g., 'Von Trap'), include the full last name. \"\n",
        "                \"Answer with ONLY the last name string.\"\n",
        "            ),\n",
        "        }\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Protocol A\n",
        "def ask_llm(context, prompt, model, tokenizer, max_new_tokens=50):\n",
        "    \"\"\"Sends a prompt to the LLM with the given context.\"\"\"\n",
        "    if not context or not context.strip():\n",
        "        return None\n",
        "\n",
        "    model_limit = getattr(tokenizer, \"model_max_length\", 32768) \n",
        "    available_for_context = model_limit - 650\n",
        "    char_limit = available_for_context * 4\n",
        "\n",
        "    if len(context) > char_limit:\n",
        "        context = context[:char_limit]\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a precise data extraction assistant.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Read this SEC 10-K filing excerpt and answer the question. \\nQuestion: {prompt} \\nIf the information is not present in the context, reply with 'NULL'.\\nContext: \\\"{context}\\\"\",\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs, max_new_tokens=max_new_tokens, do_sample=False, temperature=0.0\n",
        "        )\n",
        "\n",
        "    # Clean response\n",
        "    response = tokenizer.decode(\n",
        "        outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True\n",
        "    )\n",
        "    answer = response.strip().split(\"\\n\")[0]\n",
        "    \n",
        "    # Simple clean to remove periods like \"Delaware.\"\n",
        "    answer = answer.rstrip(\".\")\n",
        "\n",
        "    if \"null\" in answer.lower() or \"not found\" in answer.lower():\n",
        "        return None\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. POST-HOC EVIDENCE FINDER (No Keywords, Just Search)\n",
        "# 2. NEW: THE LLM EVIDENCE RETRIEVER (The \"Verifier\")\n",
        "def get_evidence_with_llm(context, question ,answer_val, model, tokenizer):\n",
        "    \"\"\"\n",
        "    Asks the LLM to locate the exact sentence supporting the specific answer 'answer_val'.\n",
        "    \"\"\" \n",
        "    if not context or not context.strip():\n",
        "        return \"NULL\" \n",
        "\n",
        "    if not answer_val or answer_val.lower() == \"null\":\n",
        "        return \"NULL\"\n",
        "\n",
        "    model_limit = getattr(tokenizer, \"model_max_length\", 32768) \n",
        "    available_for_context = model_limit - 650\n",
        "    char_limit = available_for_context * 4\n",
        "\n",
        "    if len(context) > char_limit:\n",
        "        context = context[:char_limit] \n",
        "    \n",
        "    head, sep, tail = question.partition(\"?\")\n",
        "    question_only = head + sep\n",
        "    # Specific Prompt to force \"Quote Finding\"\n",
        "    prompt = (\n",
        "        f\"You previously determined for this question: {question_only} \\n that the answer is: '{answer_val}'.\\n\"\n",
        "        \"Find the EXACT sentence in the text that supports this answer.\\n\"\n",
        "        \"1. Copy the sentence word-for-word from the text.\\n\"\n",
        "        \"2. Do not rewrite or summarize it.\\n\"\n",
        "        f\"3. If {answer_val} is NULL, return NULL.\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a forensic text analyst. You only output exact quotes.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Task: {prompt}\\n\\n Text: \\\"{context}\\\"\"}\n",
        "    ] \n",
        "\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Allow more tokens (200) because sentences can be long\n",
        "        outputs = model.generate(**inputs, max_new_tokens=200, do_sample=False, temperature=0.0)\n",
        "\n",
        "    evidence_candidate = tokenizer.decode(outputs[0][inputs.input_ids.shape[1] :], skip_special_tokens=True).strip()\n",
        "    \n",
        "    return evidence_candidate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. NEW: THE VALIDATOR (The \"Guardrail\")\n",
        "def get_fingerprint(text):\n",
        "    return re.sub(r'[\\W_]+', '', text).lower() \n",
        "\n",
        "def validate_evidence(full_text, evidence_candidate):\n",
        "    \"\"\"\n",
        "    Checks if the LLM's 'exact quote' actually exists in the text.\n",
        "    Returns: (final_evidence_string)\n",
        "    \"\"\"\n",
        "    if not evidence_candidate or \"null\" in evidence_candidate.lower():\n",
        "        return \"NULL\"\n",
        "\n",
        "    # 1. Exact Match Check\n",
        "    if evidence_candidate in full_text:\n",
        "        return evidence_candidate\n",
        "    \n",
        "    evidence_candidate = evidence_candidate.strip('\"\\'')\n",
        "    # 2. Robust Match (Ignore case and extra whitespace)\n",
        "    # LLMs often strip extra spaces or change capitalization slightly.\n",
        "    clean_text = \" \".join(full_text.split()).lower()\n",
        "    clean_evd = \" \".join(evidence_candidate.split()).lower() \n",
        "\n",
        "    fp_text = get_fingerprint(clean_text)\n",
        "    fp_evd = get_fingerprint(clean_evd)\n",
        "\n",
        "    if fp_evd in fp_text:\n",
        "        return \"Verified (Approx - Punctuation Ignored): \" + evidence_candidate\n",
        "\n",
        "    return f\"HALLUCINATION_FLAG: {evidence_candidate}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_feature(doc, feature_id, model, tokenizer):\n",
        "    \"\"\"\n",
        "    1. Ask LLM for Answer.\n",
        "    2. Ask LLM for Evidence of that Answer.\n",
        "    3. Python Validates the Evidence exists.\n",
        "    \"\"\"\n",
        "    # Setup\n",
        "    config = None\n",
        "    target_section = \"section_1\"\n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        for q in questions:\n",
        "            if q[\"id\"] == feature_id:\n",
        "                config = q\n",
        "                target_section = section\n",
        "                break\n",
        "    if not config:\n",
        "        return None, None\n",
        "    text = doc.get(target_section, \"\")\n",
        "\n",
        "    # Step 1: Extract Value\n",
        "    val_ext = ask_llm(text, config[\"prompt\"], model, tokenizer)\n",
        "\n",
        "    # Step 2: Get Evidence via LLM\n",
        "    evd_candidate = get_evidence_with_llm(text, config[\"prompt\"], val_ext, model, tokenizer)\n",
        "\n",
        "    # Step 3: Validate Evidence (Python Check)\n",
        "    final_evd = validate_evidence(text, evd_candidate) \n",
        "    \n",
        "    return val_ext, final_evd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Helper Functions (Your provided normalizers) ---\n",
        "def normalize_text(val):\n",
        "    if pd.isna(val) or str(val).lower() in [\"null\", \"nan\", \"none\", \"\"]:\n",
        "        return \"NULL\"\n",
        "    return str(val).strip().lower().replace(\".\", \"\").replace(\",\", \"\")\n",
        "\n",
        "def normalize_year(val):\n",
        "    try:\n",
        "        if pd.isna(val) or str(val).lower() in ['null', 'nan', '']:\n",
        "            return \"NULL\"\n",
        "        # Handles 1996.0 -> 1996\n",
        "        return str(int(float(val)))\n",
        "    except:\n",
        "        return \"NULL\"\n",
        "\n",
        "\n",
        "def normalize_count(val):\n",
        "    try:\n",
        "        if pd.isna(val) or str(val).lower() in [\"null\", \"nan\", \"none\", \"\"]:\n",
        "            return \"NULL\"\n",
        "        clean_string = str(val).replace(\",\", \"\").strip()\n",
        "        return str(int(float(clean_string)))\n",
        "    except:\n",
        "        return \"NULL\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Protocol B: Validation Logic\n",
        "\n",
        "# --- MAPPING FEATURES TO NORMALIZERS ---\n",
        "NORMALIZER_MAP = {\n",
        "    \"company_name\": normalize_text,\n",
        "    \"headquarters_city\": normalize_text,\n",
        "    \"original_incorporation_state\": normalize_text,\n",
        "    \"headquarters_state\": normalize_text,\n",
        "    \"ceo_lastname\": normalize_text,\n",
        "    \"original_incorporation_year\": normalize_year,\n",
        "    \"employee_count\": normalize_count,\n",
        "}\n",
        "\n",
        "\n",
        "def validate_row(val_ext, val_gt, feature_id):\n",
        "    \"\"\"\n",
        "    Compares Extracted Value vs Ground Truth with Normalization.\n",
        "    \"\"\"\n",
        "    # 1. Get the correct normalizer for this feature\n",
        "    cleaner = NORMALIZER_MAP.get(feature_id, normalize_text)  # Default to text\n",
        "\n",
        "    # 2. Normalize both inputs\n",
        "    s1 = cleaner(val_ext)\n",
        "    s2 = cleaner(val_gt)\n",
        "\n",
        "    # 3. Check for NULLs\n",
        "    if s1 == \"NULL\" and s2 == \"NULL\":\n",
        "        return 100, \"1\"\n",
        "    if s1 == \"NULL\" or s2 == \"NULL\":\n",
        "        return 0, \"0\"\n",
        "\n",
        "    # 4. Exact Match (Post-Normalization)\n",
        "    if s1 == s2:\n",
        "        return 100, \"1\"\n",
        "\n",
        "    # 5. Fuzzy Match (Fallback for typos in text)\n",
        "    # We only fuzzy match if it's NOT a year or number (those should be exact)\n",
        "    if feature_id not in [\"original_incorporation_year\", \"employee_count\"]:\n",
        "        score = fuzz.ratio(s1, s2)\n",
        "        if score > 90:\n",
        "            return score, \"1\"\n",
        "        if score < 50:\n",
        "            return score, \"0\"\n",
        "        return score, \"?\"  # Ambiguous\n",
        "\n",
        "    return 0, \"0\"  # Fail if numbers/years don't match exactly\n",
        "\n",
        "\n",
        "def run_validation_loop(target_feature, gt_csv_path, limit=10):\n",
        "    print(f\"--- STARTING VALIDATION FOR: {target_feature} ---\")\n",
        "\n",
        "    # 1. Load Ground Truth\n",
        "    df_gt = pd.read_csv(gt_csv_path)\n",
        "    gt_col = GT_COLUMN_MAPPING.get(target_feature)\n",
        "\n",
        "    if not gt_col or gt_col not in df_gt.columns:\n",
        "        print(f\"Error: Ground Truth column '{gt_col}' not found in CSV.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # 2. Load Documents (Streaming with Fix for Parquet Revision)\n",
        "    dataset = load_dataset(\n",
        "        \"c3po-ai/edgar-corpus\",\n",
        "        \"default\",\n",
        "        split=\"train\",\n",
        "        streaming=True,\n",
        "        revision=\"refs/convert/parquet\",\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "\n",
        "    count = 0\n",
        "    for i, doc in enumerate(dataset):\n",
        "        fname = doc.get(\"filename\")\n",
        "        # Ineffecient for streaming but necessary if GT is sparse\n",
        "        if fname not in df_gt[\"filename\"].values:\n",
        "            continue\n",
        "\n",
        "        if count >= limit:\n",
        "            break\n",
        "        count += 1\n",
        "\n",
        "        print(f\"Processing {fname}...\")\n",
        "\n",
        "        # Extract\n",
        "        val_ext, evd_ext = extract_feature(doc, target_feature, model, tokenizer)\n",
        "\n",
        "        # Get Truth\n",
        "        val_gt = df_gt.loc[df_gt[\"filename\"] == fname, gt_col].values[0]\n",
        "\n",
        "        # Validate\n",
        "        score, status = validate_row(val_ext, val_gt, target_feature)\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"Filename\": fname,\n",
        "                \"GT_Value\": val_gt,\n",
        "                \"Ext_Value\": val_ext,\n",
        "                \"Protocol_B_Score\": score,\n",
        "                \"Protocol_B_Status\": status,\n",
        "                \"Evidence\": evd_ext,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Run Validation\n",
        "\n",
        "# Update path to your actual GT CSV location\n",
        "GT_PATH = \"../csvs/ground_truth/Ground Truth Data - edgar_gt_verified_slim.csv\"\n",
        "\n",
        "df_val = run_validation_loop(\n",
        "    target_feature=TARGET_FEATURE_ID, # Defined in Config cell\n",
        "    gt_csv_path=GT_PATH,\n",
        "    limit=5\n",
        ")\n",
        "df_val.fillna(\"NULL\", inplace=True) \n",
        "\n",
        "# Display Results\n",
        "print(\"\\n--- VALIDATION RESULTS ---\")\n",
        "display(df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_judge(gt_value, ext_value, evidence):\n",
        "    \"\"\"\n",
        "    Sends the specific Prompt to model and enforces a 1/0 response.\n",
        "    \"\"\"\n",
        "    # 1. Construct the User Prompt\n",
        "    user_prompt = (\n",
        "        f\"Ground Truth says: '{gt_value}'. \"\n",
        "        f\"Extraction says: '{ext_value}' based on evidence '{evidence}'. \"\n",
        "        \"Is the extraction factually correct despite the text mismatch? \"\n",
        "        \"Answer 1 for Yes, 0 for No.\"\n",
        "    )\n",
        "\n",
        "    # 2. System Prompt to force binary output\n",
        "    system_prompt = (\n",
        "        \"You are a strict Judge. Your job is to resolve ambiguity in data extraction. \"\n",
        "        \"Compare the Extraction against the Ground Truth. \"\n",
        "        \"If they represent the same underlying fact (even if phrased differently), answer 1. \"\n",
        "        \"If they are different or the evidence is wrong, answer 0. \"\n",
        "        \"Output ONLY the number '1' or '0'.\"\n",
        "    )\n",
        "\n",
        "    # 3. Format for Qwen (Chat Template)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "    \n",
        "    # 4. Tokenize & Generate\n",
        "    text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text_input], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # strict limit: we only need 1 token (the number), but give 5 buffer\n",
        "        outputs = model.generate(\n",
        "            **inputs, \n",
        "            max_new_tokens=5, \n",
        "            do_sample=False, \n",
        "            temperature=0.0\n",
        "        )\n",
        "    \n",
        "    # 5. Decode & Clean\n",
        "    # Slice off the input prompt to get just the response\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    clean_resp = response.strip()\n",
        "    \n",
        "    # 6. Parse strictly\n",
        "    if \"1\" in clean_resp:\n",
        "        return \"1\"\n",
        "    elif \"0\" in clean_resp:\n",
        "        return \"0\"\n",
        "    else:\n",
        "        # Fallback: if Qwen chats (e.g., \"The answer is 0\"), catch the digit\n",
        "        if \"1\" in clean_resp and \"0\" not in clean_resp: return \"1\"\n",
        "        if \"0\" in clean_resp and \"1\" not in clean_resp: return \"0\"\n",
        "        return \"0\" # Default to Fail if unclear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Protocol C: The Judge (Binary 1/0)\n",
        "# Run this cell to resolve ambiguous cases ('?') from Protocol B\n",
        "\n",
        "def judge_row(row):\n",
        "    # If it was already a clear pass (1) or fail (0), keep it.\n",
        "    if row['Protocol_B_Status'] in ['1', '0']:\n",
        "        return row['Protocol_B_Status']\n",
        "        \n",
        "    # 2. Handle missing data gracefully\n",
        "    if pd.isna(row['Ext_Value']) or pd.isna(row['Evidence']):\n",
        "        return \"0\"  \n",
        "    \n",
        "    print(f\"Judging Row {row.name}...\") \n",
        "    return ask_judge(row['GT_Value'], row['Ext_Value'], row['Evidence'])\n",
        "\n",
        "if not df_val.empty:\n",
        "    df_val['Judge_Valid'] = df_val.apply(judge_row, axis=1)\n",
        "    display(df_val[[ 'GT_Value', 'Ext_Value', 'Protocol_B_Status', 'Judge_Valid', 'Evidence']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(df_val['Judge_Valid'] == '1').sum() / 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(df_val['Judge_Valid'] == '1').sum() / 250"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Protocol D: Discovery Mode (New Feature Extraction)\n",
        "\n",
        "**Goal**: Extract a NEW feature that has no Ground Truth yet.\n",
        "\n",
        "**Output**: Creates or Updates `discovered_GT.csv` so you can manually review the evidence and mark it as truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discover_new_feature(target_feature, output_file=\"discovered_GT.csv\", limit=50):\n",
        "    print(f\"--- STARTING DISCOVERY FOR: {target_feature} ---\")\n",
        "\n",
        "    update = False\n",
        "    # 1. Load Existing CSV if available (Smart Append)\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"Loading existing file: {output_file}\")\n",
        "        df_master = pd.read_csv(output_file)\n",
        "        update = True\n",
        "    else:\n",
        "        print(\"Creating NEW discovery file.\")\n",
        "        df_master = pd.DataFrame(columns=[\"filename\", \"cik\", \"year\"])\n",
        "\n",
        "    # 2. Stream Data\n",
        "    dataset = load_dataset(\n",
        "        \"c3po-ai/edgar-corpus\",\n",
        "        \"default\",\n",
        "        split=\"train\",\n",
        "        streaming=True,\n",
        "        revision=\"refs/convert/parquet\",\n",
        "    )\n",
        "\n",
        "    new_rows = []\n",
        "    updates = 0\n",
        "\n",
        "    count = 0\n",
        "    for doc in tqdm(dataset, total=limit):\n",
        "        if count >= limit:\n",
        "            break\n",
        "        count += 1\n",
        "\n",
        "        fname = doc.get(\"filename\")\n",
        "\n",
        "        # Check if we already have this file in our Master CSV\n",
        "        existing_idx = df_master.index[df_master[\"filename\"] == fname].tolist()\n",
        "\n",
        "        # Extract Data\n",
        "        val, evd = extract_feature(doc, target_feature, model, tokenizer)\n",
        "\n",
        "        # Prepare data dict\n",
        "        data = {f\"{target_feature}_value\": val, f\"{target_feature}_evidence\": evd}\n",
        "\n",
        "        if existing_idx:\n",
        "            # UPDATE existing row\n",
        "            idx = existing_idx[0]\n",
        "            for k, v in data.items():\n",
        "                df_master.at[idx, k] = v\n",
        "            updates += 1\n",
        "        else:\n",
        "            # CREATE new row\n",
        "            # Ensure we capture identity columns\n",
        "            row = {\n",
        "                \"filename\": fname,\n",
        "                \"cik\": doc.get(\"cik\"),\n",
        "                \"year\": doc.get(\"year\"),\n",
        "                **data,\n",
        "            }\n",
        "            new_rows.append(row)\n",
        "\n",
        "    # 3. Save Results\n",
        "    if new_rows:\n",
        "        df_new = pd.DataFrame(new_rows)\n",
        "        # Merge\n",
        "        df_master = pd.concat([df_master, df_new], ignore_index=True)\n",
        "\n",
        "    print(\n",
        "        f\"\\nDiscovery Complete. Updated {updates} rows. Added {len(new_rows)} new rows.\"\n",
        "    )\n",
        "\n",
        "    filename = output_file\n",
        "    if update:\n",
        "        filename = output_file.replace(\".csv\", \"_updated.csv\") \n",
        "        \n",
        "    df_master.fillna(\"NULL\", inplace=True)\n",
        "    df_master.to_csv(filename, index=False)\n",
        "    return df_master.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Discover 'fiscal_year_end'\n",
        "# Ensure you add 'fiscal_year_end' to QUESTION_BANK above first!\n",
        "\n",
        "# UNCOMMENT TO RUN:\n",
        "# df_disc = discover_new_feature(\"fiscal_year_end\", limit=5)\n",
        "# display(df_disc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
