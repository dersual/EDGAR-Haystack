{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDGAR Ground Truth Single Feature Extraction/Validation\n",
        "\n",
        "**Goal**: Validate specific features against Ground Truth with high precision.\n",
        "\n",
        "**Methodology**:\n",
        "1. **Protocol A (Extraction)**: Use LLM to extract `Value ||| Evidence`. The evidence (quote) is required.\n",
        "2. **Protocol B (Validation)**: Compare extracted value with Ground Truth using Fuzzy Matching.\n",
        "3. **Protocol C (Judge)**: (Optional) Use LLM to judge specific ambiguous cases.\n",
        "4. **Protocol D (Discovery)**: Bootstrap Ground Truth for NEW features by extracting and manually reviewing evidence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Installation & Setup\n",
        "# !pip install -q torch transformers datasets pandas tqdm accelerate bitsandbytes thefuzz python-Levenshtein\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Fuzzy Matching for Validation\n",
        "from thefuzz import fuzz\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(\"Setup Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Load Model (Qwen 2.5-7B-Instruct)\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, logging\n",
        "\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DTYPE = torch.bfloat16\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=DTYPE,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "print(\"Loaded:\", MODEL_NAME, \"| device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Configuration & Question Bank\n",
        "\n",
        "# TARGET FEATURE to Run (Change this to switch focus)\n",
        "TARGET_FEATURE_ID = \"company_name\"\n",
        "GT_COLUMN_MAPPING = {\n",
        "    \"company_name\": \"company_name_truth\",\n",
        "    \"original_incorporation_state\": \"original_inc_state_truth\",\n",
        "    \"original_incorporation_year\": \"original_inc_year_truth\",\n",
        "    \"employee_count\": \"employee_count_truth\",\n",
        "}\n",
        "\n",
        "QUESTION_BANK = {\n",
        "    \"section_1\": [\n",
        "        {\n",
        "            \"id\": \"company_name\",\n",
        "            \"prompt\": (\n",
        "                \"What is the full legal name of the company? \"\n",
        "                \"Look for the entity being introduced (e.g., 'X Corp (the Company)'). \"\n",
        "                \"Exclude headers like 'Item 1' or 'General'. \"\n",
        "                \"Answer with ONLY the legal name.\"\n",
        "            ),\n",
        "        },\n",
        "        # --- MENTOR'S REQUEST: HQ CITY ---\n",
        "        {\n",
        "            \"id\": \"headquarters_city\",\n",
        "            \"prompt\": (\n",
        "                \"What **city** are the company's executive offices located in? \"\n",
        "                \"Look for the phrase 'executive offices are located at'. \"\n",
        "                \"Answer with ONLY the city name (one word if possible).\"\n",
        "            ),\n",
        "        },\n",
        "        # --- STATE: ORIGINAL (Not Current) ---\n",
        "        {\n",
        "            \"id\": \"original_incorporation_state\",\n",
        "            \"prompt\": (\n",
        "                \"In which U.S. state was the company originally incorporated or organized? \"\n",
        "                \"1. Look for 'originally incorporated in [State]' or 'organized as a [State] corporation'. \"\n",
        "                \"2. If it mentions 'reincorporated in Delaware', IGNORE Delaware and find the PREVIOUS state. \"\n",
        "                \"3. If no history is mentioned, return the current state. \"\n",
        "                \"Answer with ONLY the state name.\"\n",
        "            ),\n",
        "        },\n",
        "        # --- YEAR: ORIGINAL (Not Re-inc) ---\n",
        "        {\n",
        "            \"id\": \"original_incorporation_year\",\n",
        "            \"prompt\": (\n",
        "                \"In what year was the company originally incorporated or organized? \"\n",
        "                \"Ignore reincorporation dates. Answer with the EARLIEST year found (YYYY). \"\n",
        "                \"If no other history is mentioned, return the year mentioned.\"\n",
        "            ),\n",
        "        },\n",
        "        # --- EMPLOYEES: FULL-TIME (Exclude Enrollment) OR TOTAL ---\n",
        "        {\n",
        "            \"id\": \"employee_count\",\n",
        "            \"prompt\": (\n",
        "                \"How many full-time employees does the company have? \"\n",
        "                \"1. Exclude 'enrollment', 'members', or 'agents'. \"\n",
        "                \"2. If full-time is not specified, return Total. \"\n",
        "                \"Answer with ONLY the integer.\"\n",
        "            ),\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"headquarters_state\",\n",
        "            \"prompt\": \"In which U.S. state are the company's principal executive offices located? Answer with ONLY the state name. If not found, answer NULL.\",\n",
        "        },\n",
        "    ],\n",
        "    \"section_10\": [\n",
        "        {\n",
        "            \"id\": \"ceo_lastname\",\n",
        "            \"prompt\": \"What is the Last Name of the current CEO? Answer with ONLY the last name. If not found, answer NULL.\",\n",
        "        }\n",
        "    ],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Protocol A: Evidence-First Extraction\n",
        "\n",
        "def ask_llm_with_evidence(context, prompt, model, tokenizer, max_new_tokens=100):\n",
        "    \"\"\"\n",
        "    Asks LLM to output: VALUE ||| EVIDENCE\n",
        "    Returns: (Value, Evidence)\n",
        "    \"\"\"\n",
        "    if not context or not context.strip():\n",
        "        return None, None\n",
        "\n",
        "    # Standard Limit check (same as tournament code)\n",
        "    model_limit = getattr(tokenizer, \"model_max_length\", 32768) \n",
        "    available_for_context = model_limit - 1000 # Reserve more for evidence output\n",
        "    char_limit = available_for_context * 4 \n",
        "    if len(context) > char_limit:\n",
        "        context = context[:char_limit]\n",
        "\n",
        "    system_prompt = (\n",
        "        \"Do not answer unless you can quote the exact sentence from the text. \"\n",
        "        \"Output Format: VALUE ||| EVIDENCE\"\n",
        "    )\n",
        "    \n",
        "    user_prompt = (\n",
        "        f\"Read this text: \\\"{context}\\\"\\n\\n\"\n",
        "        f\"Question: {prompt}\\n\\n\"\n",
        "        \"Rules:\\n\"\n",
        "        \"1. Find the exact sentence supporting the answer.\\n\"\n",
        "        \"2. Extract the short value (e.g., 'Delaware').\\n\"\n",
        "        \"3. Output ONLY: Value ||| Evidence\\n\"\n",
        "        \"4. If not found, output: NULL ||| NULL\"\n",
        "    )\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False, temperature=0.0)\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    \n",
        "    # Parse Output\n",
        "    clean_resp = response.strip().split('\\n')[0]\n",
        "    parts = clean_resp.split(\"|||\")\n",
        "    \n",
        "    if len(parts) >= 2:\n",
        "        val = parts[0].strip()\n",
        "        evd = parts[1].strip()\n",
        "        if val.lower() == \"null\": return None, None\n",
        "        return val, evd\n",
        "        \n",
        "    # Fallback Parsing\n",
        "    if \"null\" in clean_resp.lower(): return None, None\n",
        "    return clean_resp, \"FORMAT_ERROR\"\n",
        "\n",
        "\n",
        "def extract_feature(doc, feature_id, model, tokenizer):\n",
        "    \"\"\"Extracts a specific feature using the Evidence-First protocol.\"\"\"\n",
        "    # Find the prompt in QUESTION_BANK\n",
        "    prompt = None\n",
        "    target_section = \"section_1\" # Default\n",
        "    \n",
        "    for section, questions in QUESTION_BANK.items():\n",
        "        for q in questions:\n",
        "            if q[\"id\"] == feature_id:\n",
        "                prompt = q[\"prompt\"]\n",
        "                target_section = section\n",
        "                break\n",
        "    \n",
        "    if not prompt:\n",
        "        print(f\"Warning: Feature ID '{feature_id}' not found in Question Bank.\")\n",
        "        return None, None\n",
        "        \n",
        "    text = doc.get(target_section, \"\")\n",
        "    return ask_llm_with_evidence(text, prompt, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Protocol B: Validation Logic\n",
        "\n",
        "def validate_row(val_ext, val_gt):\n",
        "    \"\"\"\n",
        "    Compares Extracted Value vs Ground Truth.\n",
        "    Returns: Score (0-100), Status (1=Pass, 0=Fail, ?=Ambiguous)\n",
        "    \"\"\"\n",
        "    if pd.isna(val_ext) and pd.isna(val_gt): return 100, \"1\" # Both Null\n",
        "    if pd.isna(val_ext) or pd.isna(val_gt): return 0, \"0\"   # One Null\n",
        "    \n",
        "    # Normalize\n",
        "    s1 = str(val_ext).lower().strip().strip(\".\")\n",
        "    s2 = str(val_gt).lower().strip().strip(\".\")\n",
        "    \n",
        "    # Exact Match\n",
        "    if s1 == s2: return 100, \"1\"\n",
        "    \n",
        "    # Fuzzy Match\n",
        "    score = fuzz.ratio(s1, s2)\n",
        "    if score > 90: return score, \"1\"\n",
        "    if score < 50: return score, \"0\"\n",
        "    return score, \"?\" # Ambiguous\n",
        "\n",
        "def run_validation_loop(target_feature, gt_csv_path, limit=10):\n",
        "    print(f\"--- STARTING VALIDATION FOR: {target_feature} ---\")\n",
        "    \n",
        "    # 1. Load Ground Truth\n",
        "    df_gt = pd.read_csv(gt_csv_path)\n",
        "    gt_col = GT_COLUMN_MAPPING.get(target_feature)\n",
        "    \n",
        "    if not gt_col or gt_col not in df_gt.columns:\n",
        "        print(f\"Error: Ground Truth column '{gt_col}' not found in CSV.\")\n",
        "        return pd.DataFrame()\n",
        "        \n",
        "    # 2. Load Documents (Streaming with Fix for Parquet Revision)\n",
        "    dataset = load_dataset(\"c3po-ai/edgar-corpus\", \"default\", split=\"train\", streaming=True, revision=\"refs/convert/parquet\")\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    count = 0\n",
        "    for i, doc in enumerate(dataset):\n",
        "        fname = doc.get('filename')\n",
        "        # Ineffecient for streaming but necessary if GT is sparse\n",
        "        if fname not in df_gt['filename'].values:\n",
        "            continue \n",
        "            \n",
        "        if count >= limit: break\n",
        "        count += 1\n",
        "        \n",
        "        print(f\"Processing {fname}...\")\n",
        "        \n",
        "        # Extract\n",
        "        val_ext, evd_ext = extract_feature(doc, target_feature, model, tokenizer)\n",
        "        \n",
        "        # Get Truth\n",
        "        val_gt = df_gt.loc[df_gt['filename'] == fname, gt_col].values[0]\n",
        "        \n",
        "        # Validate\n",
        "        score, status = validate_row(val_ext, val_gt)\n",
        "        \n",
        "        results.append({\n",
        "            \"Filename\": fname,\n",
        "            \"GT_Value\": val_gt,\n",
        "            \"Ext_Value\": val_ext,\n",
        "            \"Protocol_B_Score\": score,\n",
        "            \"Protocol_B_Status\": status,\n",
        "            \"Evidence\": evd_ext\n",
        "        })\n",
        "        \n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Run Validation\n",
        "\n",
        "# Update path to your actual GT CSV location\n",
        "GT_PATH = \"../csvs/ground_truth/Ground Truth Data - edgar_gt_verified_slim.csv\"\n",
        "\n",
        "df_val = run_validation_loop(\n",
        "    target_feature=TARGET_FEATURE_ID, # Defined in Config cell\n",
        "    gt_csv_path=GT_PATH,\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "# Display Results\n",
        "print(\"\\n--- VALIDATION RESULTS ---\")\n",
        "display(df_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Protocol C: The Judge (Binary 1/0)\n",
        "# Run this cell to resolve ambiguous cases ('?') from Protocol B\n",
        "\n",
        "def judge_row(row):\n",
        "    # If it was already a clear pass (1) or fail (0), keep it.\n",
        "    if row['Protocol_B_Status'] in ['1', '0']:\n",
        "        return row['Protocol_B_Status']\n",
        "        \n",
        "    # Construct Judge Prompt for '?' Ambiguous cases\n",
        "    prompt = (\n",
        "        f\"Ground Truth says: '{row['GT_Value']}'. \"\n",
        "        f\"Extraction says: '{row['Ext_Value']}' based on evidence '{row['Evidence']}'. \"\n",
        "        \"Is the extraction factually correct despite the text mismatch? \"\n",
        "        \"Answer 1 for Yes, 0 for No.\"\n",
        "    )\n",
        "    \n",
        "    # Placeholder: In a real run, send this to LLM.\n",
        "    # For this demo, we can just return '0' or assume fail.\n",
        "    return \"0\" \n",
        "\n",
        "if not df_val.empty:\n",
        "    df_val['Judge_Valid'] = df_val.apply(judge_row, axis=1)\n",
        "    display(df_val[[ 'GT_Value', 'Ext_Value', 'Protocol_B_Status', 'Judge_Valid', 'Evidence']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Protocol D: Discovery Mode (New Feature Extraction)\n",
        "\n",
        "**Goal**: Extract a NEW feature that has no Ground Truth yet.\n",
        "\n",
        "**Output**: Creates or Updates `discovered_GT.csv` so you can manually review the evidence and mark it as truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def discover_new_feature(target_feature, output_file=\"discovered_GT.csv\", limit=50):\n",
        "    print(f\"--- STARTING DISCOVERY FOR: {target_feature} ---\")\n",
        "\n",
        "    update = False\n",
        "    # 1. Load Existing CSV if available (Smart Append)\n",
        "    if os.path.exists(output_file):\n",
        "        print(f\"Loading existing file: {output_file}\")\n",
        "        df_master = pd.read_csv(output_file)\n",
        "        update = True\n",
        "    else:\n",
        "        print(\"Creating NEW discovery file.\")\n",
        "        df_master = pd.DataFrame(columns=[\"filename\", \"cik\", \"year\"])\n",
        "\n",
        "    # 2. Stream Data\n",
        "    dataset = load_dataset(\n",
        "        \"c3po-ai/edgar-corpus\",\n",
        "        \"default\",\n",
        "        split=\"train\",\n",
        "        streaming=True,\n",
        "        revision=\"refs/convert/parquet\",\n",
        "    )\n",
        "\n",
        "    new_rows = []\n",
        "    updates = 0\n",
        "\n",
        "    count = 0\n",
        "    for doc in tqdm(dataset, total=limit):\n",
        "        if count >= limit:\n",
        "            break\n",
        "        count += 1\n",
        "\n",
        "        fname = doc.get(\"filename\")\n",
        "\n",
        "        # Check if we already have this file in our Master CSV\n",
        "        existing_idx = df_master.index[df_master[\"filename\"] == fname].tolist()\n",
        "\n",
        "        # Extract Data\n",
        "        val, evd = extract_feature(doc, target_feature, model, tokenizer)\n",
        "\n",
        "        # Prepare data dict\n",
        "        data = {f\"{target_feature}_value\": val, f\"{target_feature}_evidence\": evd}\n",
        "\n",
        "        if existing_idx:\n",
        "            # UPDATE existing row\n",
        "            idx = existing_idx[0]\n",
        "            for k, v in data.items():\n",
        "                df_master.at[idx, k] = v\n",
        "            updates += 1\n",
        "        else:\n",
        "            # CREATE new row\n",
        "            # Ensure we capture identity columns\n",
        "            row = {\n",
        "                \"filename\": fname,\n",
        "                \"cik\": doc.get(\"cik\"),\n",
        "                \"year\": doc.get(\"year\"),\n",
        "                **data,\n",
        "            }\n",
        "            new_rows.append(row)\n",
        "\n",
        "    # 3. Save Results\n",
        "    if new_rows:\n",
        "        df_new = pd.DataFrame(new_rows)\n",
        "        # Merge\n",
        "        df_master = pd.concat([df_master, df_new], ignore_index=True)\n",
        "\n",
        "    print(\n",
        "        f\"\\nDiscovery Complete. Updated {updates} rows. Added {len(new_rows)} new rows.\"\n",
        "    )\n",
        "\n",
        "    filename = output_file\n",
        "    if update:\n",
        "        filename = output_file.replace(\".csv\", \"_updated.csv\")\n",
        "    df_master.to_csv(filename, index=False)\n",
        "    return df_master.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Discover 'fiscal_year_end'\n",
        "# Ensure you add 'fiscal_year_end' to QUESTION_BANK above first!\n",
        "\n",
        "# UNCOMMENT TO RUN:\n",
        "# df_disc = discover_new_feature(\"fiscal_year_end\", limit=5)\n",
        "# display(df_disc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
